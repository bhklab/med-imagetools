{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Med-Imagetools: Transparent and Reproducible Medical Image Processing Pipelines in Python","text":""},{"location":"#med-imagetools-core-features","title":"Med-ImageTools core features","text":"<p>Med-Imagetools, a python package offers the perfect tool to transform messy medical dataset folders to deep learning ready format in few lines of code. It not only processes DICOMs consisting of different modalities (like CT, PET, RTDOSE and RTSTRUCTS), it also transforms them into deep learning ready subject based format taking the dependencies of these modalities into consideration.  </p> <p></p>"},{"location":"#introduction","title":"Introduction","text":"<p>A medical dataset, typically contains multiple different types of scans for a single patient in a single study. As seen in the figure below, the different scans containing DICOM of different modalities are interdependent on each other. For making effective machine learning models, one ought to take different modalities into account.</p> <p>Click here for Full Screen Interactive Documentation</p> <p>Fig.1 - Different network topology for different studies of different patients</p> <p>Med-Imagetools is a unique tool, which focuses on subject based Machine learning. It crawls the dataset and makes a network by connecting different modalities present in the dataset. Based on the user defined modalities, med-imagetools, queries the graph and process the queried raw DICOMS. The processed DICOMS are saved as nrrds, which med-imagetools converts to torchio subject dataset and eventually torch dataloader for ML pipeline.</p>"},{"location":"#installing-med-imagetools","title":"Installing med-imagetools","text":"<pre><code>pip install med-imagetools\n</code></pre>"},{"location":"#repository-stars","title":"Repository Stars","text":""},{"location":"#license","title":"License","text":"<p>This project uses the following license: MIT License</p>"},{"location":"database_report/","title":"Database Report","text":""},{"location":"database_report/#number-of-patients-100","title":"Number of Patients: 100","text":""},{"location":"database_report/#number-of-studies-315","title":"Number of Studies: 315","text":""},{"location":"database_report/#number-of-series-3123","title":"Number of Series: 3123","text":""},{"location":"database_report/#modality-summary","title":"Modality Summary","text":"Modality Count MR 152 SEG 68 CT 138 OT 1 PT 10 RTSTRUCT 16 RTDOSE 16 RTPLAN 13 DX 8 SR 10 MG 5 NM 1 CR 11"},{"location":"database_report/#data-summary","title":"Data Summary","text":"Patient ID Number of Studies Number of Series Unique Modalities TCGA-06-0184 12 183 {'SEG', 'MR'} TCGA-06-0185 16 176 {'SEG', 'MR'} TCGA-BB-A5HY 11 121 {'CT', 'OT', 'MR'} TCGA-06-0188 9 110 {'SEG', 'MR'} TCGA-CV-7090 15 93 {'PT', 'CT', 'MR'} TCGA-06-0138 6 90 {'SEG', 'MR'} TCGA-06-1084 6 77 {'SEG', 'MR'} TCGA-06-0139 5 76 {'SEG', 'MR'} TCGA-CV-7243 15 72 {'RTDOSE', 'CT', 'RTSTRUCT'} TCGA-CV-A6K0 13 71 {'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-A6JY 14 68 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-DD-A4NG 8 58 {'PT', 'CT', 'MR'} TCGA-06-0238 3 54 {'SEG', 'MR'} TCGA-06-1802 3 51 {'SEG', 'MR'} TCGA-D1-A16D 9 43 {'PT', 'CT', 'MR'} TCGA-06-0164 3 36 {'SEG', 'MR'} TCGA-06-0137 2 34 {'SEG', 'MR'} TCGA-14-1794 4 34 {'CT', 'SEG', 'MR'} TCGA-06-0130 2 33 {'SEG', 'MR'} TCGA-CS-6186 2 33 {'CT', 'SEG', 'MR'} TCGA-CV-A6JO 8 32 {'RTPLAN', 'CT', 'RTDOSE', 'PT', 'RTSTRUCT'} TCGA-06-0154 1 32 {'SEG', 'MR'} TCGA-02-0075 1 31 {'SEG', 'MR'} TCGA-06-0190 2 31 {'SEG', 'MR'} TCGA-02-0087 1 30 {'SEG', 'MR'} TCGA-CV-A6K1 7 28 {'RTDOSE', 'CT', 'RTSTRUCT', 'RTPLAN'} TCGA-G2-A3IE 6 28 {'CT', 'DX', 'MR'} TCGA-AO-A0JB 3 26 {'SR', 'MG', 'MR'} TCGA-02-0064 1 25 {'SEG', 'MR'} TCGA-02-0059 1 25 {'SEG', 'MR'} TCGA-02-0034 1 25 {'SEG', 'MR'} TCGA-06-0122 1 25 {'SEG', 'MR'} TCGA-06-0644 1 25 {'SEG', 'MR'} TCGA-06-0646 1 25 {'SEG', 'MR'} TCGA-06-5413 1 25 {'SEG', 'MR'} TCGA-06-0192 2 25 {'SEG', 'MR'} TCGA-06-0179 1 25 {'SEG', 'MR'} TCGA-08-0359 1 25 {'SEG', 'MR'} TCGA-06-5417 1 25 {'SEG', 'MR'} TCGA-02-0069 1 24 {'SEG', 'MR'} TCGA-06-0119 1 24 {'SEG', 'MR'} TCGA-CV-A6K2 5 24 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-08-0389 1 24 {'SEG', 'MR'} TCGA-06-0149 1 24 {'SEG', 'MR'} TCGA-CR-7368 4 24 {'PT', 'CT', 'MR'} TCGA-06-0158 2 24 {'SEG', 'MR'} TCGA-02-0086 1 24 {'SEG', 'MR'} TCGA-08-0355 1 24 {'SEG', 'MR'} TCGA-06-2570 1 23 {'SEG', 'MR'} TCGA-02-0027 1 22 {'SEG', 'MR'} TCGA-B9-A44B 2 22 {'PT', 'CT', 'MR'} TCGA-06-6389 1 22 {'SEG', 'MR'} TCGA-50-5072 4 22 {'PT', 'CT', 'NM'} TCGA-08-0385 2 22 {'SEG', 'MR'} TCGA-06-0142 1 22 {'SEG', 'MR'} TCGA-02-0068 1 22 {'SEG', 'MR'} TCGA-06-0240 1 22 {'SEG', 'MR'} TCGA-08-0360 1 21 {'SEG', 'MR'} TCGA-06-0182 1 21 {'SEG', 'MR'} TCGA-G2-A2EK 11 21 {'CR', 'CT', 'DX'} TCGA-06-0187 1 21 {'SEG', 'MR'} TCGA-08-0392 1 21 {'SEG', 'MR'} TCGA-06-5408 1 21 {'SEG', 'MR'} TCGA-08-0390 1 20 {'SEG', 'MR'} TCGA-02-0070 1 20 {'SEG', 'MR'} TCGA-02-0054 1 19 {'SEG', 'MR'} TCGA-08-0356 1 19 {'SEG', 'MR'} TCGA-G2-A2EL 5 19 {'CR', 'CT', 'DX'} TCGA-AO-A0JF 5 19 {'SR', 'MG', 'MR'} TCGA-06-0177 1 19 {'SEG', 'MR'} TCGA-02-0011 1 18 {'SEG', 'MR'} TCGA-02-0033 1 18 {'SEG', 'MR'} TCGA-02-0047 1 18 {'SEG', 'MR'} TCGA-02-0037 1 18 {'SEG', 'MR'} TCGA-06-0162 1 18 {'SEG', 'MR'} TCGA-02-0106 1 18 {'SEG', 'MR'} TCGA-06-0145 1 18 {'SEG', 'MR'} TCGA-06-0176 1 17 {'SEG', 'MR'} TCGA-G2-AA3D 5 17 {'CR', 'CT', 'DX'} TCGA-G2-A2EO 7 17 {'CR', 'CT', 'DX'} TCGA-AO-A0JI 2 17 {'SR', 'MG', 'MR'} TCGA-02-0116 1 17 {'SEG', 'MR'} TCGA-02-0009 1 16 {'SEG', 'MR'} TCGA-02-0006 1 16 {'SEG', 'MR'} TCGA-02-0046 1 16 {'SEG', 'MR'} TCGA-G7-A8LD 3 16 {'PT', 'CT', 'MR'} TCGA-02-0102 1 16 {'SEG', 'MR'} TCGA-02-0085 1 16 {'SEG', 'MR'} TCGA-CR-6478 2 14 {'PT', 'CT', 'MR'} TCGA-CV-5973 2 13 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-CV-5977 2 13 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-7235 2 10 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5976 2 10 {'RTDOSE', 'CT', 'RTSTRUCT', 'RTPLAN'} TCGA-CV-6433 2 9 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-08-0509 1 8 {'SEG', 'MR'} TCGA-CV-5966 2 8 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5978 2 8 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5970 2 7 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-CV-7236 2 7 {'RTDOSE', 'CT', 'RTSTRUCT'} TCGA-CV-7245 2 7 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'}"},{"location":"cli/imgtools/","title":"<code>imgtools</code> CLI","text":""},{"location":"cli/imgtools/#imgtools","title":"imgtools","text":"<p>A collection of tools for working with medical imaging data.</p> <p>Usage:</p> <pre><code>imgtools [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--quiet</code>, <code>-q</code> boolean Suppress all logging except errors, overrides verbosity options. <code>False</code> <code>--verbose</code>, <code>-v</code> integer range (<code>0</code> and above) Increase verbosity of logging, overrides environment variable. (0-3: ERROR, WARNING, INFO, DEBUG). <code>0</code> <code>--version</code> boolean Show the version and exit. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code> <p>Subcommands</p> <ul> <li>autopipeline: Core utility to process messy DICOM data into organized NIfTI files.</li> <li>dicomfind: A tool to find DICOM files.</li> <li>dicomshow: Display and extract DICOM file metadata in a readable table format.</li> <li>dicomsort: Sorts DICOM files into directories based on their tags.</li> <li>index: Crawl DICOM directory and create a database index.</li> <li>interlacer: Visualize DICOM series relationships after indexing.</li> <li>nnunet-pipeline: Process medical images in nnUNet format.</li> </ul>"},{"location":"cli/imgtools/#imgtools-autopipeline","title":"imgtools autopipeline","text":"<p>Core utility to process messy DICOM data into organized NIfTI files.</p> <p>This command allows you to process medical images in a directory structure, apply transformations, and save the results to a specified output directory.</p> <p>INPUT_DIRECTORY: Directory containing the input images. OUTPUT_DIRECTORY: Directory to save the processed images.</p> <p>The default filename format is: <code>{SampleNumber}__{PatientID}/{Modality}_{SeriesInstanceUID}/{ImageID}.nii.gz</code> where:</p> <ul> <li>SampleNumber: The identifier for the sample after querying.</li> <li>PatientID: The ID of the patient.</li> <li>Modality: The imaging modality (e.g., CT, MRI).</li> <li>SeriesInstanceUID: The unique identifier for the series.</li> <li>ImageID: The cutomized identifier for the image.<ul> <li>By default, the modality of the image</li> <li>If RTSTRUCT or SEG, uses custom format based on the roi_strategy     roi_match_map and roi names. TODO:: explain this in the docs?</li> </ul> </li> </ul> <p>It is not recommended to change the default filename format to prevent overwriting files. The default format is designed to ensure that the output filenames are unique and informative. If you need to customize the output make sure to use a format that maintains uniqueness and clarity.</p> <p>Usage:</p> <pre><code>imgtools autopipeline [OPTIONS] INPUT_DIRECTORY OUTPUT_DIRECTORY\n</code></pre> <p>Options:</p> Name Type Description Default <code>--filename-format</code>, <code>-f</code> text Format string for output filenames with placeholders for metadata values <code>{SampleNumber}__{PatientID}/{Modality}_{SeriesInstanceUID}/{ImageID}.nii.gz</code> <code>--modalities</code>, <code>-m</code> text List of modalities to process, in hierarchical order. For example, '--modalities CT,PT,RTSTRUCT' or '--modalities MR,SEG' _required <code>--existing-file-mode</code> choice (<code>overwrite</code> | <code>skip</code> | <code>fail</code>) How to handle existing files <code>fail</code> <code>--update-crawl</code> boolean Force recrawling of the input directory <code>False</code> <code>--jobs</code>, <code>-j</code> integer Number of parallel jobs None <code>--spacing</code> text Resampling spacing as comma-separated values i.e '--spacing 1.0,1.0,1.0' <code>0.0,0.0,0.0</code> <code>--window-width</code> float Width of the window for intensity windowing None <code>--window-level</code> float Midpoint of the window for intensity windowing None <code>--roi-ignore-case</code> / <code>--roi-case-sensitive</code> boolean Perform case\u2011insensitive ROI matching (default: enabled) <code>True</code> <code>--roi-strategy</code> choice (<code>MERGE</code> | <code>KEEP_FIRST</code> | <code>SEPARATE</code>) Strategy for handling ROI matches <code>SEPARATE</code> <code>--roi-allow-multi-matches</code> / <code>--roi-disallow-multi-matches</code> boolean Allow one ROI to match multiple keys in the match map <code>True</code> <code>--roi-on-missing-regex</code> choice (<code>IGNORE</code> | <code>WARN</code> | <code>ERROR</code>) How to handle when no ROI matches any pattern <code>WARN</code> <code>--roi-match-map</code>, <code>-rmap</code> text ROI matching patterns in format 'key:pattern1,pattern2,...'. Can be used multiple times to specify multiple roi mappings. CLI arguments take precedence over entries in the YAML file, if common keys exist. None <code>--roi-match-yaml</code>, <code>-ryaml</code> file Path to YAML file containing ROI matching patterns. None <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-dicomfind","title":"imgtools dicomfind","text":"<p>A tool to find DICOM files.</p> <p>PATH is the directory to search for DICOM files.</p> <p>SEARCH_INPUT is an optional list of substring(s) to search for in the DICOM files. If multiple substrings are provided, all substrings must match to return a result.</p> <p>i.e dicomfind /path/to/directory/ \"substring1\" \"substring2\" \"substring3\"</p> <p>Usage:</p> <pre><code>imgtools dicomfind [OPTIONS] PATH [SEARCH_INPUT]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>-e</code>, <code>--extension</code> text File extension to look for. <code>dcm</code> <code>-c</code>, <code>--count</code> boolean Whether to just print the count of files found. This is useful for scripts. <code>False</code> <code>-l</code>, <code>--limit</code> integer The limit of results to return. None <code>-ch</code>, <code>--check-header</code> boolean Whether to check DICOM header for \"DICM\" signature. <code>False</code> <code>-s</code>, <code>--sorted</code> boolean Sort the results alphabetically. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-dicomshow","title":"imgtools dicomshow","text":"<p>Display and extract DICOM file metadata in a readable table format.</p> <p>FILE_PATH[::FIELD] specifies what to display:   - FILE_PATH: Show all metadata from the DICOM file   - FILE_PATH::FIELD: Extract only the specified field</p> <p>FIELD syntax options:   \u2022 Standard tag names: Modality, PatientName, SeriesDescription   \u2022 Nested tags: tag.nested_tag   \u2022 Array indexing: tag[0]   \u2022 Combining methods: tag[0].nested_tag   \u2022 DICOM hex tags: (0008,0060) for modality</p> <p>Examples:   imgtools dicomshow scan.dcm                        # Show all metadata   imgtools dicomshow scan.dcm::Modality              # Show only the modality   imgtools dicomshow scan.dcm::PatientName           # Extract patient name   imgtools dicomshow scan.dcm::SequenceItem[0].Value # Access sequence data   imgtools dicomshow scan.dcm::(0010,0010)           # Use standard DICOM tag</p> <p>Output is formatted as a color-coded table with field names and values.</p> <p>Usage:</p> <pre><code>imgtools dicomshow [OPTIONS] FILE_PATH[::FIELD]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--pydicom</code>, <code>-p</code> boolean Use raw pydicom implementation instead of enhanced metadata extraction. Faster but excludes computed fields. <code>False</code> <code>--no-progress</code> boolean Disable progress bars when displaying large files. Use this when piping output or in scripts. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-dicomsort","title":"imgtools dicomsort","text":"<p>Sorts DICOM files into directories based on their tags.</p> <p>Usage:</p> <pre><code>imgtools dicomsort [OPTIONS] SOURCE_DIRECTORY TARGET_DIRECTORY\n</code></pre> <p>Options:</p> Name Type Description Default <code>--action</code>, <code>-a</code> choice (<code>move</code> | <code>copy</code> | <code>symlink</code> | <code>hardlink</code>) Action to perform on the files. _required <code>-n</code>, <code>--dry-run</code> boolean Do not move or copy files, just print what would be done. Always recommended to use this first to confirm the operation! <code>False</code> <code>-j</code>, <code>--num-workers</code> integer Number of worker processes to use for sorting. <code>1</code> <code>--truncate-uids</code>, <code>-t</code> integer Truncate the UIDs in the DICOM files to the specified length. Set to 0 to disable truncation. <code>5</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-index","title":"imgtools index","text":"<p>Crawl DICOM directory and create a database index.</p> <ul> <li> <p>looks for all DICOM files in the specified directory, extracts metadata,      and builds a comprehensive index of the dataset.</p> </li> <li> <p>The index includes information about the series, modalities, and other     relevant details, making it easier to manage and analyze the DICOM files.</p> </li> <li> <p>The output is saved in a structured format, including JSON and CSV files,     which can be used for further processing or analysis.</p> </li> <li> <p>By default, it saves the results in a \".imgtools\" folder right next to      your DICOM directory, but you can pick your own place to store them.</p> </li> </ul> <p>Usage:</p> <pre><code>imgtools index [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--dicom-dir</code> path Path to the DICOM directory. _required <code>--output-dir</code> path Path to the output directory. If not specified, a directory named '.imgtools' will be created in the parent directory of the DICOM directory. None <code>--dataset-name</code> text Name of the dataset. If not specified, the name of the DICOM directory will be used. None <code>--n-jobs</code> integer Number of jobs to use for parallel processing. <code>2</code> <code>--force</code> boolean Force overwrite existing files. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-interlacer","title":"imgtools interlacer","text":"<p>Visualize DICOM series relationships after indexing.</p> <p>This command will print the tree hierarchy of DICOM series relationships similar to GNU/Linux <code>tree</code> command.</p> <p>Only shows supported modalities.</p> <p>`interlacer PATH will print the series tree for the given index file / directory.</p> <p>If PATH is a path to an index.csv file, then the tree will be constructed using that index file. If instead PATH is a path to a directory, if the an index file for the directory exists it will be used to contruct the tree,  otherwise the directory will be crawled first.</p> <p>The index file should be a CSV file with the following columns: - SeriesInstanceUID - Modality - PatientID - StudyInstanceUID - folder - ReferencedSeriesUID</p> <p>Visit https://bhklab.github.io/med-imagetools/ for more information.</p> <p>Usage:</p> <pre><code>imgtools interlacer [OPTIONS] PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--query-string</code>, <code>-q</code> text Comma-separated string of modalities to query (e.g., 'CT,MR'). <p>Supported modalities: CT, PT, MR, SEG, RTSTRUCT, RTDOSE. | None | | <code>--group-by-root</code>, <code>-g</code> | boolean | If True, group the returned dicoms by their root CT/MR/PT node (i.e., avoid duplicate root nodes across results). | <code>True</code> | | <code>--n-jobs</code> | integer | Number of jobs to use for parallel processing. | <code>2</code> | | <code>--force</code> | boolean | Force overwrite existing files. | <code>False</code> | | <code>--help</code>, <code>-h</code> | boolean | Show this message and exit. | <code>False</code> |</p>"},{"location":"cli/imgtools/#imgtools-nnunet-pipeline","title":"imgtools nnunet-pipeline","text":"<p>Process medical images in nnUNet format.</p> <p>This command allows you to process medical images in the nnUNet directory structure, apply transformations, and save the results to a specified output directory.</p> <p>INPUT_DIRECTORY: Directory containing the input images. OUTPUT_DIRECTORY: Directory to save the processed images.</p> <ul> <li>SampleNumber: The identifier for the sample after querying.</li> <li>PatientID: The ID of the patient.</li> <li>Modality: The imaging modality (e.g., CT, MRI).</li> <li>SeriesInstanceUID: The unique identifier for the series.</li> <li>ImageID: The cutomized identifier for the image.<ul> <li>By default, the modality of the image</li> <li>If RTSTRUCT or SEG, uses custom format based on the roi_strategy     roi_match_map and roi names. TODO:: explain this in the docs?</li> </ul> </li> </ul> <p>Usage:</p> <pre><code>imgtools nnunet-pipeline [OPTIONS] INPUT_DIRECTORY OUTPUT_DIRECTORY\n</code></pre> <p>Options:</p> Name Type Description Default <code>--modalities</code>, <code>-m</code> text List of modalities to process, in hierarchical order. For example, '--modalities CT,RTSTRUCT' or '--modalities MR,SEG' _required <code>--roi-match-yaml</code>, <code>-ryaml</code> file Path to YAML file containing ROI matching patterns. _required <code>--mask-saving-strategy</code>, <code>-ms</code> choice (<code>label_image</code> | <code>sparse_mask</code> | <code>region_mask</code>) Strategy for saving masks. <code>label_image</code> <code>--existing-file-mode</code> choice (<code>overwrite</code> | <code>skip</code> | <code>fail</code>) How to handle existing files <code>fail</code> <code>--update-crawl</code> boolean Force recrawling of the input directory <code>False</code> <code>--jobs</code>, <code>-j</code> integer Number of parallel jobs None <code>--spacing</code> text Resampling spacing as comma-separated values i.e '--spacing 1.0,1.0,1.0' <code>0.0,0.0,0.0</code> <code>--window-width</code> float Width of the window for intensity windowing None <code>--window-level</code> float Midpoint of the window for intensity windowing None <code>--roi-ignore-case</code> / <code>--roi-case-sensitive</code> boolean Perform case\u2011insensitive ROI matching (default: enabled) <code>True</code> <code>--roi-allow-multi-matches</code> / <code>--roi-disallow-multi-matches</code> boolean Allow one ROI to match multiple keys in the match map <code>True</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/shell-completion/","title":"Shell Completion","text":"<p>Shell completion is a feature that allows you to press the <code>Tab</code> key while typing a command to automatically complete command names, options, and arguments. This can significantly improve your productivity when working with the <code>imgtools</code> CLI.</p>"},{"location":"cli/shell-completion/#generating-completion-scripts","title":"Generating Completion Scripts","text":"<p>The <code>imgtools</code> CLI provides a built-in command to generate shell completion scripts for various shells:</p> <pre><code>imgtools shell-completion [SHELL]\n</code></pre> <p>Where <code>[SHELL]</code> is one of: <code>bash</code>, <code>zsh</code>, or <code>fish</code>.</p>"},{"location":"cli/shell-completion/#installation-instructions","title":"Installation Instructions","text":"BashZshFish <p>To enable completion for the current bash session:</p> <pre><code>source &lt;(imgtools shell-completion bash)\n</code></pre> <p>For permanent setup, add the completion script to a file in your bash completion directory:</p> <pre><code># Create the completions directory if it doesn't exist\nmkdir -p ~/.bash_completion.d\n\n# Save the completion script to a file\nimgtools shell-completion bash &gt; ~/.bash_completion.d/imgtools\n\n# Add the following to your ~/.bashrc file\necho 'source ~/.bash_completion.d/imgtools' &gt;&gt; ~/.bashrc\n\n# Reload your shell configuration\nsource ~/.bashrc\n</code></pre> <p>To enable completion for the current zsh session:</p> <pre><code>source &lt;(imgtools shell-completion zsh)\n</code></pre> <p>For permanent setup:</p> <pre><code># Create the completions directory if it doesn't exist\nmkdir -p ~/.zsh/completion\n\n# Save the completion script to a file\nimgtools shell-completion zsh &gt; ~/.zsh/completion/_imgtools\n\n# Add to your ~/.zshrc file\necho 'fpath=(~/.zsh/completion $fpath)' &gt;&gt; ~/.zshrc\necho 'autoload -U compinit &amp;&amp; compinit' &gt;&gt; ~/.zshrc\n\n# Reload your shell configuration\nsource ~/.zshrc\n</code></pre> <p>To enable completion for the current fish session:</p> <pre><code>imgtools shell-completion fish | source\n</code></pre> <p>For permanent setup:</p> <pre><code># Create the completions directory if it doesn't exist\nmkdir -p ~/.config/fish/completions\n\n# Save the completion script\nimgtools shell-completion fish &gt; ~/.config/fish/completions/imgtools.fish\n\n# Fish will automatically load the completions on next shell start\n</code></pre>"},{"location":"cli/shell-completion/#verifying-completion-works","title":"Verifying Completion Works","text":"<p>After setting up completion, you can verify it works by typing:</p> <pre><code>imgtools &lt;TAB&gt;\n</code></pre> <p>This should display available subcommands. You can also try:</p> <pre><code>imgtools dicomsort --&lt;TAB&gt;\n</code></pre> <p>This should show the <code>--</code>options available for the <code>dicomsort</code> command  (i.e <code>--action</code>, <code>--output</code>, etc.)</p>"},{"location":"cli/shell-completion/#troubleshooting","title":"Troubleshooting","text":"<p>If completions don't work after following these steps:</p> <ol> <li>Make sure you've reloaded your shell configuration or started a new terminal session</li> <li>Verify that the completion script was properly generated and saved</li> <li>Check if your shell supports tab completion (it should by default)</li> </ol>"},{"location":"reference/autopipeline/","title":"Autopipeline","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline","title":"autopipeline","text":"<p>Functions:</p> Name Description <code>process_one_sample</code> <p>Process a single medical imaging sample through the complete pipeline.</p>"},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline","title":"Autopipeline","text":"<pre><code>Autopipeline(\n    input_directory: str | pathlib.Path,\n    output_directory: str | pathlib.Path,\n    output_filename_format: str = imgtools.io.sample_output.DEFAULT_FILENAME_FORMAT,\n    existing_file_mode: imgtools.io.sample_output.ExistingFileMode = imgtools.io.sample_output.ExistingFileMode.FAIL,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    modalities: list[str] | None = None,\n    roi_match_map: imgtools.coretypes.masktypes.roi_matching.Valid_Inputs = None,\n    roi_ignore_case: bool = True,\n    roi_handling_strategy: (\n        str\n        | imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy\n    ) = imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy.SEPARATE,\n    roi_allow_multi_key_matches: bool = True,\n    roi_on_missing_regex: (\n        str\n        | imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy\n    ) = imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy.WARN,\n    spacing: tuple[float, float, float] = (0.0, 0.0, 0.0),\n    window: float | None = None,\n    level: float | None = None,\n)\n</code></pre> <p>Pipeline for processing medical images.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Directory containing the DICOM files (or subdirectories with DICOM files)</p> required <code>str | pathlib.Path</code> <p>Directory to save the output nifti files</p> required <code>str</code> <p>Format string for output filenames with placeholders for metadata values.</p> <code>imgtools.io.sample_output.DEFAULT_FILENAME_FORMAT</code> <code>imgtools.io.sample_output.ExistingFileMode</code> <p>How to handle existing files (FAIL, SKIP, OVERWRITE).</p> <code>imgtools.io.sample_output.ExistingFileMode.FAIL</code> <code>bool</code> <p>Whether to force recrawling, by default False</p> <code>False</code> <code>int | None</code> <p>Number of parallel jobs, by default None (uses CPU count - 2)</p> <code>None</code> <code>list[str] | None</code> <p>List of modalities to include, by default None (all)</p> <code>None</code> <code>imgtools.coretypes.masktypes.roi_matching.Valid_Inputs</code> <p>ROI matching patterns, by default None</p> <code>None</code> <code>bool</code> <p>Whether to ignore case in ROI matching, by default True</p> <code>True</code> <code>str | imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy</code> <p>Strategy for handling ROI matches, by default ROIMatchStrategy.MERGE</p> <code>imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy.SEPARATE</code> <code>bool</code> <p>Whether to allow one ROI to match multiple keys in the match_map.</p> <code>True</code> <code>str | imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy</code> <p>How to handle when no ROI matches any pattern in match_map. By default ROIMatchFailurePolicy.WARN</p> <code>imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy.WARN</code> <code>tuple[float, float, float]</code> <p>Spacing for resampling, by default (0.0, 0.0, 0.0)</p> <code>(0.0, 0.0, 0.0)</code> <code>float | None</code> <p>Window level for intensity normalization, by default None</p> <code>None</code> <code>float | None</code> <p>Window level for intensity normalization, by default None</p> <code>None</code> <p>Methods:</p> Name Description <code>run</code> <p>Run the pipeline on all samples.</p> Source code in <code>src/imgtools/autopipeline.py</code> <pre><code>def __init__(\n    self,\n    input_directory: str | Path,\n    output_directory: str | Path,\n    output_filename_format: str = DEFAULT_FILENAME_FORMAT,\n    existing_file_mode: ExistingFileMode = ExistingFileMode.FAIL,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    modalities: list[str] | None = None,\n    roi_match_map: ROIMatcherInputs = None,\n    roi_ignore_case: bool = True,\n    roi_handling_strategy: str\n    | ROIMatchStrategy = ROIMatchStrategy.SEPARATE,\n    roi_allow_multi_key_matches: bool = True,\n    roi_on_missing_regex: str | ROIMatchFailurePolicy = (\n        ROIMatchFailurePolicy.WARN\n    ),\n    spacing: tuple[float, float, float] = (0.0, 0.0, 0.0),\n    window: float | None = None,\n    level: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Autopipeline.\n\n    Parameters\n    ----------\n    input_directory : str | Path\n        Directory containing the DICOM files (or subdirectories with DICOM files)\n    output_directory : str | Path\n        Directory to save the output nifti files\n    output_filename_format : str\n        Format string for output filenames with placeholders for metadata values.\n    existing_file_mode : ExistingFileMode\n        How to handle existing files (FAIL, SKIP, OVERWRITE).\n    update_crawl : bool, optional\n        Whether to force recrawling, by default False\n    n_jobs : int | None, optional\n        Number of parallel jobs, by default None (uses CPU count - 2)\n    modalities : list[str] | None, optional\n        List of modalities to include, by default None (all)\n    roi_match_map : ROIMatcherInputs, optional\n        ROI matching patterns, by default None\n    roi_ignore_case : bool, optional\n        Whether to ignore case in ROI matching, by default True\n    roi_handling_strategy : str | ROIMatchStrategy, optional\n        Strategy for handling ROI matches, by default ROIMatchStrategy.MERGE\n    roi_allow_multi_key_matches : bool, default=True\n        Whether to allow one ROI to match multiple keys in the match_map.\n    roi_on_missing_regex : str | ROIMatchFailurePolicy, optional\n        How to handle when no ROI matches any pattern in match_map.\n        By default ROIMatchFailurePolicy.WARN\n    spacing : tuple[float, float, float], default=(0.0, 0.0, 0.0)\n        Spacing for resampling, by default (0.0, 0.0, 0.0)\n    window : float | None, optional\n        Window level for intensity normalization, by default None\n    level : float | None, optional\n        Window level for intensity normalization, by default None\n    \"\"\"\n    self.input = SampleInput.build(\n        directory=Path(input_directory),\n        update_crawl=update_crawl,\n        n_jobs=n_jobs,\n        modalities=modalities,\n        roi_match_map=roi_match_map,\n        roi_ignore_case=roi_ignore_case,\n        roi_handling_strategy=roi_handling_strategy,\n        roi_allow_multi_key_matches=roi_allow_multi_key_matches,\n        roi_on_missing_regex=roi_on_missing_regex,\n    )\n    self.output = SampleOutput(\n        directory=Path(output_directory),\n        filename_format=output_filename_format,\n        existing_file_mode=existing_file_mode,\n        extra_context={},\n    )\n\n    transforms: list[BaseTransform] = [\n        # we could choose to only add resampling if any spacing component\n        # is non-zero, but this currently does additional non-intuitive\n        # alignment by assuming the first image in the sample is the reference\n        # and all other images get resampled to that via sitk.Resample\n        Resample(\n            spacing,\n            interpolation=\"linear\",\n            anti_alias=True,\n            anti_alias_sigma=None,\n            transform=None,\n            output_size=None,\n        ),\n    ]\n\n    if window is not None and level is not None:\n        transforms.append(WindowIntensity(window=window, level=level))\n\n    self.transformer = Transformer(transforms)\n\n    logger.info(\"Pipeline initialized\")\n</code></pre>"},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(input_directory)","title":"<code>input_directory</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(output_directory)","title":"<code>output_directory</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(output_filename_format)","title":"<code>output_filename_format</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(existing_file_mode)","title":"<code>existing_file_mode</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(update_crawl)","title":"<code>update_crawl</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(n_jobs)","title":"<code>n_jobs</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(modalities)","title":"<code>modalities</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(roi_match_map)","title":"<code>roi_match_map</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(roi_ignore_case)","title":"<code>roi_ignore_case</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(roi_handling_strategy)","title":"<code>roi_handling_strategy</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(roi_allow_multi_key_matches)","title":"<code>roi_allow_multi_key_matches</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(roi_on_missing_regex)","title":"<code>roi_on_missing_regex</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(spacing)","title":"<code>spacing</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(window)","title":"<code>window</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline(level)","title":"<code>level</code>","text":""},{"location":"reference/autopipeline/#imgtools.autopipeline.Autopipeline.run","title":"run","text":"<pre><code>run() -&gt; typing.Dict[\n    str,\n    typing.List[imgtools.autopipeline.ProcessSampleResult],\n]\n</code></pre> <p>Run the pipeline on all samples.</p> <p>Returns:</p> Type Description <code>typing.Dict[str, typing.List[imgtools.autopipeline.ProcessSampleResult]]</code> <p>Dictionary with 'success' and 'failure' keys, each containing a list of ProcessSampleResult objects.</p> Source code in <code>src/imgtools/autopipeline.py</code> <pre><code>def run(\n    self,\n) -&gt; Dict[str, List[ProcessSampleResult]]:\n    \"\"\"\n    Run the pipeline on all samples.\n\n    Returns\n    -------\n    Dict[str, List[ProcessSampleResult]]\n        Dictionary with 'success' and 'failure' keys, each containing a list of\n        ProcessSampleResult objects.\n    \"\"\"\n\n    # Load the samples\n    samples = self.input.query()\n    samples = sorted(samples, key=lambda x: x[0].PatientID.lower())\n\n    if not samples:\n        raise NoValidSamplesError(\n            message=\"No valid samples found.\",\n            user_query=self.input.modalities,\n            valid_queries=self.input.interlacer.valid_queries,\n        )\n\n    # Create a timestamp for this run\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # Prepare arguments for parallel processing\n    arg_tuples = [\n        (\n            f\"{idx:04}\",\n            sample,\n            self.input,\n            self.transformer,\n            self.output,\n        )\n        for idx, sample in enumerate(samples)\n    ]\n\n    # Lists to track results\n    all_results = []\n    successful_results = []\n    failed_results = []\n\n    with (\n        tqdm_logging_redirect(),\n        tqdm(\n            total=len(arg_tuples),\n            desc=\"Processing samples\",\n            unit=\"sample\",\n        ) as pbar,\n    ):\n        # Process samples in parallel\n        for result in Parallel(\n            n_jobs=self.input.n_jobs,\n            backend=\"loky\",\n            return_as=\"generator\",\n        )(delayed(process_one_sample)(arg) for arg in arg_tuples):\n            all_results.append(result)\n\n            # Update progress bar and track results by success/failure\n            if not result.has_error:\n                successful_results.append(result)\n            else:\n                failed_results.append(result)\n            pbar.update(1)\n\n    # Create pipeline results object\n    pipeline_results = PipelineResults(\n        successful_results=successful_results,\n        failed_results=failed_results,\n        all_results=all_results,\n        timestamp=timestamp,\n    )\n\n    # Save reports and get file paths\n    save_pipeline_reports(\n        results=pipeline_results,\n        index_file=self.output.writer.index_file,\n        root_dir_name=self.output.writer.root_directory.name,\n        simplified_columns=SIMPLIFIED_COLUMNS,\n        index_lock_check_func=self.output.writer._get_index_lock,\n    )\n\n    # Return all results categorized\n    return pipeline_results.to_dict()\n</code></pre>"},{"location":"reference/autopipeline/#imgtools.autopipeline.NoValidSamplesError","title":"NoValidSamplesError","text":"<pre><code>NoValidSamplesError(\n    message: str,\n    user_query: list[str] | None,\n    valid_queries: list[str],\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when no valid samples are found.</p> Source code in <code>src/imgtools/autopipeline.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    user_query: list[str] | None,\n    valid_queries: list[str],\n) -&gt; None:\n    msg = (\n        f\"{message}\\n\"\n        f\"User query: {user_query}\\n\"\n        f\"Valid queries: {valid_queries}\\n\"\n        # TODO::when we write docs on the logic of modality queries,\n        # we should add a link to the docs in this message\n    )\n\n    super().__init__(msg)\n    self.user_query = user_query\n    self.valid_queries = valid_queries\n</code></pre>"},{"location":"reference/autopipeline/#imgtools.autopipeline.ProcessSampleResult","title":"ProcessSampleResult  <code>dataclass</code>","text":"<pre><code>ProcessSampleResult(\n    sample_id: str,\n    sample: typing.Sequence[\n        imgtools.dicom.interlacer.SeriesNode\n    ],\n    output_files: typing.List[pathlib.Path] = list(),\n    success: bool = False,\n    error_type: typing.Optional[str] = None,\n    error_message: typing.Optional[str] = None,\n    error_details: typing.Optional[typing.Dict] = None,\n    processing_time: typing.Optional[float] = None,\n)\n</code></pre> <p>Result of processing a single sample.</p> <p>Methods:</p> Name Description <code>to_dict</code> <p>Convert the result to a dictionary.</p>"},{"location":"reference/autopipeline/#imgtools.autopipeline.ProcessSampleResult.has_error","title":"has_error  <code>property</code>","text":"<pre><code>has_error: bool\n</code></pre> <p>Check if the processing had an error.</p>"},{"location":"reference/autopipeline/#imgtools.autopipeline.ProcessSampleResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; typing.Dict\n</code></pre> <p>Convert the result to a dictionary.</p> Source code in <code>src/imgtools/autopipeline.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert the result to a dictionary.\"\"\"\n\n    base_dict = {\n        \"SampleID\": self.sample_id,\n        \"PatientID\": self.sample[0].PatientID if self.sample else None,\n        \"samples\": [\n            {\n                \"SeriesInstanceUID\": s.SeriesInstanceUID,\n                \"Modality\": s.Modality,\n                \"folder\": s.folder,\n            }\n            for s in self.sample\n        ],\n        \"processing_time\": f\"{self.processing_time:.2f}s\",\n    }\n\n    if not self.has_error:\n        return {\n            **base_dict,\n            \"output_files\": [str(f) for f in self.output_files],\n        }\n    else:\n        return {\n            **base_dict,\n            \"error_type\": self.error_type,\n            \"error_message\": self.error_message,\n            \"error_details\": self.error_details,\n        }\n</code></pre>"},{"location":"reference/autopipeline/#imgtools.autopipeline.process_one_sample","title":"process_one_sample","text":"<pre><code>process_one_sample(\n    args: tuple[\n        str,\n        typing.Sequence[\n            imgtools.dicom.interlacer.SeriesNode\n        ],\n        imgtools.io.sample_input.SampleInput,\n        imgtools.transforms.Transformer,\n        imgtools.io.sample_output.SampleOutput,\n    ],\n) -&gt; imgtools.autopipeline.ProcessSampleResult\n</code></pre> <p>Process a single medical imaging sample through the complete pipeline.</p> <p>The single 'args' tuple contains the following elements, likely passed in from the components of the autopipeline class: - idx: str (arbitrary, generated from enumerate) - sample: Sequence[SeriesNode] (a sample is the group of series that belong to the same reference image) - sample_input: SampleInput (class that handles loading the sample) - transformer: Transformer (class that handles the transformation pipeline) - sample_output: SampleOutput (class that handles saving the sample)</p> <p>This function handles the entire lifecycle of processing a medical image sample:</p> <ol> <li>First, we load the sample images from the provided input source</li> <li>Then, we verify that all requested images were properly loaded</li> <li>Next, we apply the transformation pipeline to the images (resampling, windowing, etc.)</li> <li>Finally, we save the processed images to the output location</li> </ol> <p>Throughout this process, we track any errors that occur and return detailed information about successes or failures for reporting purposes.</p> <p>Returns:</p> Type Description <code>imgtools.autopipeline.ProcessSampleResult</code> <p>Result of processing the sample, including success/failure information</p> Source code in <code>src/imgtools/autopipeline.py</code> <pre><code>def process_one_sample(\n    args: tuple[\n        str,\n        Sequence[SeriesNode],\n        SampleInput,\n        Transformer,\n        SampleOutput,\n    ],\n) -&gt; ProcessSampleResult:\n    \"\"\"\n    Process a single medical imaging sample through the complete pipeline.\n\n    The single 'args' tuple contains the following elements, likely passed in\n    from the components of the autopipeline class:\n    - idx: str (arbitrary, generated from enumerate)\n    - sample: Sequence[SeriesNode] (a sample is the group of series that belong to the same reference image)\n    - sample_input: SampleInput (class that handles loading the sample)\n    - transformer: Transformer (class that handles the transformation pipeline)\n    - sample_output: SampleOutput (class that handles saving the sample)\n\n    This function handles the entire lifecycle of processing a medical image sample:\n\n    1. First, we load the sample images from the provided input source\n    2. Then, we verify that all requested images were properly loaded\n    3. Next, we apply the transformation pipeline to the images (resampling, windowing, etc.)\n    4. Finally, we save the processed images to the output location\n\n    Throughout this process, we track any errors that occur and return detailed\n    information about successes or failures for reporting purposes.\n\n    Returns\n    -------\n    ProcessSampleResult\n        Result of processing the sample, including success/failure information\n    \"\"\"\n    # TODO:: the logic for all the result information is a bit messy\n    # rework it to pass in custom exception objects that get parsed in the\n    # to_dict method\n\n    start_time = time.time()\n\n    sample: Sequence[SeriesNode]\n    idx, sample, sample_input, transformer, sample_output = args\n\n    # Initialize the result with sample information\n    result = ProcessSampleResult(\n        sample_id=idx,\n        sample=sample,  # Store the entire sample\n    )\n\n    try:\n        # Load the sample\n        sample_images: Sequence[MedImage | VectorMask] = sample_input(sample)\n    except Exception as e:\n        error_message = str(e)\n        logger.exception(\"Failed to load sample\", e=e)\n        result.error_type = \"LoadError\"\n        result.error_message = f\"Failed to load sample: {error_message}\"\n        result.processing_time = time.time() - start_time\n        return result\n\n    # by this point all images SHOULD have some bare minimum\n    # metadata attribute, which should have the SeriesInstanceUID\n    # lets just quickly validate that the unique list of SeriesInstanceUIDs\n    # in our input 'samples' is the same as the unique list of SeriesInstanceUIDs\n    # in our loaded sample_images\n    series_instance_uids = {s.SeriesInstanceUID for s in sample}\n    loaded_series_instance_uids = {\n        s.metadata.get(\"SeriesInstanceUID\", None) for s in sample_images\n    }\n\n    # check if our input samples is a subset of our loaded sample_images\n    # we use subset, because we may have loaded more images than requested (subseries)\n    if not series_instance_uids.issubset(loaded_series_instance_uids):\n        error_msg = (\n            f\"Loaded {len(loaded_series_instance_uids)} sample\"\n            f\" images do not match input samples {len(series_instance_uids)}. \"\n            \"This most likely may be due to failures to match ROIs. \"\n            f\"We will save {len(loaded_series_instance_uids)} loaded, \"\n            f\"out of {len(series_instance_uids)} input series. \"\n        )\n        result.error_type = \"ROIMatchError\"\n        result.error_message = error_msg\n        result.error_details = {\n            \"loaded_series\": list(loaded_series_instance_uids),\n            \"input_series\": list(series_instance_uids),\n        }\n\n    try:\n        transformed_images = transformer(sample_images)\n    except Exception as e:\n        error_message = str(e)\n        result.error_type = \"TransformError\"\n        result.error_message = f\"Failed during transformation: {error_message}\"\n        result.processing_time = time.time() - start_time\n        return result\n\n    try:\n        saved_files = sample_output(\n            transformed_images,\n            SampleNumber=idx,\n        )\n        result.output_files = list(saved_files)\n        if not result.output_files:\n            raise ValueError(\n                \"No output files were saved. Check the output directory.\"\n            )\n        result.success = True\n    except Exception as e:\n        error_message = str(e)\n        result.error_type = \"SaveError\"\n        result.error_message = f\"Failed to save output: {error_message}\"\n        result.processing_time = time.time() - start_time\n        return result\n\n    result.processing_time = time.time() - start_time\n    return result\n</code></pre>"},{"location":"reference/autopipeline_utils/","title":"Autopipeline utils","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils","title":"autopipeline_utils","text":"<p>Utility functions for the autopipeline module.</p> <p>Functions:</p> Name Description <code>save_pipeline_reports</code> <p>Save pipeline reports including success/failure reports and simplified index.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults","title":"PipelineResults  <code>dataclass</code>","text":"<pre><code>PipelineResults(\n    successful_results: typing.List[\n        imgtools.autopipeline_utils.ResultType\n    ],\n    failed_results: typing.List[\n        imgtools.autopipeline_utils.ResultType\n    ],\n    all_results: typing.List[\n        imgtools.autopipeline_utils.ResultType\n    ],\n    timestamp: str | None = None,\n)\n</code></pre> <p>               Bases: <code>typing.Generic[imgtools.autopipeline_utils.ResultType]</code></p> <p>Class to store and handle pipeline processing results.</p> <p>This class stores successful and failed results from processing samples through the autopipeline and provides methods for saving reports and generating summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>typing.List[imgtools.autopipeline_utils.ResultType]</code> <p>List of successful processing results</p> required <code>typing.List[imgtools.autopipeline_utils.ResultType]</code> <p>List of failed processing results</p> required <code>typing.List[imgtools.autopipeline_utils.ResultType]</code> <p>List of all processing results</p> required <code>str</code> <p>Timestamp for this run, by default current datetime</p> <code>None</code> <p>Methods:</p> Name Description <code>log_summary</code> <p>Log summary information about the results.</p> <code>to_dict</code> <p>Convert results to a dictionary.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults(successful_results)","title":"<code>successful_results</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults(failed_results)","title":"<code>failed_results</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults(all_results)","title":"<code>all_results</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults(timestamp)","title":"<code>timestamp</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.failure_count","title":"failure_count  <code>property</code>","text":"<pre><code>failure_count: int\n</code></pre> <p>Number of failed results.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.success_count","title":"success_count  <code>property</code>","text":"<pre><code>success_count: int\n</code></pre> <p>Number of successful results.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.success_rate","title":"success_rate  <code>property</code>","text":"<pre><code>success_rate: float\n</code></pre> <p>Success rate as a percentage.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.total_count","title":"total_count  <code>property</code>","text":"<pre><code>total_count: int\n</code></pre> <p>Total number of results.</p>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.log_summary","title":"log_summary","text":"<pre><code>log_summary() -&gt; None\n</code></pre> <p>Log summary information about the results.</p> Source code in <code>src/imgtools/autopipeline_utils.py</code> <pre><code>def log_summary(self) -&gt; None:\n    \"\"\"Log summary information about the results.\"\"\"\n    logger.info(\n        f\"Processing complete. {self.success_count} successful, {self.failure_count} failed \"\n        f\"out of {self.total_count} total samples ({self.success_rate:.1f}% success rate).\"\n    )\n</code></pre>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.PipelineResults.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; typing.Dict[\n    str,\n    typing.List[imgtools.autopipeline_utils.ResultType],\n]\n</code></pre> <p>Convert results to a dictionary.</p> Source code in <code>src/imgtools/autopipeline_utils.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, List[ResultType]]:\n    \"\"\"Convert results to a dictionary.\"\"\"\n    return {\n        \"success\": self.successful_results,\n        \"failure\": self.failed_results,\n    }\n</code></pre>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports","title":"save_pipeline_reports","text":"<pre><code>save_pipeline_reports(\n    results: imgtools.autopipeline_utils.PipelineResults,\n    index_file: \"Path\",\n    root_dir_name: str,\n    simplified_columns: typing.List[str],\n    index_lock_check_func: (\n        typing.Callable[[], \"Path\"] | None\n    ) = None,\n) -&gt; typing.Dict[str, \"Path\"]\n</code></pre> <p>Save pipeline reports including success/failure reports and simplified index.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.autopipeline_utils.PipelineResults</code> <p>The pipeline results to save</p> required <code>pathlib.Path</code> <p>Path to the index file</p> required <code>str</code> <p>Name of the root directory for output</p> required <code>typing.List[str]</code> <p>List of columns to include in the simplified index</p> required <code>callable</code> <p>Function to check and remove index lock file</p> <code>None</code> <p>Returns:</p> Type Description <code>typing.Dict[str, pathlib.Path]</code> <p>Dictionary of saved file paths</p> Source code in <code>src/imgtools/autopipeline_utils.py</code> <pre><code>def save_pipeline_reports(\n    results: PipelineResults,\n    index_file: \"Path\",\n    root_dir_name: str,\n    simplified_columns: List[str],\n    index_lock_check_func: Callable[[], \"Path\"] | None = None,\n) -&gt; Dict[str, \"Path\"]:\n    \"\"\"\n    Save pipeline reports including success/failure reports and simplified index.\n\n    Parameters\n    ----------\n    results : PipelineResults\n        The pipeline results to save\n    index_file : Path\n        Path to the index file\n    root_dir_name : str\n        Name of the root directory for output\n    simplified_columns : List[str]\n        List of columns to include in the simplified index\n    index_lock_check_func : callable, optional\n        Function to check and remove index lock file\n\n    Returns\n    -------\n    Dict[str, Path]\n        Dictionary of saved file paths\n    \"\"\"\n    # Log summary\n    results.log_summary()\n\n    # Generate report file names\n    success_file = index_file.with_name(\n        f\"{root_dir_name}_successful_{results.timestamp}.json\"\n    )\n    failure_file = index_file.with_name(\n        f\"{root_dir_name}_failed_{results.timestamp}.json\"\n    )\n\n    # Write simplified index file\n    simple_index = index_file.parent / f\"{index_file.stem}-simple.csv\"\n\n    try:\n        index_df = pd.read_csv(index_file)\n\n        # Get columns in the order we want\n        # If a column is not in the index_df, it will be filled with NaN\n        simple_index_df = index_df.reindex(columns=simplified_columns)\n\n        # Sort by 'filepath' to make it easier to read\n        if \"filepath\" in simple_index_df.columns:\n            simple_index_df = simple_index_df.sort_values(by=[\"filepath\"])\n\n        simple_index_df.to_csv(simple_index, index=False)\n        logger.info(f\"Index file saved to {simple_index}\")\n    except Exception as e:\n        logger.error(f\"Failed to create simplified index: {e}\")\n\n    # Remove lockfile if a function was provided\n    # TODO:: probably a better way to do this\n    if index_lock_check_func is not None:\n        lock_file = index_lock_check_func()\n        if lock_file is not None and lock_file.exists():\n            lock_file.unlink()\n            logger.debug(f\"Lock file removed: {lock_file}\")\n\n    # Convert results to dictionaries for JSON serialization\n    success_dicts = [result.to_dict() for result in results.successful_results]\n\n    # Write success report\n    with success_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(success_dicts, f, indent=2)\n    logger.info(f\"Detailed success report saved to {success_file}\")\n\n    saved_files = {\"success_file\": success_file, \"simple_index\": simple_index}\n\n    # If no failures, we can skip writing the failure file\n    if results.failure_count == 0:\n        return saved_files\n\n    # Write failure report\n    failure_dicts = [result.to_dict() for result in results.failed_results]\n    with failure_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(failure_dicts, f, indent=2)\n    logger.info(f\"Detailed failure report saved to {failure_file}\")\n\n    saved_files[\"failure_file\"] = failure_file\n    return saved_files\n</code></pre>"},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports(results)","title":"<code>results</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports(index_file)","title":"<code>index_file</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports(root_dir_name)","title":"<code>root_dir_name</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports(simplified_columns)","title":"<code>simplified_columns</code>","text":""},{"location":"reference/autopipeline_utils/#imgtools.autopipeline_utils.save_pipeline_reports(index_lock_check_func)","title":"<code>index_lock_check_func</code>","text":""},{"location":"reference/exceptions/","title":"Exceptions","text":""},{"location":"reference/exceptions/#imgtools.exceptions","title":"exceptions","text":""},{"location":"reference/exceptions/#imgtools.exceptions.MissingROIError","title":"MissingROIError","text":"<p>               Bases: <code>KeyError</code></p> <p>Custom exception for missing ROI in the structure set.</p>"},{"location":"reference/exceptions/#imgtools.exceptions.ROIContourError","title":"ROIContourError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for missing ROI contour data in the RTSTRUCT file.</p>"},{"location":"reference/exceptions/#imgtools.exceptions.RTSTRUCTAttributeError","title":"RTSTRUCTAttributeError","text":"<p>               Bases: <code>imgtools.exceptions.InvalidDicomError</code></p> <p>Exception raised for attribute errors in the RTSTRUCT DICOM file.</p>"},{"location":"reference/nnunet_pipeline/","title":"Nnunet pipeline","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline","title":"nnunet_pipeline","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline","title":"nnUNetPipeline","text":"<pre><code>nnUNetPipeline(\n    input_directory: str | pathlib.Path,\n    output_directory: str | pathlib.Path,\n    modalities: list[str],\n    roi_match_map: imgtools.coretypes.masktypes.roi_matching.Valid_Inputs,\n    mask_saving_strategy: imgtools.io.nnunet_output.MaskSavingStrategy,\n    existing_file_mode: imgtools.io.sample_output.ExistingFileMode = imgtools.io.sample_output.ExistingFileMode.FAIL,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    roi_ignore_case: bool = True,\n    roi_allow_multi_key_matches: bool = True,\n    spacing: tuple[float, float, float] = (0.0, 0.0, 0.0),\n    window: float | None = None,\n    level: float | None = None,\n)\n</code></pre> <p>Pipeline for processing medical images in nnUNet format.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Directory containing the DICOM files (or subdirectories with DICOM files)</p> required <code>str | pathlib.Path</code> <p>Directory to save the output nifti files    existing_file_mode : ExistingFileMode How to handle existing files (FAIL, SKIP, OVERWRITE).</p> required <code>list[str]</code> <p>List of modalities to include</p> required <code>imgtools.coretypes.masktypes.roi_matching.Valid_Inputs</code> <p>ROI matching patterns</p> required <code>imgtools.io.nnunet_output.MaskSavingStrategy</code> <p>Mask saving strateg</p> required <code>bool</code> <p>Whether to force recrawling, by default False</p> <code>False</code> <code>int | None</code> <p>Number of parallel jobs, by default None (uses CPU count - 2)</p> <code>None</code> <code>bool</code> <p>Whether to ignore case in ROI matching, by default True</p> <code>True</code> <code>bool</code> <p>Whether to allow multiple key matches in ROI matching, by default True</p> <code>True</code> <code>tuple[float, float, float]</code> <p>Spacing for resampling, by default (0.0, 0.0, 0.0)</p> <code>(0.0, 0.0, 0.0)</code> <code>float | None</code> <p>Window level for intensity normalization, by default None</p> <code>None</code> <code>float | None</code> <p>Window level for intensity normalization, by default None</p> <code>None</code> <p>Methods:</p> Name Description <code>run</code> <p>Run the pipeline on all samples.</p> Source code in <code>src/imgtools/nnunet_pipeline.py</code> <pre><code>def __init__(\n    self,\n    input_directory: str | Path,\n    output_directory: str | Path,\n    modalities: list[str],\n    roi_match_map: ROIMatcherInputs,\n    mask_saving_strategy: MaskSavingStrategy,\n    existing_file_mode: ExistingFileMode = ExistingFileMode.FAIL,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    roi_ignore_case: bool = True,\n    roi_allow_multi_key_matches: bool = True,\n    spacing: tuple[float, float, float] = (0.0, 0.0, 0.0),\n    window: float | None = None,\n    level: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the nnUNetpipeline.\n\n    Parameters\n    ----------\n    input_directory : str | Path\n        Directory containing the DICOM files (or subdirectories with DICOM files)\n    output_directory : str | Path\n        Directory to save the output nifti files\n           existing_file_mode : ExistingFileMode\n        How to handle existing files (FAIL, SKIP, OVERWRITE).\n    modalities : list[str]\n        List of modalities to include\n    roi_match_map : ROIMatcherInputs\n        ROI matching patterns\n    mask_saving_strategy : MaskSavingStrategy\n        Mask saving strateg\n    update_crawl : bool, optional\n        Whether to force recrawling, by default False\n    n_jobs : int | None, optional\n        Number of parallel jobs, by default None (uses CPU count - 2)\n    roi_ignore_case : bool, optional\n        Whether to ignore case in ROI matching, by default True\n    roi_allow_multi_key_matches : bool, optional\n        Whether to allow multiple key matches in ROI matching, by default True\n    spacing : tuple[float, float, float], default=(0.0, 0.0, 0.0)\n        Spacing for resampling, by default (0.0, 0.0, 0.0)\n    window : float | None, optional\n        Window level for intensity normalization, by default None\n    level : float | None, optional\n        Window level for intensity normalization, by default None\n    \"\"\"\n\n    # Validate modalities\n    allowed_modalities = {\n        frozenset((\"CT\", \"SEG\")),\n        frozenset((\"MR\", \"SEG\")),\n        frozenset((\"CT\", \"RTSTRUCT\")),\n        frozenset((\"MR\", \"RTSTRUCT\")),\n    }\n    if frozenset(modalities) not in allowed_modalities:\n        msg = (\n            f\"Invalid modalities: {','.join(modalities)}. \"\n            f\"Allowed combinations are: {[','.join(allowed) for allowed in allowed_modalities]}\"\n        )\n        raise ValueError(msg)\n\n    self.input = SampleInput.build(\n        directory=Path(input_directory),\n        update_crawl=update_crawl,\n        n_jobs=n_jobs,\n        modalities=modalities,\n        roi_match_map=roi_match_map,\n        roi_ignore_case=roi_ignore_case,\n        roi_handling_strategy=ROIMatchStrategy.MERGE,\n        roi_on_missing_regex=ROIMatchFailurePolicy.ERROR,\n        roi_allow_multi_key_matches=roi_allow_multi_key_matches,\n    )\n\n    self.output = nnUNetOutput(\n        directory=Path(output_directory),\n        existing_file_mode=existing_file_mode,\n        dataset_name=Path(input_directory).name,\n        roi_keys=list(self.input.roi_matcher.match_map.keys()),\n        mask_saving_strategy=mask_saving_strategy,\n        extra_context={},\n    )\n\n    transforms: list[BaseTransform] = [\n        # we could choose to only add resampling if any spacing component\n        # is non-zero, but this currently does additional non-intuitive\n        # alignment by assuming the first image in the sample is the reference\n        # and all other images get resampled to that via sitk.Resample\n        Resample(\n            spacing,\n            interpolation=\"linear\",\n            anti_alias=True,\n            anti_alias_sigma=None,\n            transform=None,\n            output_size=None,\n        ),\n    ]\n\n    if window is not None and level is not None:\n        transforms.append(WindowIntensity(window=window, level=level))\n\n    self.transformer = Transformer(transforms)\n\n    logger.info(\"Pipeline initialized\")\n</code></pre>"},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(input_directory)","title":"<code>input_directory</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(output_directory)","title":"<code>output_directory</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(modalities)","title":"<code>modalities</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(roi_match_map)","title":"<code>roi_match_map</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(mask_saving_strategy)","title":"<code>mask_saving_strategy</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(update_crawl)","title":"<code>update_crawl</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(n_jobs)","title":"<code>n_jobs</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(roi_ignore_case)","title":"<code>roi_ignore_case</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(roi_allow_multi_key_matches)","title":"<code>roi_allow_multi_key_matches</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(spacing)","title":"<code>spacing</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(window)","title":"<code>window</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline(level)","title":"<code>level</code>","text":""},{"location":"reference/nnunet_pipeline/#imgtools.nnunet_pipeline.nnUNetPipeline.run","title":"run","text":"<pre><code>run() -&gt; typing.Dict[\n    str,\n    typing.List[imgtools.autopipeline.ProcessSampleResult],\n]\n</code></pre> <p>Run the pipeline on all samples.</p> <p>Returns:</p> Type Description <code>typing.Dict[str, typing.List[imgtools.autopipeline.ProcessSampleResult]]</code> <p>Dictionary with 'success' and 'failure' keys, each containing a list of ProcessSampleResult objects.</p> Source code in <code>src/imgtools/nnunet_pipeline.py</code> <pre><code>def run(\n    self,\n) -&gt; Dict[str, List[ProcessSampleResult]]:\n    \"\"\"\n    Run the pipeline on all samples.\n\n    Returns\n    -------\n    Dict[str, List[ProcessSampleResult]]\n        Dictionary with 'success' and 'failure' keys, each containing a list of\n        ProcessSampleResult objects.\n    \"\"\"\n    import json\n\n    # Load the samples\n    samples = self.input.query()\n    samples = sorted(samples, key=lambda x: x[0].PatientID.lower())\n\n    # Create a timestamp for this run\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # Prepare arguments for parallel processing\n    arg_tuples = [\n        (\n            f\"{idx:04}\",\n            sample,\n            self.input,\n            self.transformer,\n            self.output,\n        )\n        for idx, sample in enumerate(samples, start=1)\n    ]\n\n    # Lists to track results\n    all_results = []\n    successful_results = []\n    failed_results = []\n\n    with (\n        tqdm_logging_redirect(),\n        tqdm(\n            total=len(arg_tuples),\n            desc=\"Processing samples\",\n            unit=\"sample\",\n        ) as pbar,\n    ):\n        # Process samples in parallel\n        for result in Parallel(\n            n_jobs=self.input.n_jobs,\n            backend=\"loky\",\n            return_as=\"generator\",\n        )(delayed(process_one_sample)(arg) for arg in arg_tuples):\n            all_results.append(result)\n\n            # Update progress bar and track results by success/failure\n            if result.success:\n                successful_results.append(result)\n                pbar.update(1)\n            else:\n                failed_results.append(result)\n                pbar.update(0)\n\n    # Log summary information\n    success_count = len(successful_results)\n    failure_count = len(failed_results)\n    total_count = len(all_results)\n\n    logger.info(\n        f\"Processing complete. {success_count} successful, {failure_count} failed \"\n        f\"out of {total_count} total samples ({success_count / total_count * 100:.1f}% success rate).\"\n    )\n\n    # Finalize output(Generate dataset.json and nnUNet scripts)\n    if success_count &gt; 0:\n        self.output.finalize_dataset()\n\n    index_file = self.output.writer.index_file\n    # TODO:: discuss how we want to name these files\n    # Generate report file names\n    success_file = index_file.with_name(\n        f\"{self.output.writer.root_directory.name}_successful_{timestamp}.json\"\n    )\n    failure_file = index_file.with_name(\n        f\"{self.output.writer.root_directory.name}_failed_{timestamp}.json\"\n    )\n\n    # Convert results to dictionaries for JSON serialization\n    success_dicts = [result.to_dict() for result in successful_results]\n    failure_dicts = [result.to_dict() for result in failed_results]\n\n    # Write reports\n    # with open(success_file, \"w\") as f:\n    with success_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(success_dicts, f, indent=2)\n    logger.info(f\"Detailed success report saved to {success_file}\")\n\n    # if no failures, we can skip writing the failure file\n    if failure_count == 0:\n        return {\"success\": successful_results, \"failure\": []}\n\n    with failure_file.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(failure_dicts, f, indent=2)\n    logger.info(f\"Detailed failure report saved to {failure_file}\")\n\n    # Return all results categorized\n    return {\"success\": successful_results, \"failure\": failed_results}\n</code></pre>"},{"location":"reference/coretypes/base_masks/","title":"Base masks","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks","title":"base_masks","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask","title":"Mask  <code>dataclass</code>","text":"<pre><code>Mask(image: SimpleITK.Image, metadata: dict[str, str])\n</code></pre> <p>               Bases: <code>imgtools.coretypes.MedImage</code></p> <p>A scalar label mask image with sitk.sitkUInt8 or sitk.sitkLabelUInt8 pixel type.</p> <p>This class represents 2D or 3D labeled mask images stored as SimpleITK scalar images. Each voxel contains a single integer label value, where:     - 0 is conventionally reserved for background     - Positive integers (1-255) represent different labels/segments</p> <p>The dimensionality (2D or 3D) is inherited from the source image data.</p> <p>Background handling:     - Value 0 is always interpreted as background</p> <p>This class provides operations for handling labeled/segmentation data and converting between different representations.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>dict[str, str]</code> <p>Dictionary containing metadata about the mask</p> Notes <ul> <li>Multiple disjoint objects with the same label value are considered part     of the same segment</li> <li>To separate disjoint objects with the same label, where each has     its own unique value use <code>to_labeled_image()</code></li> <li>When converting from label images to vector masks, each unique label     becomes a separate channel</li> </ul> <p>image : sitk.Image     A SimpleITK image with pixel type sitk.sitkUInt8 or sitk.sitkLabelUInt8 metadata : dict[str, str]     Dictionary containing metadata about the mask</p> <p>Methods:</p> Name Description <code>from_file</code> <p>Create a MedImage from a file path with optional metadata.</p> <code>get_label_bounding_box</code> <p>Get bounding box around a label image for a given label or name.</p> <code>to_numpy</code> <p>Convert the image to a NumPy array.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def __init__(\n    self,\n    image: sitk.Image,\n    metadata: dict[str, str],\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    image : sitk.Image\n        A SimpleITK image with pixel type sitk.sitkUInt8 or sitk.sitkLabelUInt8\n    metadata : dict[str, str]\n        Dictionary containing metadata about the mask\n    \"\"\"\n    super().__init__(image)\n    self.metadata = metadata\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.elongation","title":"elongation  <code>property</code>","text":"<pre><code>elongation: float\n</code></pre> <p>Get how 'stretched out' the mask is</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.equivalent_ellipsoid_diameters","title":"equivalent_ellipsoid_diameters  <code>property</code>","text":"<pre><code>equivalent_ellipsoid_diameters: tuple[float, float, float]\n</code></pre> <p>Get the diameters of the ellipsoid that has the same principal moments of inertia as the mask image.</p> <p>Interpretation:     If you built an ellipsoid that \u201cbehaves\u201d like your shape under     rotation, these would be its diameters.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.equivalent_spherical_perimeter","title":"equivalent_spherical_perimeter  <code>property</code>","text":"<pre><code>equivalent_spherical_perimeter: float\n</code></pre> <p>Get the perimeter of the sphere that has the same volume as the mask image.</p> <p>Interpretation:     If you built a sphere that has the same volume as your shape,     this would be its perimeter.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.equivalent_spherical_radius","title":"equivalent_spherical_radius  <code>property</code>","text":"<pre><code>equivalent_spherical_radius: float\n</code></pre> <p>Get the radius of the sphere that has the same volume as the mask image.</p> <p>Interpretation:     If you built a sphere that has the same volume as your shape,     this would be its radius.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.feret_diameter","title":"feret_diameter  <code>property</code>","text":"<pre><code>feret_diameter: float\n</code></pre> <p>Get the longest distance between any two points on the mask image.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p> <p>Append to MedImage fingerprint</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.flatness","title":"flatness  <code>property</code>","text":"<pre><code>flatness: float\n</code></pre> <p>Get the flatness of the mask image.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.label_shape_filter","title":"label_shape_filter  <code>property</code>","text":"<pre><code>label_shape_filter: (\n    SimpleITK.LabelShapeStatisticsImageFilter\n)\n</code></pre> <p>Get the label shape filter for the mask image.</p> <p>Returns:</p> Type Description <code>SimpleITK.LabelShapeStatisticsImageFilter</code> <p>The label shape filter for the mask image.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.roundness","title":"roundness  <code>property</code>","text":"<pre><code>roundness: float\n</code></pre> <p>Get how similar the mask image is to a sphere</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.volume_count","title":"volume_count  <code>property</code>","text":"<pre><code>volume_count: int\n</code></pre> <p>Get the number of connected components in the mask image.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, str] | None = None,\n) -&gt; \"Mask\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> <p>Create a Mask from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the mask image file</p> required <code>dict[str, str] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_masks.Mask</code> <p>A new Mask instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - All keys filtered by MedImage.from_file - Additionally, any key starting with \"mask.\" (mask shape statistics)</p> <p>Since this is a Mask class, it will only accept images with pixel types compatible with label images (typically sitk.sitkUInt8 or sitk.sitkLabelUInt8).</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, str] | None = None\n) -&gt; \"Mask\":\n    \"\"\"Create a Mask from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the mask image file\n    metadata : dict[str, str] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    Mask\n        A new Mask instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - All keys filtered by MedImage.from_file\n    - Additionally, any key starting with \"mask.\" (mask shape statistics)\n\n    Since this is a Mask class, it will only accept images with pixel types\n    compatible with label images (typically sitk.sitkUInt8 or sitk.sitkLabelUInt8).\n    \"\"\"\n    # Let MedImage.from_file do most of the work\n    instance = MedImage.from_file(filepath, metadata)\n\n    # Additional filtering for mask.* keys in metadata\n    if instance.metadata:\n        instance.metadata = {\n            k: v\n            for k, v in instance.metadata.items()\n            if not k.startswith(\"mask.\")\n        }\n\n    # Check if the image is appropriate for a mask\n    pixel_id = instance.GetPixelID()\n    if pixel_id not in [sitk.sitkUInt8, sitk.sitkLabelUInt8]:\n        logger.warning(\n            f\"Image loaded has pixel type {sitk.GetPixelIDValueAsString(pixel_id)} \"\n            f\"which may not be appropriate for a mask. Consider converting to UInt8.\"\n        )\n\n    if not isinstance(instance, Mask):\n        # If the instance is not a Mask, convert it to one\n        instance = cls(\n            image=instance,\n            metadata=instance.metadata.copy(),\n        )\n\n    return instance\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.get_label_bounding_box","title":"get_label_bounding_box","text":"<pre><code>get_label_bounding_box() -&gt; (\n    imgtools.coretypes.box.RegionBox\n)\n</code></pre> <p>Get bounding box around a label image for a given label or name.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>Bounding box around non-zero voxels in the label image. Contains min and max coordinates and size.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def get_label_bounding_box(\n    self,\n) -&gt; RegionBox:\n    \"\"\"Get bounding box around a label image for a given label or name.\n\n    Returns\n    -------\n    RegionBox\n        Bounding box around non-zero voxels in the label image.\n        Contains min and max coordinates and size.\n    \"\"\"\n    return RegionBox.from_mask_bbox(self, label=self._label_value)\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.Mask.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.TooManyComponentsError","title":"TooManyComponentsError","text":"<pre><code>TooManyComponentsError(\n    n_components: int, max_supported: int = 32\n)\n</code></pre> <p>               Bases: <code>ValueError</code></p> <p>Raised when attempting to encode a mask with more components than supported by the available integer types.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def __init__(self, n_components: int, max_supported: int = 32) -&gt; None:\n    msg = (\n        f\"Cannot encode masks with {n_components} components: \"\n        f\"maximum supported is {max_supported} due to bitmask size limits.\"\n    )\n    super().__init__(msg)\n    self.n_components = n_components\n    self.max_supported = max_supported\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask","title":"VectorMask","text":"<pre><code>VectorMask(\n    image: SimpleITK.Image,\n    roi_mapping: dict[\n        int, imgtools.coretypes.base_masks.ROIMaskMapping\n    ],\n    metadata: dict[str, str],\n    errors: typing.Mapping[str, Exception] | None = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.MedImage</code></p> <p>A multi-label binary mask image with vector pixels (sitkVectorUInt8).</p> <p>This class represents 3D multi-label mask images stored as SimpleITK vector images. Each voxel in the image contains a vector of values, where each component in the vector represents a binary indicator (0 or 1) for a specific label/ROI.</p> <p>The VectorMask design supports non-overlapping and overlapping segmentations, preserving metadata from the RTSTRUCT/SEG DICOM file. The dimensionality (3D) is inherited from the reference image which the RTSTR/SEG was constructed from.</p> <p>Background is handled automatically: - Index 0 is always reserved for the background, calculated as the     absence of any ROI across all components of the vectors. - When extracting an ROI from the mask, the background is handled implicitly</p> Properties <p>n_masks : int     Number of binary mask channels (components per voxel)     Does not include the background channel roi_keys : list[str]     List of ROI keys from mapping, excluding the background</p> <p>Attributes:</p> Name Type Description <code>roi_mapping</code> <code>dict[int, imgtools.coretypes.base_masks.ROIMaskMapping]</code> <p>Mapping from integer indices to ROIMaskMapping objects</p> <code>metadata</code> <code>dict[str, str]</code> <p>Dictionary containing metadata about the mask</p> <code>errors</code> <code>dict[str, Exception] | None</code> <p>Dictionary with error messages from ROI extraction, if any</p> <p>Methods:</p> Name Description <code>iter_masks</code> <p>Yield (index, roi_key, roi_names, Mask) for each mask channel</p> <code>has_overlap</code> <p>Return True if any voxel has &gt;1 mask. Determined by summing each voxel's vector components and checking if &gt;1</p> <code>extract_mask</code> <p>Extract a single binary mask by index or ROI key Output will have a single label == 1, output type is <code>sitk.sitkUInt8</code></p> <code>__getitem__</code> <p>Allow accessing masks via indexing First tries to <code>extract_mask(key)</code> using the given key If that fails, falls back to the standard sitk.Image behavior</p> <p>image : sitk.Image     A SimpleITK image with pixel type sitk.sitkVectorUInt8 roi_mapping : dict[int, ROIMaskMapping]     Mapping from integer indices to ROIMaskMapping objects     containing roi_key and roi_names metadata : dict[str, str]     Dictionary containing metadata about the mask errors : dict[str, Exception] | None, optional     Optional dictionary with error messages from ROI extraction, by default None</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def __init__(\n    self,\n    image: sitk.Image,\n    roi_mapping: dict[int, ROIMaskMapping],\n    metadata: dict[str, str],\n    errors: Mapping[str, Exception] | None = None,\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    image : sitk.Image\n        A SimpleITK image with pixel type sitk.sitkVectorUInt8\n    roi_mapping : dict[int, ROIMaskMapping]\n        Mapping from integer indices to ROIMaskMapping objects\n        containing roi_key and roi_names\n    metadata : dict[str, str]\n        Dictionary containing metadata about the mask\n    errors : dict[str, Exception] | None, optional\n        Optional dictionary with error messages from ROI extraction, by default None\n    \"\"\"\n    super().__init__(image)\n    # Shift index to start from 1 for user-facing keys\n    self.roi_mapping = {}\n\n    if 0 in roi_mapping and roi_mapping[0].roi_key == \"Background\":\n        # Background is already set to 0, no need to change\n        self.roi_mapping = roi_mapping\n    elif 0 in roi_mapping:\n        # Background is not set to 0, so we need to adjust the mapping\n        self.roi_mapping[0] = ROIMaskMapping(\n            \"Background\", [\"Background\"], \"Background\"\n        )\n        for old_idx, roi_mask_mapping in roi_mapping.items():\n            self.roi_mapping[old_idx + 1] = roi_mask_mapping\n    else:\n        # no background, so just set 0 and keep the rest\n        self.roi_mapping[0] = ROIMaskMapping(\n            \"Background\", [\"Background\"], \"Background\"\n        )\n        for old_idx, roi_mask_mapping in roi_mapping.items():\n            self.roi_mapping[old_idx] = roi_mask_mapping\n\n    self.metadata = metadata\n    self.errors = errors\n    self._mask_cache = {}\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.n_masks","title":"n_masks  <code>property</code>","text":"<pre><code>n_masks: int\n</code></pre> <p>Number of binary mask channels (components per voxel).</p> Notes <p>This does not include the background channel. The background channel is always the first channel (index 0).</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.roi_keys","title":"roi_keys  <code>property</code>","text":"<pre><code>roi_keys: list[str]\n</code></pre> <p>List of ROI keys from mapping</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.extract_mask","title":"extract_mask","text":"<pre><code>extract_mask(\n    key: str | int,\n) -&gt; imgtools.coretypes.base_masks.Mask\n</code></pre> <p>Extract a single binary mask by index or ROI key.</p> <p>Result would have a single label == 1 , output type is <code>sitk.sitkUInt8</code>. The mask is cached after first extraction for improved performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; roi_mapping = {\n...     0: ROIMaskMapping(\"Background\", [\"bg\"]),\n...     1: ROIMaskMapping(\"Tumor\", [\"tumor\"]),\n...     2: ROIMaskMapping(\"Lung\", [\"lung\"]),\n... }\n&gt;&gt;&gt; vector_mask = VectorMask(image, roi_mapping)\n&gt;&gt;&gt; mask = vector_mask.extract_mask(1)\n# gets the mask for Tumor\n&gt;&gt;&gt; mask = vector_mask.extract_mask(\"Lung\")\n# gets the mask for Lung\n&gt;&gt;&gt; mask = vector_mask.extract_mask(0)\n# gets the mask for Background\n</code></pre> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def extract_mask(self, key: str | int) -&gt; Mask:\n    \"\"\"Extract a single binary mask by index or ROI key.\n\n    Result would have a single label == 1 , output type is `sitk.sitkUInt8`.\n    The mask is cached after first extraction for improved performance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; roi_mapping = {\n    ...     0: ROIMaskMapping(\"Background\", [\"bg\"]),\n    ...     1: ROIMaskMapping(\"Tumor\", [\"tumor\"]),\n    ...     2: ROIMaskMapping(\"Lung\", [\"lung\"]),\n    ... }\n    &gt;&gt;&gt; vector_mask = VectorMask(image, roi_mapping)\n    &gt;&gt;&gt; mask = vector_mask.extract_mask(1)\n    # gets the mask for Tumor\n    &gt;&gt;&gt; mask = vector_mask.extract_mask(\"Lung\")\n    # gets the mask for Lung\n    &gt;&gt;&gt; mask = vector_mask.extract_mask(0)\n    # gets the mask for Background\n    \"\"\"\n    # Check if the mask is already in the cache\n    if key in self._mask_cache:\n        logger.debug(f\"Cache hit for mask {key}\")\n        return self._mask_cache[key]\n\n    mask_metadata = (\n        self.metadata.copy()\n    )  # Copy the metadata from vector mask\n\n    match key:\n        case int(idx) if idx &gt; self.n_masks or idx &lt; 0:\n            msg = f\"Index {idx} out of bounds for {self.n_masks=} masks.\"\n            raise IndexError(msg)\n        case int(0) | str(\"Background\"):\n            arr = sitk.GetArrayViewFromImage(self)\n            # create binary image where background is 1 and all others are 0\n            mask_image = sitk.GetImageFromArray(\n                (arr.sum(-1) == 0).astype(np.uint8)\n            )\n\n            # Update metadata with ROINames\n            mask_metadata[\"ROINames\"] = \"Background\"\n        case int(idx):\n            mask_image = sitk.VectorIndexSelectionCast(self, idx - 1)\n            # Update metadata with ROINames if mapping exists\n            mask_metadata[\"ROINames\"] = \"|\".join(\n                self.roi_mapping[idx].roi_names\n            )\n        case str(key_str):\n            if key_str not in self.roi_keys:\n                msg = f\"Key '{key_str}' not found in mapping\"\n                msg += f\" {self.roi_mapping=}\"\n                raise KeyError(msg)\n\n            # note: background is bypassed here automatically!\n            idx = self.roi_keys.index(key_str)\n            mask_image = sitk.VectorIndexSelectionCast(self, idx)\n\n            # Get the corresponding mapping entry and update ROINames\n            mask_metadata[\"ROINames\"] = \"|\".join(\n                self.roi_mapping[idx].roi_names\n            )\n        case _:\n            msg = (\n                f\"Invalid key type {type(key)=} where {key=}. \"\n                \"Expected int or str.\"\n            )\n            raise TypeError(msg)\n\n    mask = Mask(\n        mask_image,\n        metadata=mask_metadata,\n    )\n    self._mask_cache[key] = mask\n    return mask\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; \"MedImage\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_medimage.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, Any] | None = None\n) -&gt; \"MedImage\":\n    \"\"\"Create a MedImage from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the image file\n    metadata : dict[str, Any] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    MedImage\n        A new MedImage instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - class, hash, size, ndim, nvoxels, spacing, origin, direction\n    - min, max, sum, mean, std, variance\n    - dtype_str, dtype_numpy\n    \"\"\"\n    # Load the image file\n    image = sitk.ReadImage(filepath)\n\n    # Create instance of the class (MedImage or a subclass)\n    instance = cls(image)\n\n    if not metadata:\n        return instance\n\n    # Process metadata if provided\n    # Define fingerprint keys to filter out\n    fingerprint_keys = {\n        \"class\", \"hash\", \"size\", \"ndim\", \"nvoxels\", \"spacing\", \n        \"origin\", \"direction\", \"min\", \"max\", \"sum\", \"mean\", \"std\", \n        \"variance\", \"dtype_str\", \"dtype_numpy\"\n    }  # fmt: skip\n    # Filter metadata to exclude fingerprint keys\n    instance.metadata = {\n        k: v for k, v in metadata.items() if k not in fingerprint_keys\n    }\n    return instance\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.from_rtstruct","title":"from_rtstruct  <code>classmethod</code>","text":"<pre><code>from_rtstruct(\n    reference_image: imgtools.coretypes.MedImage,\n    rtstruct: imgtools.coretypes.masktypes.structureset.RTStructureSet,\n    roi_matcher: imgtools.coretypes.masktypes.structureset.ROIMatcher,\n) -&gt; imgtools.coretypes.base_masks.VectorMask | None\n</code></pre> <p>Create VectorMask from RTSTRUCT using ROI matching.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>@classmethod\ndef from_rtstruct(\n    cls,\n    reference_image: MedImage,\n    rtstruct: RTStructureSet,  # StructureSet\n    roi_matcher: ROIMatcher,\n) -&gt; VectorMask | None:\n    \"\"\"Create VectorMask from RTSTRUCT using ROI matching.\"\"\"\n    return rtstruct.get_vector_mask(\n        reference_image=reference_image,\n        roi_matcher=roi_matcher,\n    )\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.from_seg","title":"from_seg  <code>classmethod</code>","text":"<pre><code>from_seg(\n    reference_image: imgtools.coretypes.MedImage,\n    seg: imgtools.coretypes.masktypes.seg.SEG,\n    roi_matcher: imgtools.coretypes.masktypes.structureset.ROIMatcher,\n) -&gt; imgtools.coretypes.base_masks.VectorMask | None\n</code></pre> <p>Create VectorMask from SEG using ROI matching.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>@classmethod\ndef from_seg(\n    cls,\n    reference_image: MedImage,\n    seg: SEG,\n    roi_matcher: ROIMatcher,\n) -&gt; VectorMask | None:\n    \"\"\"Create VectorMask from SEG using ROI matching.\"\"\"\n    return seg.get_vector_mask(\n        reference_image=reference_image,\n        roi_matcher=roi_matcher,\n    )\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.has_overlap","title":"has_overlap","text":"<pre><code>has_overlap() -&gt; bool\n</code></pre> <p>Return True if any voxel has &gt;1 mask</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def has_overlap(self) -&gt; bool:\n    \"\"\"Return True if any voxel has &gt;1 mask\"\"\"\n    arr = sitk.GetArrayFromImage(self)\n    return self.n_masks &gt; 1 and bool(np.any(np.sum(arr, axis=-1) &gt; 1))\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.iter_masks","title":"iter_masks","text":"<pre><code>iter_masks(\n    include_background: bool = False,\n) -&gt; typing.Iterator[\n    tuple[\n        int,\n        str,\n        list[str],\n        str,\n        imgtools.coretypes.base_masks.Mask,\n    ]\n]\n</code></pre> <p>Yield (index, roi_key, roi_names, image_id, Mask) for each mask channel.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def iter_masks(\n    self, include_background: bool = False\n) -&gt; Iterator[tuple[int, str, list[str], str, Mask]]:\n    \"\"\"Yield (index, roi_key, roi_names, image_id, Mask) for each mask channel.\"\"\"\n    for i, mapping in self.roi_mapping.items():\n        if i == 0 and not include_background:\n            continue\n        yield (\n            i,\n            mapping.roi_key,\n            mapping.roi_names,\n            mapping.image_id,\n            self.extract_mask(i),\n        )\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_label_image","title":"to_label_image","text":"<pre><code>to_label_image() -&gt; imgtools.coretypes.base_masks.Mask\n</code></pre> <p>Convert the vector mask to a scalar label image with unique labels for each ROI.</p> <p>Generates a single multi-label mask where each voxel contains one integer value representing its class. This conversion only works if there are no overlapping ROIs in the vector mask.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_masks.Mask</code> <p>A multi-label mask where each original ROI is represented by a unique integer. The background is represented by 0, and each ROI gets a value from 1 to N. The output is a standard Mask with pixel type sitk.sitkLabelUInt8.</p> Notes <p>This conversion preserves all information from the original vector mask only if there are no overlaps. The mapping between label values and original ROI names is preserved in the metadata.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def to_label_image(self) -&gt; Mask:\n    \"\"\"Convert the vector mask to a scalar label image with unique labels for each ROI.\n\n    Generates a single multi-label mask where each voxel contains one integer value\n    representing its class. This conversion only works if there are no overlapping\n    ROIs in the vector mask.\n\n    Returns\n    -------\n    Mask\n        A multi-label mask where each original ROI is represented by a unique integer.\n        The background is represented by 0, and each ROI gets a value from 1 to N.\n        The output is a standard Mask with pixel type sitk.sitkLabelUInt8.\n\n    Raises\n    ------\n    ValueError\n        If the vector mask contains any overlapping regions (has_overlap() returns True).\n        In this case, a lossless conversion to a label image is not possible.\n\n    Notes\n    -----\n    This conversion preserves all information from the original vector mask only\n    if there are no overlaps. The mapping between label values and original ROI names\n    is preserved in the metadata.\n    \"\"\"\n\n    return self._to_label_array(allow_overlap=False)\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_region_mask","title":"to_region_mask","text":"<pre><code>to_region_mask() -&gt; imgtools.coretypes.base_masks.Mask\n</code></pre> <p>Encodes a VectorUInt8 image (with binary 0/1 components) into a single-channel image where each voxel value is a unique integer representing the bitmask of active components. Names are used in the lookup table.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>A VectorUInt8 image where each component is 0 or 1.</p> required <p>Returns:</p> Type Description <code>imgtools.coretypes.base_masks.Mask</code> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def to_region_mask(\n    self,\n) -&gt; Mask:\n    \"\"\"\n    Encodes a VectorUInt8 image (with binary 0/1 components) into a single-channel\n    image where each voxel value is a unique integer representing the bitmask\n    of active components. Names are used in the lookup table.\n\n    Parameters\n    ----------\n    vector_mask : sitk.Image\n        A VectorUInt8 image where each component is 0 or 1.\n\n    Returns\n    -------\n    Mask\n    \"\"\"\n    n_components = self.GetNumberOfComponentsPerPixel()\n    assert self.GetPixelID() == sitk.sitkVectorUInt8\n    assert len(self.roi_mapping) == n_components + 1  # +1 for background\n\n    if n_components &lt;= 8:\n        output_type = sitk.sitkUInt8\n    elif n_components &lt;= 16:\n        output_type = sitk.sitkUInt16\n    elif n_components &lt;= 32:\n        output_type = sitk.sitkUInt32\n    else:\n        raise TooManyComponentsError(n_components)\n\n    label_image = sitk.Image(self.GetSize(), output_type)\n    label_image.CopyInformation(self)\n\n    for i in range(n_components):\n        component = sitk.VectorIndexSelectionCast(\n            self, i, outputPixelType=output_type\n        )\n        shifted = sitk.ShiftScale(component, shift=0, scale=2**i)\n        label_image += shifted\n\n    return Mask(label_image, metadata=self.metadata.copy())\n</code></pre>"},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_region_mask(vector_mask)","title":"<code>vector_mask</code>","text":""},{"location":"reference/coretypes/base_masks/#imgtools.coretypes.base_masks.VectorMask.to_sparse_mask","title":"to_sparse_mask","text":"<pre><code>to_sparse_mask() -&gt; imgtools.coretypes.base_masks.Mask\n</code></pre> <p>Convert the vector mask to a single-channel binary mass.</p> <p>Creates a sparse representation where each voxel is assigned to exactly one class, even if there are overlaps in the original vector mask. In case of overlaps, the mask with the highest index is chosen.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_masks.Mask</code> <p>A single-channel binary mask where each voxel belongs to at most one class. The output is a standard Mask with pixel type sitk.sitkUInt8 or sitk.sitkLabelUInt8.</p> Notes <p>This is a lossy conversion when overlaps exist, as only one label can be preserved per voxel. If preserving all overlapping labels is important, keep working with the original VectorMask.</p> Source code in <code>src/imgtools/coretypes/base_masks.py</code> <pre><code>def to_sparse_mask(self) -&gt; Mask:\n    \"\"\"Convert the vector mask to a single-channel binary mass.\n\n    Creates a sparse representation where each voxel is assigned to exactly one class,\n    even if there are overlaps in the original vector mask. In case of overlaps,\n    the mask with the highest index is chosen.\n\n    Returns\n    -------\n    Mask\n        A single-channel binary mask where each voxel belongs to at most one class.\n        The output is a standard Mask with pixel type sitk.sitkUInt8 or sitk.sitkLabelUInt8.\n\n    Notes\n    -----\n    This is a lossy conversion when overlaps exist, as only one label can be\n    preserved per voxel. If preserving all overlapping labels is important,\n    keep working with the original VectorMask.\n    \"\"\"\n\n    return self._to_label_array(allow_overlap=True)\n</code></pre>"},{"location":"reference/coretypes/base_medimage/","title":"Base medimage","text":""},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage","title":"base_medimage","text":""},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage","title":"MedImage","text":"<p>               Bases: <code>SimpleITK.Image</code></p> <p>A more convenient wrapper around SimpleITK.Image.</p> <p>Extends SimpleITK.Image with additional properties and methods for medical image processing and analysis.</p> <p>Methods:</p> Name Description <code>from_file</code> <p>Create a MedImage from a file path with optional metadata.</p> <code>to_numpy</code> <p>Convert the image to a NumPy array.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; \"MedImage\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_medimage.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, Any] | None = None\n) -&gt; \"MedImage\":\n    \"\"\"Create a MedImage from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the image file\n    metadata : dict[str, Any] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    MedImage\n        A new MedImage instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - class, hash, size, ndim, nvoxels, spacing, origin, direction\n    - min, max, sum, mean, std, variance\n    - dtype_str, dtype_numpy\n    \"\"\"\n    # Load the image file\n    image = sitk.ReadImage(filepath)\n\n    # Create instance of the class (MedImage or a subclass)\n    instance = cls(image)\n\n    if not metadata:\n        return instance\n\n    # Process metadata if provided\n    # Define fingerprint keys to filter out\n    fingerprint_keys = {\n        \"class\", \"hash\", \"size\", \"ndim\", \"nvoxels\", \"spacing\", \n        \"origin\", \"direction\", \"min\", \"max\", \"sum\", \"mean\", \"std\", \n        \"variance\", \"dtype_str\", \"dtype_numpy\"\n    }  # fmt: skip\n    # Filter metadata to exclude fingerprint keys\n    instance.metadata = {\n        k: v for k, v in metadata.items() if k not in fingerprint_keys\n    }\n    return instance\n</code></pre>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/base_medimage/#imgtools.coretypes.base_medimage.MedImage.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/box/","title":"Box","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box","title":"box","text":"<p>Functions:</p> Name Description <code>calculate_image_boundaries</code> <p>Calculate boundary RegionBox of a SimpleITK image.</p>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.BoundingBoxOutsideImageError","title":"BoundingBoxOutsideImageError","text":"<pre><code>BoundingBoxOutsideImageError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when the bounding box is outside the image.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.BoxPadMethod","title":"BoxPadMethod","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enum for padding methods.</p>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox","title":"RegionBox  <code>dataclass</code>","text":"<pre><code>RegionBox(\n    min: imgtools.coretypes.spatial_types.Coordinate3D,\n    max: imgtools.coretypes.spatial_types.Coordinate3D,\n)\n</code></pre> <p>Represents a box in 3D space.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The minimum coordinate of the box.</p> <code>max</code> <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The maximum coordinate of the box.</p> <code>size</code> <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The size of the box, calculated from the min and max coordinates.</p> <p>Methods:</p> Name Description <code>check_out_of_bounds_coordinates</code> <p>Adjust the coordinates to ensure that the max values are not greater than the image size.</p> <code>copy</code> <p>Create a copy of the RegionBox.</p> <code>crop_image</code> <p>Crop an image to the coordinates defined by the box.</p> <code>crop_image_and_mask</code> <p>Crop an image and mask to the coordinates defined by the box.</p> <code>expand_to_cube</code> <p>Convert the bounding box to a cube by making the size equal along all dimensions.</p> <code>expand_to_min_size</code> <p>Ensure that the bounding box has a minimum size along each dimension.</p> <code>from_mask_bbox</code> <p>Creates a RegionBox from the bounding box of a mask image.</p> <code>from_mask_centroid</code> <p>Creates a RegionBox from the centroid of a mask image.</p> <code>from_tuple</code> <p>Creates a RegionBox from a tuple of min and max coordinates.</p> <code>pad</code> <p>Expand the bounding box by a specified padding value.</p>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.check_out_of_bounds_coordinates","title":"check_out_of_bounds_coordinates","text":"<pre><code>check_out_of_bounds_coordinates(\n    image: SimpleITK.Image,\n) -&gt; None\n</code></pre> <p>Adjust the coordinates to ensure that the max values are not greater than the image size.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def check_out_of_bounds_coordinates(self, image: sitk.Image) -&gt; None:\n    \"\"\"Adjust the coordinates to ensure that the max values are not greater than the image size.\"\"\"\n    # if any of the max values are greater than the image size, set them to the image size,\n    # and subtract the difference from the min values\n    for idx, axis in enumerate([\"x\", \"y\", \"z\"]):\n        max_value = getattr(self.max, axis)\n        image_size = image.GetSize()[idx]\n        if max_value &gt; image_size:\n            logger.debug(\n                f\"Adjusting box {axis} coordinates to be within the image size.\"\n            )\n            diff = max_value - image_size\n            setattr(self.min, axis, getattr(self.min, axis) - diff)\n            setattr(self.max, axis, image_size)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.copy","title":"copy","text":"<pre><code>copy() -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Create a copy of the RegionBox.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def copy(self) -&gt; RegionBox:\n    \"\"\"Create a copy of the RegionBox.\"\"\"\n    return RegionBox(self.min, self.max)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.crop_image","title":"crop_image","text":"<pre><code>crop_image(image: SimpleITK.Image) -&gt; SimpleITK.Image\n</code></pre> <p>Crop an image to the coordinates defined by the box.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The image to crop.</p> required <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The cropped image.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def crop_image(self, image: sitk.Image) -&gt; sitk.Image:\n    \"\"\"Crop an image to the coordinates defined by the box.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The image to crop.\n\n    Returns\n    -------\n    sitk.Image\n        The cropped image.\n    \"\"\"\n    try:\n        self.check_out_of_bounds_coordinates(image)\n        cropped_image = sitk.RegionOfInterest(image, self.size, self.min)\n    except Exception as e:\n        msg = f\"Error cropping image to the box: {e}\"\n        logger.exception(msg)\n        raise e\n    else:\n        return cropped_image\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.crop_image(image)","title":"<code>image</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.crop_image_and_mask","title":"crop_image_and_mask","text":"<pre><code>crop_image_and_mask(\n    image: SimpleITK.Image, mask: SimpleITK.Image\n) -&gt; tuple[SimpleITK.Image, SimpleITK.Image]\n</code></pre> <p>Crop an image and mask to the coordinates defined by the box.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The image to crop.</p> required <code>SimpleITK.Image</code> <p>The mask to crop.</p> required <p>Returns:</p> Type Description <code>tuple[SimpleITK.Image, SimpleITK.Image]</code> <p>The cropped image and mask.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def crop_image_and_mask(\n    self, image: sitk.Image, mask: sitk.Image\n) -&gt; tuple[sitk.Image, sitk.Image]:\n    \"\"\"Crop an image and mask to the coordinates defined by the box.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The image to crop.\n    mask : sitk.Image\n        The mask to crop.\n\n    Returns\n    -------\n    tuple[sitk.Image, sitk.Image]\n        The cropped image and mask.\n    \"\"\"\n    cropped_image = self.crop_image(image)\n    cropped_mask = self.crop_image(mask)\n\n    return cropped_image, cropped_mask\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.crop_image_and_mask(image)","title":"<code>image</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.crop_image_and_mask(mask)","title":"<code>mask</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.expand_to_cube","title":"expand_to_cube","text":"<pre><code>expand_to_cube(\n    desired_size: int | None = None,\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Convert the bounding box to a cube by making the size equal along all dimensions.</p> <p>This is done by finding which dimension is the largest, and then pad the other dimensions to make them equal to the desired size.</p> <p>Parameters:</p> Name Type Description Default <code>int | None</code> <p>The desired size of the cube. If None, the maximum dimension size is used</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>The bounding box converted to a cube.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def expand_to_cube(self, desired_size: int | None = None) -&gt; RegionBox:\n    \"\"\"Convert the bounding box to a cube by making the size equal along all dimensions.\n\n    This is done by finding which dimension is the largest,\n    and then pad the other dimensions to make them equal to the desired size.\n\n    Parameters\n    ----------\n    desired_size : int | None\n        The desired size of the cube. If None, the maximum dimension size is used\n\n    Returns\n    -------\n    RegionBox\n        The bounding box converted to a cube.\n\n    Raises\n    ------\n    ValueError\n        If the desired size is smaller than the current maximum dimension size.\n    \"\"\"\n    max_size = max(self.size)\n\n    if not desired_size:\n        return self.expand_to_min_size(max_size)\n\n    if desired_size &lt; max_size:\n        msg = (\n            f\"Desired size {desired_size} is smaller than\"\n            f\" the current maximum dimension size {max_size}.\"\n        )\n        raise ValueError(msg)\n\n    return self.expand_to_min_size(desired_size)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.expand_to_cube(desired_size)","title":"<code>desired_size</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.expand_to_min_size","title":"expand_to_min_size","text":"<pre><code>expand_to_min_size(\n    size: int = 5,\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Ensure that the bounding box has a minimum size along each dimension.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The minimum size of the bounding box along each dimension.</p> <code>5</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>The bounding box with a minimum size along each dimension.</p> Notes <p>Validation is done to ensure that any min coordinates that are negative are set to 0, and the difference is added to the maximum coordinates.</p> <p>If an extra dimension is not an integer (e.g. 1.5), the bounding box is shifted 1 voxel towards the minimum in that dimension.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def expand_to_min_size(self, size: int = 5) -&gt; RegionBox:\n    \"\"\"Ensure that the bounding box has a minimum size along each dimension.\n\n    Parameters\n    ----------\n    size : int\n        The minimum size of the bounding box along each dimension.\n\n    Returns\n    -------\n    RegionBox\n        The bounding box with a minimum size along each dimension.\n\n    Notes\n    -----\n    Validation is done to ensure that any min coordinates that are negative are set to 0,\n    and the difference is added to the maximum coordinates.\n\n    If an extra dimension is not an integer (e.g. 1.5), the bounding box is shifted 1 voxel towards the minimum in that dimension.\n    \"\"\"\n    # Calculate extra dimensions to add to the existing coordinates\n    extra_x = max(0, size - self.size.width) / 2\n    extra_y = max(0, size - self.size.height) / 2\n    extra_z = max(0, size - self.size.depth) / 2\n\n    # Round extra dimension values UP to the nearest integer before adding to existing minimum coordinates\n    min_coord = self.min - tuple(\n        [math.ceil(extra) for extra in (extra_x, extra_y, extra_z)]\n    )\n\n    # Round extra dimensions DOWN to the nearest integer before adding to existing maximum coordinates\n    max_coord = self.max + tuple(\n        [math.floor(extra) for extra in (extra_x, extra_y, extra_z)]\n    )\n\n    # Adjust negative coordinates to ensure that the min values are not negative\n    self._adjust_negative_coordinates(min_coord, max_coord)\n\n    return RegionBox(min=min_coord, max=max_coord)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.expand_to_min_size(size)","title":"<code>size</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_bbox","title":"from_mask_bbox  <code>classmethod</code>","text":"<pre><code>from_mask_bbox(\n    mask: SimpleITK.Image, label: int = 1\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Creates a RegionBox from the bounding box of a mask image.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The input mask image.</p> required <code>int</code> <code>1</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>The bounding box coordinates as a RegionBox object.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>@classmethod\ndef from_mask_bbox(cls, mask: sitk.Image, label: int = 1) -&gt; RegionBox:\n    \"\"\"Creates a RegionBox from the bounding box of a mask image.\n\n    Parameters\n    ----------\n    mask : sitk.Image\n        The input mask image.\n    label : int, optional\n\n    Returns\n    -------\n    RegionBox\n        The bounding box coordinates as a RegionBox object.\n    \"\"\"\n\n    mask_uint = sitk.Cast(mask, sitk.sitkUInt8)\n    stats = sitk.LabelShapeStatisticsImageFilter()\n    stats.Execute(mask_uint)\n    xstart, ystart, zstart, xsize, ysize, zsize = stats.GetBoundingBox(\n        label\n    )\n\n    return RegionBox(\n        Coordinate3D(xstart, ystart, zstart),\n        Coordinate3D(xstart + xsize, ystart + ysize, zstart + zsize),\n    )\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_bbox(mask)","title":"<code>mask</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_bbox(label)","title":"<code>label</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_centroid","title":"from_mask_centroid  <code>classmethod</code>","text":"<pre><code>from_mask_centroid(\n    mask: SimpleITK.Image,\n    label: int = 1,\n    desired_size: int | None = None,\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Creates a RegionBox from the centroid of a mask image.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The input mask image.</p> required <code>int</code> <p>label in the mask image to calculate the centroid.</p> <code>1</code> <code>int | None</code> <p>The desired size of the box. If None, the minimum size default from <code>expand_to_min_size</code> is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>The bounding box coordinates as a RegionBox object.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>@classmethod\ndef from_mask_centroid(\n    cls, mask: sitk.Image, label: int = 1, desired_size: int | None = None\n) -&gt; RegionBox:\n    \"\"\"Creates a RegionBox from the centroid of a mask image.\n\n    Parameters\n    ----------\n    mask : sitk.Image\n        The input mask image.\n    label : int, optional\n        label in the mask image to calculate the centroid.\n    desired_size : int | None, optional\n        The desired size of the box. If None, the minimum size default from `expand_to_min_size` is used.\n\n    Returns\n    -------\n    RegionBox\n        The bounding box coordinates as a RegionBox object.\n    \"\"\"\n    mask_uint = sitk.Cast(mask, sitk.sitkUInt8)\n    stats = sitk.LabelShapeStatisticsImageFilter()\n    stats.Execute(mask_uint)\n\n    centroid = stats.GetCentroid(label)\n    centroid_idx = mask.TransformPhysicalPointToIndex(centroid)\n\n    return RegionBox(\n        Coordinate3D(*centroid_idx), Coordinate3D(*centroid_idx)\n    ).expand_to_cube(desired_size)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_centroid(mask)","title":"<code>mask</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_centroid(label)","title":"<code>label</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_mask_centroid(desired_size)","title":"<code>desired_size</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.from_tuple","title":"from_tuple  <code>classmethod</code>","text":"<pre><code>from_tuple(\n    coordmin: tuple[int, int, int],\n    coordmax: tuple[int, int, int],\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Creates a RegionBox from a tuple of min and max coordinates.</p> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>@classmethod\ndef from_tuple(\n    cls, coordmin: tuple[int, int, int], coordmax: tuple[int, int, int]\n) -&gt; RegionBox:\n    \"\"\"Creates a RegionBox from a tuple of min and max coordinates.\"\"\"\n    return cls(Coordinate3D(*coordmin), Coordinate3D(*coordmax))\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.pad","title":"pad","text":"<pre><code>pad(\n    padding: int,\n    method: imgtools.coretypes.box.BoxPadMethod = imgtools.coretypes.box.BoxPadMethod.SYMMETRIC,\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Expand the bounding box by a specified padding value.</p> <p>Can be applied symmetrically on both sides or only at the end of the box. If the padded result has negative coordinates, they region is adjusted by shifting the min coordinates to 0 and adding the difference to the max coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The padding value to expand the bounding box.</p> required <code>imgtools.coretypes.box.BoxPadMethod</code> <p>The padding method to use. Default is BoxPadMethod.SYMMETRIC. Options are: - BoxPadMethod.SYMMETRIC: Pad symmetrically on both sides. - BoxPadMethod.END: Pad only at the end of the box (furthest from the origin).</p> <code>imgtools.coretypes.box.BoxPadMethod.SYMMETRIC</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>The expanded bounding box.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; box = RegionBox(\n...     Coordinate3D(5, 5, 5),\n...     Coordinate3D(10, 10, 10),\n... )\n&gt;&gt;&gt; box.pad(5)\nRegionBox(\n... min=Coordinate3D(x=0, y=0, z=0),\n... max=Coordinate3D(x=15, y=15, z=15)\n... size=(15, 15, 15)\n)\n&gt;&gt;&gt; box.pad(5, method=BoxPadMethod.END)\nRegionBox(\n... min=Coordinate3D(x=5, y=5, z=5),\n... max=Coordinate3D(x=15, y=15, z=15)\n... size=(10, 10, 10)\n)\n</code></pre> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def pad(\n    self, padding: int, method: BoxPadMethod = BoxPadMethod.SYMMETRIC\n) -&gt; RegionBox:\n    \"\"\"Expand the bounding box by a specified padding value.\n\n    Can be applied symmetrically on both sides or only at the end of the box.\n    If the padded result has negative coordinates, they region is adjusted by\n    shifting the min coordinates to 0 and adding the difference to the max coordinates.\n\n    Parameters\n    ----------\n    padding : int\n        The padding value to expand the bounding box.\n    method : BoxPadMethod, optional\n        The padding method to use. Default is BoxPadMethod.SYMMETRIC.\n        Options are:\n        - BoxPadMethod.SYMMETRIC: Pad symmetrically on both sides.\n        - BoxPadMethod.END: Pad only at the end of the box (furthest from the origin).\n\n    Returns\n    -------\n    RegionBox\n        The expanded bounding box.\n\n    Examples\n    --------\n    &gt;&gt;&gt; box = RegionBox(\n    ...     Coordinate3D(5, 5, 5),\n    ...     Coordinate3D(10, 10, 10),\n    ... )\n    &gt;&gt;&gt; box.pad(5)\n    RegionBox(\n    ... min=Coordinate3D(x=0, y=0, z=0),\n    ... max=Coordinate3D(x=15, y=15, z=15)\n    ... size=(15, 15, 15)\n    )\n    &gt;&gt;&gt; box.pad(5, method=BoxPadMethod.END)\n    RegionBox(\n    ... min=Coordinate3D(x=5, y=5, z=5),\n    ... max=Coordinate3D(x=15, y=15, z=15)\n    ... size=(10, 10, 10)\n    )\n    \"\"\"\n    if padding == 0:\n        return self\n\n    match method:\n        case BoxPadMethod.SYMMETRIC:\n            padded_min = self.min - padding\n            padded_max = self.max + padding\n        case BoxPadMethod.END:\n            padded_min = self.min\n            padded_max = self.max + padding\n        case _:\n            errmsg = f\"Invalid padding method: {method}\"\n            errmsg += f\" Options are: {BoxPadMethod.__members__.keys()}\"\n            raise ValueError(errmsg)\n\n    self._adjust_negative_coordinates(padded_min, padded_max)\n\n    return RegionBox(min=padded_min, max=padded_max)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.pad(padding)","title":"<code>padding</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.RegionBox.pad(method)","title":"<code>method</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.calculate_image_boundaries","title":"calculate_image_boundaries","text":"<pre><code>calculate_image_boundaries(\n    image: SimpleITK.Image,\n    use_world_coordinates: bool = False,\n) -&gt; imgtools.coretypes.box.RegionBox\n</code></pre> <p>Calculate boundary RegionBox of a SimpleITK image.</p> <p>Calculates the origin and size of the image in either index or world coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The input SimpleITK image.</p> required <code>bool</code> <p>If True, the origin and size are calculated in world coordinates. Meant to be used internally and for debugging purposes. Use with caution as it may not work as the resulting RegionBox may not work as expected.</p> <code>False</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.box.RegionBox</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculate_image_boundaries(image)\nRegionBox(\n    min=Coordinate3D(x=0, y=0, z=0),\n    max=Coordinate3D(x=512, y=512, z=135)\n    size=Size3D(w=512, h=512, d=135)\n)\n</code></pre> Source code in <code>src/imgtools/coretypes/box.py</code> <pre><code>def calculate_image_boundaries(\n    image: sitk.Image, use_world_coordinates: bool = False\n) -&gt; RegionBox:\n    \"\"\"Calculate boundary RegionBox of a SimpleITK image.\n\n    Calculates the origin and size of the image in either index or world coordinates.\n\n    Parameters\n    ----------\n    image: sitk.Image\n        The input SimpleITK image.\n    use_world_coordinates: bool, optional\n        If True, the origin and size are calculated in world coordinates.\n        Meant to be used internally and for debugging purposes.\n        Use with caution as it may not work as the resulting RegionBox\n        may not work as expected.\n\n    Returns\n    -------\n    RegionBox\n\n    Examples\n    --------\n    &gt;&gt;&gt; calculate_image_boundaries(image)\n    RegionBox(\n        min=Coordinate3D(x=0, y=0, z=0),\n        max=Coordinate3D(x=512, y=512, z=135)\n        size=Size3D(w=512, h=512, d=135)\n    )\n    \"\"\"\n\n    if use_world_coordinates:\n        min_coord = Coordinate3D(*image.GetOrigin())\n        size = Size3D(*image.GetSize())\n\n    else:\n        min_coord = Coordinate3D(\n            *image.TransformPhysicalPointToIndex(image.GetOrigin())\n        )\n        size = Size3D(*image.GetSize())\n\n    return RegionBox(min_coord, min_coord + size)\n</code></pre>"},{"location":"reference/coretypes/box/#imgtools.coretypes.box.calculate_image_boundaries(image)","title":"<code>image</code>","text":""},{"location":"reference/coretypes/box/#imgtools.coretypes.box.calculate_image_boundaries(use_world_coordinates)","title":"<code>use_world_coordinates</code>","text":""},{"location":"reference/coretypes/imagetypes/dose/","title":"Dose","text":""},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose","title":"dose","text":""},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose","title":"Dose  <code>dataclass</code>","text":"<pre><code>Dose(\n    image: SimpleITK.Image, metadata: typing.Dict[str, str]\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.MedImage</code></p> <p>Methods:</p> Name Description <code>from_dicom</code> <p>Reads the data and returns the data frame and the image dosage in SITK format</p> <code>from_file</code> <p>Create a MedImage from a file path with optional metadata.</p> <code>to_numpy</code> <p>Convert the image to a NumPy array.</p> Source code in <code>src/imgtools/coretypes/imagetypes/dose.py</code> <pre><code>def __init__(self, image: sitk.Image, metadata: Dict[str, str]) -&gt; None:\n    super().__init__(image)\n    self.metadata = metadata\n</code></pre>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.from_dicom","title":"from_dicom  <code>classmethod</code>","text":"<pre><code>from_dicom(\n    path: str,\n    series_id: str | None = None,\n    file_names: list[str] | None = None,\n    **kwargs: typing.Any\n) -&gt; imgtools.coretypes.imagetypes.dose.Dose\n</code></pre> <p>Reads the data and returns the data frame and the image dosage in SITK format</p> Source code in <code>src/imgtools/coretypes/imagetypes/dose.py</code> <pre><code>@classmethod\ndef from_dicom(\n    cls,\n    path: str,\n    series_id: str | None = None,\n    file_names: list[str] | None = None,\n    **kwargs: Any,  # type: ignore # noqa\n) -&gt; Dose:\n    \"\"\"\n    Reads the data and returns the data frame and the image dosage in SITK format\n    \"\"\"\n    dose, metadata = read_dicom_series(\n        path,\n        series_id=series_id,\n        file_names=file_names,\n        **kwargs,\n    )\n\n    # if 4D, make 3D\n    if dose.GetDimension() == 4:\n        dose = dose[:, :, :, 0]\n\n    # Get the metadata\n    df = dcmread(path)\n\n    # Convert to SUV\n    factor = float(df.DoseGridScaling)\n    img_dose = sitk.Cast(dose, sitk.sitkFloat32)\n    img_dose = img_dose * factor\n\n    metadata.update(\n        {\n            \"DoseGridScaling\": str(factor),\n            \"DoseUnits\": str(df.DoseUnits),\n            \"DoseType\": str(df.DoseType),\n        }\n    )\n\n    return cls(img_dose, metadata)\n</code></pre>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; \"MedImage\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_medimage.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, Any] | None = None\n) -&gt; \"MedImage\":\n    \"\"\"Create a MedImage from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the image file\n    metadata : dict[str, Any] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    MedImage\n        A new MedImage instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - class, hash, size, ndim, nvoxels, spacing, origin, direction\n    - min, max, sum, mean, std, variance\n    - dtype_str, dtype_numpy\n    \"\"\"\n    # Load the image file\n    image = sitk.ReadImage(filepath)\n\n    # Create instance of the class (MedImage or a subclass)\n    instance = cls(image)\n\n    if not metadata:\n        return instance\n\n    # Process metadata if provided\n    # Define fingerprint keys to filter out\n    fingerprint_keys = {\n        \"class\", \"hash\", \"size\", \"ndim\", \"nvoxels\", \"spacing\", \n        \"origin\", \"direction\", \"min\", \"max\", \"sum\", \"mean\", \"std\", \n        \"variance\", \"dtype_str\", \"dtype_numpy\"\n    }  # fmt: skip\n    # Filter metadata to exclude fingerprint keys\n    instance.metadata = {\n        k: v for k, v in metadata.items() if k not in fingerprint_keys\n    }\n    return instance\n</code></pre>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/imagetypes/dose/#imgtools.coretypes.imagetypes.dose.Dose.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/imagetypes/pet/","title":"Pet","text":""},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet","title":"pet","text":""},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET","title":"PET  <code>dataclass</code>","text":"<pre><code>PET(\n    image: SimpleITK.Image, metadata: typing.Dict[str, str]\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.MedImage</code></p> <p>Methods:</p> Name Description <code>from_dicom</code> <p>Read the PET scan and returns the data frame and the image dosage in SITK format</p> <code>from_file</code> <p>Create a MedImage from a file path with optional metadata.</p> <code>to_numpy</code> <p>Convert the image to a NumPy array.</p> Source code in <code>src/imgtools/coretypes/imagetypes/pet.py</code> <pre><code>def __init__(self, image: sitk.Image, metadata: Dict[str, str]) -&gt; None:\n    super().__init__(image)\n    self.metadata = metadata\n</code></pre>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.from_dicom","title":"from_dicom  <code>classmethod</code>","text":"<pre><code>from_dicom(\n    path: str,\n    series_id: typing.Optional[str] = None,\n    recursive: bool = False,\n    file_names: typing.Optional[list[str]] = None,\n    pet_image_type: imgtools.coretypes.imagetypes.pet.PETImageType = imgtools.coretypes.imagetypes.pet.PETImageType.SUV,\n    **kwargs: typing.Any\n) -&gt; imgtools.coretypes.imagetypes.pet.PET\n</code></pre> <p>Read the PET scan and returns the data frame and the image dosage in SITK format</p> <p>There are two types of existing formats which has to be mentioned in the type type:     SUV: gets the image with each pixel value having SUV value     ACT: gets the image with each pixel value having activity concentration SUV = Activity concenteration/(Injected dose quantity/Body weight)</p> <p>Please refer to the pseudocode: https://qibawiki.rsna.org/index.php/Standardized_Uptake_Value_(SUV) If there is no data on SUV/ACT then backup calculation is done based on the formula in the documentation, although, it may have some error.</p> Source code in <code>src/imgtools/coretypes/imagetypes/pet.py</code> <pre><code>@classmethod\ndef from_dicom(\n    cls,\n    path: str,\n    series_id: Optional[str] = None,\n    recursive: bool = False,\n    file_names: Optional[list[str]] = None,\n    pet_image_type: PETImageType = PETImageType.SUV,\n    **kwargs: Any,  # noqa\n) -&gt; PET:\n    \"\"\"Read the PET scan and returns the data frame and the image dosage in SITK format\n\n    There are two types of existing formats which has to be mentioned in the type\n    type:\n        SUV: gets the image with each pixel value having SUV value\n        ACT: gets the image with each pixel value having activity concentration\n    SUV = Activity concenteration/(Injected dose quantity/Body weight)\n\n    Please refer to the pseudocode: https://qibawiki.rsna.org/index.php/Standardized_Uptake_Value_(SUV)\n    If there is no data on SUV/ACT then backup calculation is done based on the formula in the documentation, although, it may\n    have some error.\n    \"\"\"\n    # TODO: this logic might repetitive... idk how pet is supposed to be used\n    image, metadata = read_dicom_series(\n        path,\n        series_id=series_id,\n        recursive=recursive,\n        file_names=file_names,\n        **kwargs,\n    )\n    pet = cls(image, metadata=metadata)\n\n    img_pet: sitk.Image = sitk.Cast(pet, sitk.sitkFloat32)\n    directory_path = pathlib.Path(path)\n\n    try:\n        first_file = next(directory_path.iterdir()).as_posix()\n    except StopIteration as e:\n        msg = f\"No files found in directory: {path}\"\n        raise FileNotFoundError(msg) from e\n\n    dcm: FileDataset = dcmread(first_file)\n\n    pet_type: PETImageType = PETImageType(pet_image_type)\n\n    # Finding the Scale factor\n    try:\n        if pet_type == PETImageType.SUV:\n            factor: float = dcm.to_json_dict()[\"70531000\"][\"Value\"][0]\n        else:\n            factor = dcm.to_json_dict()[\"70531009\"][\"Value\"][0]\n    except KeyError:\n        logger.warning(\n            \"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\"\n        )\n        # factor = cls.calc_factor(dcm, pet_type)\n        factor = 1.0  # fallback to 1.0 or re-enable the calc_factor logic\n\n    # SimpleITK reads some pixel values as negative but with correct value\n    img_pet = sitk.Abs(img_pet * factor)\n\n    # metadata: Dict[str, Union[str, float, bool]] = cls.get_metadata(dcm)\n    # metadata[\"factor\"] = factor\n\n    return cls(img_pet, metadata=metadata)\n</code></pre>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; \"MedImage\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_medimage.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, Any] | None = None\n) -&gt; \"MedImage\":\n    \"\"\"Create a MedImage from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the image file\n    metadata : dict[str, Any] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    MedImage\n        A new MedImage instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - class, hash, size, ndim, nvoxels, spacing, origin, direction\n    - min, max, sum, mean, std, variance\n    - dtype_str, dtype_numpy\n    \"\"\"\n    # Load the image file\n    image = sitk.ReadImage(filepath)\n\n    # Create instance of the class (MedImage or a subclass)\n    instance = cls(image)\n\n    if not metadata:\n        return instance\n\n    # Process metadata if provided\n    # Define fingerprint keys to filter out\n    fingerprint_keys = {\n        \"class\", \"hash\", \"size\", \"ndim\", \"nvoxels\", \"spacing\", \n        \"origin\", \"direction\", \"min\", \"max\", \"sum\", \"mean\", \"std\", \n        \"variance\", \"dtype_str\", \"dtype_numpy\"\n    }  # fmt: skip\n    # Filter metadata to exclude fingerprint keys\n    instance.metadata = {\n        k: v for k, v in metadata.items() if k not in fingerprint_keys\n    }\n    return instance\n</code></pre>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PET.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/imagetypes/pet/#imgtools.coretypes.imagetypes.pet.PETImageType","title":"PETImageType","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enumeration for PET image types used in DICOM processing.</p> <p>This enumeration defines the two primary types of PET image representations: - <code>SUV</code> (Standardized Uptake Value): Represents pixel values as SUV,   calculated using the formula:   <code>SUV = Activity Concentration / (Injected Dose Quantity / Body Weight)</code>. - <code>ACT</code> (Activity Concentration): Represents pixel values as raw   activity concentrations.</p> <p>Attributes:</p> Name Type Description <code>SUV</code> <code>str</code> <p>Indicates the SUV image type.</p> <code>ACT</code> <code>str</code> <p>Indicates the activity concentration image type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image_type = PETImageType.SUV  # SUV image type\n&gt;&gt;&gt; print(image_type)\n'SUV'\n&gt;&gt;&gt; isinstance(image_type, str)\nTrue\n&gt;&gt;&gt; repr(PetImageType(\"SUV\"))\n&lt;PETImageType.SUV: 'SUV'&gt;\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/","title":"Scan","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan","title":"scan","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan","title":"Scan","text":"<pre><code>Scan(\n    image: SimpleITK.Image,\n    metadata: typing.Dict[str, typing.Any],\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.MedImage</code></p> <p>Methods:</p> Name Description <code>from_dicom</code> <p>Read a DICOM scan from a directory.</p> <code>from_file</code> <p>Create a MedImage from a file path with optional metadata.</p> <code>to_numpy</code> <p>Convert the image to a NumPy array.</p> Source code in <code>src/imgtools/coretypes/imagetypes/scan.py</code> <pre><code>def __init__(self, image: sitk.Image, metadata: Dict[str, Any]) -&gt; None:\n    super().__init__(image)\n    self.metadata = metadata\n    self._fix_direction()\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.direction","title":"direction  <code>property</code>","text":"<pre><code>direction: imgtools.coretypes.spatial_types.Direction\n</code></pre> <p>Get the direction cosine matrix for image orientation.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Direction</code> <p>The 3x3 direction matrix representing image orientation.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: int\n</code></pre> <p>Wrapper around GetPixelID.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.dtype_np","title":"dtype_np  <code>property</code>","text":"<pre><code>dtype_np: typing.Type['np.number']\n</code></pre> <p>Get the NumPy data type corresponding to the image's pixel type.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.dtype_str","title":"dtype_str  <code>property</code>","text":"<pre><code>dtype_str: str\n</code></pre> <p>Wrapper around GetPixelIDTypeAsString.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.fingerprint","title":"fingerprint  <code>property</code>","text":"<pre><code>fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get image statistics.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.geometry","title":"geometry  <code>property</code>","text":"<pre><code>geometry: imgtools.coretypes.spatial_types.ImageGeometry\n</code></pre> <p>Get a complete representation of the image geometry.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.ImageGeometry</code> <p>A dataclass containing size, origin, direction, and spacing.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Wrapper around GetDimension.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.origin","title":"origin  <code>property</code>","text":"<pre><code>origin: imgtools.coretypes.spatial_types.Coordinate3D\n</code></pre> <p>Get the physical coordinates of the first voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Coordinate3D</code> <p>The physical coordinates (x, y, z) of the origin.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.serialized_fingerprint","title":"serialized_fingerprint  <code>property</code>","text":"<pre><code>serialized_fingerprint: dict[str, typing.Any]\n</code></pre> <p>Get a serialized version of the image fingerprint with primitive types.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Any]</code> <p>A dictionary with serialized image metadata that can be easily converted to JSON or other serialization formats.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.size","title":"size  <code>property</code>","text":"<pre><code>size: imgtools.coretypes.spatial_types.Size3D\n</code></pre> <p>Get the size of the image in voxels.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Size3D</code> <p>The dimensions of the image (width, height, depth).</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.spacing","title":"spacing  <code>property</code>","text":"<pre><code>spacing: imgtools.coretypes.spatial_types.Spacing3D\n</code></pre> <p>Get the physical size of each voxel.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.Spacing3D</code> <p>The spacing between voxels in physical units.</p>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom","title":"from_dicom  <code>classmethod</code>","text":"<pre><code>from_dicom(\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: typing.Any\n) -&gt; imgtools.coretypes.imagetypes.scan.Scan\n</code></pre> <p>Read a DICOM scan from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the directory containing the DICOM files.</p> required <code>str | None</code> <p>Series ID to read, by default None</p> <code>None</code> <code>bool</code> <p>Whether to read the files recursively, by default False</p> <code>False</code> <code>list[str] | None</code> <p>List of file names to read, by default None</p> <code>None</code> <code>typing.Any</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.imagetypes.scan.Scan</code> <p>The read scan.</p> Source code in <code>src/imgtools/coretypes/imagetypes/scan.py</code> <pre><code>@classmethod\ndef from_dicom(\n    cls,\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: Any,  # noqa\n) -&gt; Scan:\n    \"\"\"Read a DICOM scan from a directory.\n\n    Parameters\n    ----------\n    path : str\n        Path to the directory containing the DICOM files.\n    series_id : str | None, optional\n        Series ID to read, by default None\n    recursive : bool, optional\n        Whether to read the files recursively, by default False\n    file_names : list[str] | None, optional\n        List of file names to read, by default None\n    **kwargs : Any\n        Unused keyword arguments.\n    Returns\n    -------\n    Scan\n        The read scan.\n    \"\"\"\n    image, metadata = read_dicom_series(\n        path,\n        series_id=series_id,\n        recursive=recursive,\n        file_names=file_names,\n        **kwargs,\n    )\n\n    return cls(image, metadata)\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom(path)","title":"<code>path</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom(series_id)","title":"<code>series_id</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom(file_names)","title":"<code>file_names</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_dicom(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    filepath: str | \"Path\",\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; \"MedImage\"\n</code></pre> <p>Create a MedImage from a file path with optional metadata.</p> <p>This method filters out any fingerprint-related keys from the provided metadata.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to the image file</p> required <code>dict[str, typing.Any] | None</code> <p>Optional metadata dictionary, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.base_medimage.MedImage</code> <p>A new MedImage instance</p> Notes <p>The following fingerprint-related keys will be filtered out from metadata: - class, hash, size, ndim, nvoxels, spacing, origin, direction - min, max, sum, mean, std, variance - dtype_str, dtype_numpy</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>@classmethod\ndef from_file(\n    cls, filepath: str | \"Path\", metadata: dict[str, Any] | None = None\n) -&gt; \"MedImage\":\n    \"\"\"Create a MedImage from a file path with optional metadata.\n\n    This method filters out any fingerprint-related keys from the provided metadata.\n\n    Parameters\n    ----------\n    filepath : str | Path\n        Path to the image file\n    metadata : dict[str, Any] | None, optional\n        Optional metadata dictionary, by default None\n\n    Returns\n    -------\n    MedImage\n        A new MedImage instance\n\n    Notes\n    -----\n    The following fingerprint-related keys will be filtered out from metadata:\n    - class, hash, size, ndim, nvoxels, spacing, origin, direction\n    - min, max, sum, mean, std, variance\n    - dtype_str, dtype_numpy\n    \"\"\"\n    # Load the image file\n    image = sitk.ReadImage(filepath)\n\n    # Create instance of the class (MedImage or a subclass)\n    instance = cls(image)\n\n    if not metadata:\n        return instance\n\n    # Process metadata if provided\n    # Define fingerprint keys to filter out\n    fingerprint_keys = {\n        \"class\", \"hash\", \"size\", \"ndim\", \"nvoxels\", \"spacing\", \n        \"origin\", \"direction\", \"min\", \"max\", \"sum\", \"mean\", \"std\", \n        \"variance\", \"dtype_str\", \"dtype_numpy\"\n    }  # fmt: skip\n    # Filter metadata to exclude fingerprint keys\n    instance.metadata = {\n        k: v for k, v in metadata.items() if k not in fingerprint_keys\n    }\n    return instance\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_file(filepath)","title":"<code>filepath</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.from_file(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    view: bool = False,\n) -&gt; tuple[\n    numpy.ndarray,\n    imgtools.coretypes.spatial_types.ImageGeometry,\n]\n</code></pre> <p>Convert the image to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>bool</code> <p>Whether to return a view instead of a copy of the array, by default False. Views are more memory efficient but dont allow for modification of the array.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, imgtools.coretypes.spatial_types.ImageGeometry]</code> <p>A tuple containing the NumPy array and the image geometry with size, origin, direction, and spacing.</p> Notes <p>The returned NumPy array has axes ordered as (z, y, x), which is different from the SimpleITK convention of (x, y, z).</p> Source code in <code>src/imgtools/coretypes/base_medimage.py</code> <pre><code>def to_numpy(self, view: bool = False) -&gt; tuple[np.ndarray, ImageGeometry]:\n    \"\"\"Convert the image to a NumPy array.\n\n    Parameters\n    ----------\n    view : bool, optional\n        Whether to return a view instead of a copy of the array, by default False.\n        Views are more memory efficient but dont allow for modification of the array.\n\n    Returns\n    -------\n    tuple[np.ndarray, ImageGeometry]\n        A tuple containing the NumPy array and the image geometry with\n        size, origin, direction, and spacing.\n\n    Notes\n    -----\n    The returned NumPy array has axes ordered as (z, y, x), which is different\n    from the SimpleITK convention of (x, y, z).\n    \"\"\"\n    if view:\n        array = sitk.GetArrayViewFromImage(self)\n    else:\n        array = sitk.GetArrayFromImage(self)\n    return array, self.geometry\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.Scan.to_numpy(view)","title":"<code>view</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan","title":"read_dicom_scan","text":"<pre><code>read_dicom_scan(\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: typing.Any\n) -&gt; imgtools.coretypes.imagetypes.scan.Scan\n</code></pre> <p>Read a DICOM scan from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the directory containing the DICOM files.</p> required <code>str | None</code> <p>Series ID to read, by default None</p> <code>None</code> <code>bool</code> <p>Whether to read the files recursively, by default False</p> <code>False</code> <code>list[str] | None</code> <p>List of file names to read, by default None</p> <code>None</code> <code>typing.Any</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.imagetypes.scan.Scan</code> <p>The read scan.</p> Source code in <code>src/imgtools/coretypes/imagetypes/scan.py</code> <pre><code>def read_dicom_scan(\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: Any,  # noqa\n) -&gt; Scan:\n    \"\"\"Read a DICOM scan from a directory.\n\n    Parameters\n    ----------\n    path : str\n        Path to the directory containing the DICOM files.\n    series_id : str | None, optional\n        Series ID to read, by default None\n    recursive : bool, optional\n        Whether to read the files recursively, by default False\n    file_names : list[str] | None, optional\n        List of file names to read, by default None\n    **kwargs : Any\n        Unused keyword arguments.\n    Returns\n    -------\n    Scan\n        The read scan.\n    \"\"\"\n    return Scan.from_dicom(\n        path,\n        series_id=series_id,\n        recursive=recursive,\n        file_names=file_names,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan(path)","title":"<code>path</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan(series_id)","title":"<code>series_id</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan(file_names)","title":"<code>file_names</code>","text":""},{"location":"reference/coretypes/imagetypes/scan/#imgtools.coretypes.imagetypes.scan.read_dicom_scan(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/","title":"Roi matching","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching","title":"roi_matching","text":"<p>Functions:</p> Name Description <code>handle_roi_matching</code> <p>Match ROI names against regex patterns and apply a handling strategy.</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.Valid_Inputs","title":"Valid_Inputs  <code>module-attribute</code>","text":"<pre><code>Valid_Inputs = (\n    imgtools.coretypes.masktypes.roi_matching.ROIGroupPatterns\n    | dict[\n        str,\n        imgtools.coretypes.masktypes.roi_matching.PatternString,\n    ]\n    | list[\n        imgtools.coretypes.masktypes.roi_matching.PatternString\n    ]\n    | imgtools.coretypes.masktypes.roi_matching.PatternString\n    | None\n)\n</code></pre> <p>These are the valid inputs for the ROI matcher. 1) A dictionary with keys as strings and values as lists of regex patterns. 2) A dictionary with keys as strings and value as a single (str) regex pattern. 2) A list of regex patterns. 3) A single regex pattern as a string.</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy","title":"ROIMatchFailurePolicy","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Policy for how to handle total match failure (when no ROIs match any patterns).</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy","title":"ROIMatchStrategy","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enum for ROI handling strategies.</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatcher","title":"ROIMatcher","text":"<p>               Bases: <code>pydantic.BaseModel</code></p> <p>Methods:</p> Name Description <code>match_rois</code> <p>Match ROI names against the provided patterns.</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatcher.allow_multi_key_matches","title":"allow_multi_key_matches  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allow_multi_key_matches: bool = True\n</code></pre> <p>Whether to allow one ROI to match multiple keys in the match_map.</p> <p>When set to False, an ROI will only be associated with the first key  it matches, based on the order of keys in the match_map.</p> <p>Example:     With match_map = {'gtv': 'GTV.', 'tumor': 'GTVp.'}     ROI name \"GTVp\" would match both 'gtv' and 'tumor' patterns.</p> <pre><code>If allow_multi_key_matches=True: \"GTVp\" appears in both key results\nIf allow_multi_key_matches=False: \"GTVp\" only appears in 'gtv' results\n</code></pre>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatcher.on_missing_regex","title":"on_missing_regex  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>on_missing_regex: (\n    imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy\n) = (\n    imgtools.coretypes.masktypes.roi_matching.ROIMatchFailurePolicy.WARN\n)\n</code></pre> <p>How to handle when no ROI matches any pattern in match_map.</p> <ul> <li>IGNORE: Silently continue execution</li> <li>WARN: Log a warning but continue execution</li> <li>ERROR: Raise an error and halt execution</li> </ul> <p>Note: This only applies when NO ROIs match ANY patterns. If at least one ROI matches at least one pattern, this policy is not activated.</p>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatcher.match_rois","title":"match_rois","text":"<pre><code>match_rois(\n    roi_names: list[str],\n) -&gt; list[tuple[str, list[str]]]\n</code></pre> <p>Match ROI names against the provided patterns.</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>List of ROI names to match.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, list[str]]]</code> <p>List of tuples containing the key and matched ROI names. See <code>handle_roi_matching</code> for notes on the handling strategies.</p> See Also <p>handle_roi_matching : Function to handle the matching logic.</p> Source code in <code>src/imgtools/coretypes/masktypes/roi_matching.py</code> <pre><code>def match_rois(self, roi_names: list[str]) -&gt; list[tuple[str, list[str]]]:\n    \"\"\"\n    Match ROI names against the provided patterns.\n\n    Parameters\n    ----------\n    roi_names : list[str]\n        List of ROI names to match.\n\n    Returns\n    -------\n    list[tuple[str, list[str]]]\n        List of tuples containing the key and matched ROI names.\n        See `handle_roi_matching` for notes on the handling strategies.\n\n    See Also\n    --------\n    handle_roi_matching : Function to handle the matching logic.\n    \"\"\"\n    return handle_roi_matching(\n        roi_names,\n        self.match_map,\n        self.handling_strategy,\n        ignore_case=self.ignore_case,\n        allow_multi_key_matches=self.allow_multi_key_matches,\n    )\n</code></pre>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatcher.match_rois(roi_names)","title":"<code>roi_names</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchingError","title":"ROIMatchingError","text":"<pre><code>ROIMatchingError(\n    message: str,\n    roi_names: list[str],\n    match_patterns: imgtools.coretypes.masktypes.roi_matching.ROIGroupPatterns,\n)\n</code></pre> <p>               Bases: <code>ValueError</code></p> <p>Custom exception for ROI matching failures.</p> <p>Provides detailed information about the ROIs and patterns that failed to match.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Base error message</p> required <code>list[str]</code> <p>ROI names that were being matched</p> required <code>imgtools.coretypes.masktypes.roi_matching.ROIGroupPatterns</code> <p>The pattern dictionary that was used for matching</p> required Source code in <code>src/imgtools/coretypes/masktypes/roi_matching.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    roi_names: list[str],\n    match_patterns: ROIGroupPatterns,\n) -&gt; None:\n    \"\"\"Initialize with detailed information about the matching failure.\n\n    Parameters\n    ----------\n    message : str\n        Base error message\n    roi_names : list[str]\n        ROI names that were being matched\n    match_patterns : ROIGroupPatterns\n        The pattern dictionary that was used for matching\n    \"\"\"\n    self.roi_names = roi_names\n    self.match_patterns = match_patterns\n\n    # Build detailed error message\n    detailed_message = f\"{message}\\n\\n\"\n    detailed_message += \"ROI Matching Details:\\n\"\n    detailed_message += (\n        f\"- ROIs to match ({len(roi_names)}): {roi_names}\\n\"\n    )\n    detailed_message += f\"- Pattern groups ({len(match_patterns)}): {{\\n\"\n\n    for key, patterns in match_patterns.items():\n        detailed_message += f\"    '{key}': {patterns}\\n\"\n\n    detailed_message += \"  }\\n\"\n    detailed_message += \"\\nPlease check your ROI names and match patterns.\"\n\n    super().__init__(detailed_message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchingError(message)","title":"<code>message</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchingError(roi_names)","title":"<code>roi_names</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.ROIMatchingError(match_patterns)","title":"<code>match_patterns</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching","title":"handle_roi_matching","text":"<pre><code>handle_roi_matching(\n    roi_names: list[str],\n    roi_matching: imgtools.coretypes.masktypes.roi_matching.ROIGroupPatterns,\n    strategy: imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy,\n    ignore_case: bool = True,\n    allow_multi_key_matches: bool = True,\n) -&gt; list[tuple[str, list[str]]]\n</code></pre> <p>Match ROI names against regex patterns and apply a handling strategy.</p> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>List of ROI names to match.</p> required <code>imgtools.coretypes.masktypes.roi_matching.ROIGroupPatterns</code> <p>Mapping of keys to list of regex patterns.</p> required <code>imgtools.coretypes.masktypes.roi_matching.ROIMatchStrategy</code> <p>Strategy to use: MERGE, KEEP_FIRST, or SEPARATE.</p> required <code>bool</code> <p>Whether to ignore case during matching.</p> <code>True</code> <code>bool</code> <p>Whether to allow an ROI to match multiple keys in the match_map. If False, an ROI will only be associated with the first key it matches, based on the order of keys in the roi_matching dictionary.</p> <code>True</code> <p>How to handle when no ROI matches any pattern in roi_matching. IGNORE: Silently continue execution WARN: Log a warning but continue execution ERROR: Raise an error and halt execution Note: This parameter is processed at the caller level, not within this function.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, list[str]]]</code> <p>List of tuples containing the key and matched ROI names. See Notes for details on the handling strategies.</p> Notes <ul> <li>MERGE: Merge all ROIs with the same key. Returns a single tuple for each key,     but the value is a list of all the matched ROIs.     i.e [('GTV', ['GTV1', 'GTV2']), ('PTV', ['PTV1', 'ptv x'])]</li> <li>KEEP_FIRST: For each key, keep the first ROI found based on the pattern.     Returns a single tuple for each key, but the value is a list of size ONE with     the first matched ROI.     i.e [('GTV', ['GTV1']), ('PTV', ['PTV1'])]</li> <li>SEPARATE: Separate all ROIs. Returns possibly multiple tuples for each key,     because a key may have multiple ROIs matching the pattern.     i.e [('GTV', ['GTV1']), ('GTV', ['GTV2']), ('PTV', ['PTV1'])]</li> </ul> <p>Complex Interaction with allow_multi_key_matches:</p> <p>When allow_multi_key_matches=True:     - One ROI can match to multiple keys     - The strategy then only determines how ROIs are organized within         each key group</p> <p>When allow_multi_key_matches=False:     - Each ROI is assigned to exactly one key at most (the first matching key)     - Strategy application happens AFTER this restriction</p> <p>Example 1:     roi_names=['GTVp', 'GTVn', 'PTV']     match_map={'gtv': ['GTV.*'], 'primary': ['GTVp'], 'ptv': ['PTV']}</p> <pre><code>With allow_multi_key_matches=True:\n    - MERGE: [('gtv', ['GTVp', 'GTVn']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n    - KEEP_FIRST: [('gtv', ['GTVp']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n    - SEPARATE: [('gtv', ['GTVp']), ('gtv', ['GTVn']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n\nWith allow_multi_key_matches=False:\n    **note that primary's GTVp got matched already in 'gtv'**\n    **and thus is never added to the results**\n    - MERGE: [('gtv', ['GTVp', 'GTVn']), ('ptv', ['PTV'])]\n    - KEEP_FIRST: [('gtv', ['GTVp']), ('ptv', ['PTV'])]\n    - SEPARATE: [('gtv', ['GTVp']), ('gtv', ['GTVn']), ('ptv', ['PTV'])]\n</code></pre> <p>Example 2 (demonstrating the key issue):     roi_names=['GTVp', 'GTVp_2']     match_map={'primary': ['GTVp'], 'gtv': ['GTV.*']}</p> <pre><code>With allow_multi_key_matches=True:\n    - KEEP_FIRST: [('primary', ['GTVp']), ('gtv', ['GTVp'])]  # GTVp appears in both keys\n\nWith allow_multi_key_matches=False:\n    **though technically, GTVp_2 wouldve matched in 'primary', since its KEEP_FIRST,**\n    **its still available for 'gtv'**\n    - KEEP_FIRST: [('primary', ['GTVp']), ('gtv', ['GTVp_2'])]\n</code></pre> Source code in <code>src/imgtools/coretypes/masktypes/roi_matching.py</code> <pre><code>def handle_roi_matching(  # noqa: PLR0912\n    roi_names: list[str],\n    roi_matching: ROIGroupPatterns,\n    strategy: ROIMatchStrategy,\n    ignore_case: bool = True,\n    allow_multi_key_matches: bool = True,\n) -&gt; list[tuple[str, list[str]]]:\n    \"\"\"\n    Match ROI names against regex patterns and apply a handling strategy.\n\n    Parameters\n    ----------\n    roi_names : list[str]\n        List of ROI names to match.\n    roi_matching : ROIGroupPatterns\n        Mapping of keys to list of regex patterns.\n    strategy : ROIMatchStrategy\n        Strategy to use: MERGE, KEEP_FIRST, or SEPARATE.\n    ignore_case : bool\n        Whether to ignore case during matching.\n    allow_multi_key_matches : bool\n        Whether to allow an ROI to match multiple keys in the match_map.\n        If False, an ROI will only be associated with the first key it matches,\n        based on the order of keys in the roi_matching dictionary.\n    on_missing_regex :ROIMatchFailurePolicy\n        How to handle when no ROI matches any pattern in roi_matching.\n        IGNORE: Silently continue execution\n        WARN: Log a warning but continue execution\n        ERROR: Raise an error and halt execution\n        Note: This parameter is processed at the caller level, not within this function.\n\n    Returns\n    -------\n    list[tuple[str, list[str]]]\n        List of tuples containing the key and matched ROI names.\n        See Notes for details on the handling strategies.\n\n    Notes\n    -----\n    - MERGE: Merge all ROIs with the same key. Returns a single tuple for each key,\n        but the value is a list of all the matched ROIs.\n        i.e [('GTV', ['GTV1', 'GTV2']), ('PTV', ['PTV1', 'ptv x'])]\n    - KEEP_FIRST: For each key, keep the first ROI found based on the pattern.\n        Returns a single tuple for each key, but the value is a list of size ONE with\n        the first matched ROI.\n        i.e [('GTV', ['GTV1']), ('PTV', ['PTV1'])]\n    - SEPARATE: Separate all ROIs. Returns possibly multiple tuples for each key,\n        because a key may have multiple ROIs matching the pattern.\n        i.e [('GTV', ['GTV1']), ('GTV', ['GTV2']), ('PTV', ['PTV1'])]\n\n    Complex Interaction with allow_multi_key_matches:\n\n    When allow_multi_key_matches=True:\n        - One ROI can match to multiple keys\n        - The strategy then only determines how ROIs are organized within\n            each key group\n\n    When allow_multi_key_matches=False:\n        - Each ROI is assigned to exactly one key at most (the first matching key)\n        - Strategy application happens AFTER this restriction\n\n    Example 1:\n        roi_names=['GTVp', 'GTVn', 'PTV']\n        match_map={'gtv': ['GTV.*'], 'primary': ['GTVp'], 'ptv': ['PTV']}\n\n        With allow_multi_key_matches=True:\n            - MERGE: [('gtv', ['GTVp', 'GTVn']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n            - KEEP_FIRST: [('gtv', ['GTVp']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n            - SEPARATE: [('gtv', ['GTVp']), ('gtv', ['GTVn']), ('primary', ['GTVp']), ('ptv', ['PTV'])]\n\n        With allow_multi_key_matches=False:\n            **note that primary's GTVp got matched already in 'gtv'**\n            **and thus is never added to the results**\n            - MERGE: [('gtv', ['GTVp', 'GTVn']), ('ptv', ['PTV'])]\n            - KEEP_FIRST: [('gtv', ['GTVp']), ('ptv', ['PTV'])]\n            - SEPARATE: [('gtv', ['GTVp']), ('gtv', ['GTVn']), ('ptv', ['PTV'])]\n\n    Example 2 (demonstrating the key issue):\n        roi_names=['GTVp', 'GTVp_2']\n        match_map={'primary': ['GTVp'], 'gtv': ['GTV.*']}\n\n        With allow_multi_key_matches=True:\n            - KEEP_FIRST: [('primary', ['GTVp']), ('gtv', ['GTVp'])]  # GTVp appears in both keys\n\n        With allow_multi_key_matches=False:\n            **though technically, GTVp_2 wouldve matched in 'primary', since its KEEP_FIRST,**\n            **its still available for 'gtv'**\n            - KEEP_FIRST: [('primary', ['GTVp']), ('gtv', ['GTVp_2'])]\n    \"\"\"\n    flags = re.IGNORECASE if ignore_case else 0\n\n    @lru_cache(maxsize=128)\n    def _match_pattern(roi_name: str, pattern: PatternString) -&gt; bool:\n        return re.fullmatch(pattern, roi_name, flags=flags) is not None\n\n    # First pass: collect all potential matches without considering allow_multi_key_matches\n    raw_results = defaultdict(list)\n    for key, patterns in roi_matching.items():\n        for pattern, roi_name in product(patterns, roi_names):\n            if _match_pattern(roi_name, pattern):\n                raw_results[key].append(roi_name)\n\n    # If no matches were found, return an empty list\n    # The ROIMatchFailurePolicy is now handled in the caller\n    if not any(raw_results.values()):\n        return []\n\n    # Apply the selected strategy to the filtered results\n    # TODO:: this is a ugly mess, apologies if youre about to read this\n    # TODO:: refactor!!!\n    match strategy:\n        case ROIMatchStrategy.MERGE:\n            # Merge all ROIs with the same key\n            # Returns a single tuple for each key with all matched ROIs\n            filtered_results = defaultdict(list)\n            assigned_rois = set()\n            for key, rois in raw_results.items():\n                for roi in rois:\n                    if allow_multi_key_matches:\n                        filtered_results[key].append(roi)\n                    elif roi not in assigned_rois:\n                        filtered_results[key].append(roi)\n                        assigned_rois.add(roi)\n            return [(key, v) for key, v in filtered_results.items() if v]\n        case ROIMatchStrategy.KEEP_FIRST:\n            # For each key, keep only the first ROI found\n            # Returns a single tuple for each key with at most one ROI\n            # more complex than it seems\n            filtered_results = defaultdict(list)\n            assigned_rois = set()\n            for key, rois in raw_results.items():\n                for roi in rois:\n                    if allow_multi_key_matches:\n                        # If allowing multi-key matches, keep the first match\n                        filtered_results[key].append(roi)\n                        break\n                    elif roi not in assigned_rois:\n                        filtered_results[key].append(roi)\n                        assigned_rois.add(roi)\n                        break  # Stop after the first match\n            return [(key, v) for key, v in filtered_results.items() if v]\n        case ROIMatchStrategy.SEPARATE:\n            # Separate all ROIs\n            # Returns one tuple per ROI\n            tuples = []\n            filtered_results = defaultdict(list)\n            assigned_rois = set()\n            for key, rois in raw_results.items():\n                for roi in rois:\n                    if allow_multi_key_matches:\n                        filtered_results[key].append(roi)\n                    elif roi not in assigned_rois:\n                        filtered_results[key].append(roi)\n                        assigned_rois.add(roi)\n            # Flatten the results into tuples\n            # This is a bit tricky because we want to keep the key\n            # but separate the ROIs\n            for key, v in filtered_results.items():\n                for roi in v:\n                    tuples.append((key, [roi]))\n            return tuples\n</code></pre>"},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(roi_names)","title":"<code>roi_names</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(roi_matching)","title":"<code>roi_matching</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(strategy)","title":"<code>strategy</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(ignore_case)","title":"<code>ignore_case</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(allow_multi_key_matches)","title":"<code>allow_multi_key_matches</code>","text":""},{"location":"reference/coretypes/masktypes/roi_matching/#imgtools.coretypes.masktypes.roi_matching.handle_roi_matching(on_missing_regex)","title":"<code>on_missing_regex</code>","text":""},{"location":"reference/coretypes/masktypes/seg/","title":"Seg","text":""},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg","title":"seg","text":""},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG","title":"SEG  <code>dataclass</code>","text":"<pre><code>SEG(\n    raw_seg: highdicom.seg.Segmentation,\n    ref_indices: numpy.ndarray,\n    segments: dict[\n        int, imgtools.coretypes.masktypes.seg.Segment\n    ] = dict(),\n    metadata: dict[str, typing.Any] = dict(),\n)\n</code></pre> <p>Represents a DICOM Segmentation (DICOM-SEG) object.</p> <p>Methods:</p> Name Description <code>extract_roi_identifiers</code> <p>Returns a mapping of ROI names to segment numbers.</p> <code>from_dicom</code> <p>Loads a DICOM-SEG object from a DICOM file.</p> <code>get_vector_mask</code> <p>Convert the SEG segments to a VectorMask using ROI matching.</p>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.descriptions","title":"descriptions  <code>property</code>","text":"<pre><code>descriptions: list[str]\n</code></pre> <p>Returns a list of unique descriptions for the segments in the DICOM-SEG object.</p>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.labels","title":"labels  <code>property</code>","text":"<pre><code>labels: list[str]\n</code></pre> <p>Returns a list of unique labels for the segments in the DICOM-SEG object.</p>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.extract_roi_identifiers","title":"extract_roi_identifiers","text":"<pre><code>extract_roi_identifiers() -&gt; dict[str, int]\n</code></pre> <p>Returns a mapping of ROI names to segment numbers.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def extract_roi_identifiers(self) -&gt; dict[str, int]:\n    \"\"\"\n    Returns a mapping of ROI names to segment numbers.\n    \"\"\"\n    names_map = {}\n    for idx, seg in self.segments.items():\n        names_map[seg.label] = idx\n    return names_map\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.from_dicom","title":"from_dicom  <code>classmethod</code>","text":"<pre><code>from_dicom(\n    dicom: imgtools.dicom.DicomInput,\n    metadata: dict[str, typing.Any] | None = None,\n) -&gt; imgtools.coretypes.masktypes.seg.SEG\n</code></pre> <p>Loads a DICOM-SEG object from a DICOM file.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>@classmethod\ndef from_dicom(  # noqa: PLR0912\n    cls,\n    dicom: DicomInput,\n    metadata: dict[str, Any] | None = None,\n) -&gt; SEG:\n    \"\"\"\n    Loads a DICOM-SEG object from a DICOM file.\n    \"\"\"\n    if isinstance(dicom, (str, Path)):\n        dicom = Path(dicom)\n        if dicom.is_dir():\n            if len(list(dicom.glob(\"*.dcm\"))) == 1:\n                dicom = list(dicom.glob(\"*.dcm\"))[0]\n            else:\n                msg = (\n                    f\"Directory `{dicom}` contains multiple DICOM files. \"\n                    f\"Cannot determine which one to load.\"\n                )\n                raise SegmentationError(msg)\n    ds_seg = load_dicom(dicom, stop_before_pixels=False)\n    try:\n        seg = hd.seg.Segmentation.from_dataset(ds_seg)\n    except KeyError as e:\n        #  check if KeyError: (0062,000A)\n        msg = (\n            f\"Segmentation object {dicom!r} does not contain \"\n            f\"SegmentationType. Skipping.\"\n        )\n        raise SegmentationError(msg) from e\n\n    metadata = metadata or extract_metadata(ds_seg, \"SEG\", extra_tags=None)  # type: ignore\n    segments: dict[int, Segment] = {}\n    for segnum in seg.segment_numbers:\n        segdesc = seg.get_segment_description(segnum)\n\n        if segnum in segments:\n            raise SegmentDuplicateError(segdesc.SegmentLabel, segnum)\n        segments[segnum] = Segment(\n            number=segnum,\n            label=segdesc.SegmentLabel,\n            description=segdesc.get(\"SegmentDescription\", None),\n        )\n\n    ref_indices = get_ref_indices(seg)\n\n    match hd.seg.SegmentationTypeValues(seg.SegmentationType):\n        case hd.seg.SegmentationTypeValues.BINARY:\n            # Binary segmentation\n            for segment_number, segment in segments.items():\n                segment.data_array = (\n                    seg.get_volume(\n                        combine_segments=False,\n                        rescale_fractional=False,\n                        skip_overlap_checks=False,\n                        segment_numbers=[segment_number],\n                    )\n                    .squeeze_channel()\n                    .array\n                )\n\n        case hd.seg.SegmentationTypeValues.FRACTIONAL:\n            # should only be 1 segment\n            assert len(seg.segment_numbers) == 1, (\n                f\"Fractional DICOM-SEG has {len(seg.segment_numbers)} \"\n                f\"segments, but expected 1.\"\n            )\n            # assume that the pixel array is just for one segment\n            # add to the segment\n            segments[seg.segment_numbers[0]].data_array = seg.pixel_array\n        case _:\n            raise SegmentationTypeUnsupportedError(seg.SegmentationType)\n\n    # sanity check\n    for segment in segments.values():\n        if segment.data_array is None:\n            raise SegmentDataMissingError(segment.number, segment.label)\n        # length of ref_indices should be the same as 0th dimension of data_array\n        if segment.data_array.shape[0] != len(ref_indices):\n            raise SegmentationValidationError(\n                segment.number,\n                segment.label,\n                segment.data_array.shape,\n                len(ref_indices),\n            )\n\n    return cls(\n        raw_seg=seg,\n        ref_indices=ref_indices,\n        segments=segments,\n        metadata={k: v for k, v in metadata.items() if v},\n    )\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.get_vector_mask","title":"get_vector_mask","text":"<pre><code>get_vector_mask(\n    reference_image: imgtools.coretypes.MedImage,\n    roi_matcher: imgtools.coretypes.masktypes.roi_matching.ROIMatcher,\n) -&gt; imgtools.coretypes.base_masks.VectorMask | None\n</code></pre> <p>Convert the SEG segments to a VectorMask using ROI matching.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.coretypes.MedImage</code> <p>The reference image to align the mask with.</p> required <code>imgtools.coretypes.masktypes.roi_matching.ROIMatcher</code> <p>The matcher used to match segment identifiers to ROI keys.</p> required <p>Returns:</p> Type Description <code>imgtools.coretypes.base_masks.VectorMask | None</code> <p>A VectorMask representation of the matched segments, or None if no matches were found and the ROIMatchFailurePolicy is not ERROR.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def get_vector_mask(\n    self,\n    reference_image: MedImage,\n    roi_matcher: ROIMatcher,\n) -&gt; VectorMask | None:\n    \"\"\"\n    Convert the SEG segments to a VectorMask using ROI matching.\n\n    Parameters\n    ----------\n    reference_image : MedImage\n        The reference image to align the mask with.\n    roi_matcher : ROIMatcher\n        The matcher used to match segment identifiers to ROI keys.\n\n    Returns\n    -------\n    VectorMask | None\n        A VectorMask representation of the matched segments, or None if\n        no matches were found and the ROIMatchFailurePolicy is not ERROR.\n\n    Raises\n    ------\n    SegmentationError\n        If no segments matched the specified patterns and the ROIMatchFailurePolicy is ERROR.\n    \"\"\"\n    roi_identifier_mapping = self.extract_roi_identifiers()\n    roi_names = self.labels\n\n    matched_results = roi_matcher.match_rois(\n        list(roi_identifier_mapping.keys())\n    )\n\n    # Handle the case where no matches were found, according to the policy\n    if not matched_results:\n        message = \"No ROIs matched any patterns in the match_map.\"\n        match roi_matcher.on_missing_regex:\n            case ROIMatchFailurePolicy.IGNORE:\n                # Silently return None\n                pass\n            case ROIMatchFailurePolicy.WARN:\n                logger.warning(\n                    message,\n                    roi_names=list(roi_identifier_mapping.keys()),\n                    roi_matching=roi_matcher.match_map,\n                )\n            case ROIMatchFailurePolicy.ERROR:\n                # Raise an error\n                errmsg = f\"{message} Available ROIs: {list(roi_identifier_mapping.keys())}, \"\n                raise ROIMatchingError(\n                    errmsg,\n                    roi_names=roi_names,\n                    match_patterns=roi_matcher.match_map,\n                )\n        return None\n\n    # Process the matches and build the segments\n    matched_rois: list[tuple[str, list[Segment]]] = []\n    for key, matches in matched_results:\n        segs = [\n            self.segments[idx]\n            for idx in set(\n                roi_identifier_mapping[match] for match in matches\n            )\n        ]\n        matched_rois.append((key, segs))\n\n    ref_image_indices = [\n        reference_image.TransformPhysicalPointToIndex(pos)\n        for pos in self.ref_indices\n    ]\n\n    # we need something to store the mapping\n    # so that we can keep track of what the 3D mask matches to\n    # the original roi name(s)\n    mapping: dict[int, ROIMaskMapping] = {}\n    mask_images = []\n\n    for iroi, (roi_key, segment_matches) in enumerate(matched_rois):\n        mask_array_3d = np.zeros(\n            (\n                reference_image.size.depth,\n                reference_image.size.height,\n                reference_image.size.width,\n            ),\n            dtype=np.uint8,\n        )\n        # we use the pre-stored data in each segment\n        # then for each z slice, we need to determine which index in the output mask\n        for segment_of_interest in segment_matches:\n            # Use the pre-stored data_array from the segment\n            arr = segment_of_interest.data_array\n            if arr is None:\n                logger.warning(\n                    f\"Segment {segment_of_interest.number} ({segment_of_interest.label}) \"\n                    f\"has no data array. Skipping.\"\n                )\n                continue\n\n            # now we insert each slice into the correct index in the 3D mask array\n            for index, seg_slice in zip(\n                ref_image_indices, arr, strict=True\n            ):\n                # we need to check if the z index is in bounds of the mask_array\n                (_x, _y, z) = index\n                if 0 &lt;= z &lt; reference_image.size.depth:\n                    mask_array_3d[z, :, :] = np.logical_or(\n                        mask_array_3d[z, :, :], seg_slice\n                    )\n                else:\n                    logger.warning(\n                        f\"Z-index {z} out of bounds for reference image shape. \"\n                        f\"Skipping slice for ROI '{roi_key}'\"\n                    )\n            mapping[iroi] = ROIMaskMapping(\n                roi_key=roi_key,\n                roi_names=segment_matches,\n                image_id=roi_key\n                if roi_matcher.handling_strategy.value == \"merge\"\n                else f\"{roi_key}__[{segment_matches[0].label}]\",\n            )\n        mask_images.append(sitk.GetImageFromArray(mask_array_3d))\n\n    mask_image = sitk.Compose(mask_images)\n    mask_image.CopyInformation(reference_image)\n\n    assert mask_image.GetNumberOfComponentsPerPixel() == len(mapping)\n\n    # Update the map to be to strings, not segments\n    for idx, m in mapping.items():\n        mapping[idx] = ROIMaskMapping(\n            roi_key=m.roi_key,\n            roi_names=[f\"{segment.label}\" for segment in m.roi_names],\n            image_id=m.image_id,\n        )\n\n    return VectorMask(\n        image=mask_image,\n        roi_mapping=mapping,\n        metadata=self.metadata,\n        errors=None,\n    )\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.get_vector_mask(reference_image)","title":"<code>reference_image</code>","text":""},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SEG.get_vector_mask(roi_matcher)","title":"<code>roi_matcher</code>","text":""},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.Segment","title":"Segment  <code>dataclass</code>","text":"<pre><code>Segment(\n    number: int,\n    label: str,\n    description: str | None = None,\n    data_array: numpy.ndarray | None = None,\n)\n</code></pre> <p>Represents a segment in a DICOM Segmentation object.</p> <p>Attributes:</p> Name Type Description <code>number</code> <code>int</code> <p>The segment number.</p> <code>label</code> <code>str</code> <p>The label of the segment.</p> <code>description</code> <code>str</code> <p>A description of the segment.</p> <code>data_array</code> <code>numpy.ndarray | None</code> <p>The data array representing the segment. Defaults to None.</p>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentDataMissingError","title":"SegmentDataMissingError","text":"<pre><code>SegmentDataMissingError(\n    segment_number: int, segment_label: str\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.masktypes.seg.SegmentationError</code></p> <p>Raised when a segment has no data array.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def __init__(self, segment_number: int, segment_label: str) -&gt; None:\n    self.segment_number = segment_number\n    self.segment_label = segment_label\n    message = f\"Segment {segment_number} ({segment_label}) has no data array. Check the DICOM-SEG file.\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentDuplicateError","title":"SegmentDuplicateError","text":"<pre><code>SegmentDuplicateError(\n    segment_label: str, segment_number: int\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.masktypes.seg.SegmentationError</code></p> <p>Raised when a segment is duplicated in the DICOM-SEG file.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def __init__(self, segment_label: str, segment_number: int) -&gt; None:\n    self.segment_label = segment_label\n    self.segment_number = segment_number\n    message = f\"Segment {segment_label} (number: {segment_number}) is duplicated in the DICOM-SEG file.\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentReferenceIndicesError","title":"SegmentReferenceIndicesError","text":"<pre><code>SegmentReferenceIndicesError(error_message: str)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.masktypes.seg.SegmentationError</code></p> <p>Raised when there's an error with reference indices in the segmentation.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def __init__(self, error_message: str) -&gt; None:\n    message = f\"Unable to get reference indices from segmentation: {error_message}\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentationError","title":"SegmentationError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for DICOM SEG errors.</p>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentationTypeUnsupportedError","title":"SegmentationTypeUnsupportedError","text":"<pre><code>SegmentationTypeUnsupportedError(seg_type: str)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.masktypes.seg.SegmentationError</code></p> <p>Raised when the segmentation type is unsupported.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def __init__(self, seg_type: str) -&gt; None:\n    self.seg_type = seg_type\n    message = f\"Unsupported SegmentationType: {seg_type}\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.SegmentationValidationError","title":"SegmentationValidationError","text":"<pre><code>SegmentationValidationError(\n    segment_number: int,\n    segment_label: str,\n    data_array_shape: tuple,\n    ref_indices_length: int,\n)\n</code></pre> <p>               Bases: <code>imgtools.coretypes.masktypes.seg.SegmentationError</code></p> <p>Raised when a segmentation validation check fails.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def __init__(\n    self,\n    segment_number: int,\n    segment_label: str,\n    data_array_shape: tuple,\n    ref_indices_length: int,\n) -&gt; None:\n    self.segment_number = segment_number\n    self.segment_label = segment_label\n    self.data_array_shape = data_array_shape\n    self.ref_indices_length = ref_indices_length\n    message = (\n        f\"Segment {segment_number} ({segment_label}) has a data array with shape \"\n        f\"{data_array_shape}, but expected first dimension to be {ref_indices_length}.\"\n    )\n    super().__init__(message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/seg/#imgtools.coretypes.masktypes.seg.get_ref_indices","title":"get_ref_indices","text":"<pre><code>get_ref_indices(\n    seg: highdicom.seg.Segmentation,\n) -&gt; numpy.ndarray\n</code></pre> <p>Returns the reference indices for a given segmentation object. This function attempts to extract the reference indices from the segmentation object. If the segmentation is fractional, it uses the volume geometry to calculate the reference indices. If the segmentation is not fractional, it extracts the reference indices directly from the segmentation object.</p> Source code in <code>src/imgtools/coretypes/masktypes/seg.py</code> <pre><code>def get_ref_indices(\n    seg: hd.seg.Segmentation,\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns the reference indices for a given segmentation object.\n    This function attempts to extract the reference indices from the segmentation\n    object. If the segmentation is fractional, it uses the volume geometry to\n    calculate the reference indices.\n    If the segmentation is not fractional, it extracts the reference indices\n    directly from the segmentation object.\n    \"\"\"\n    try:\n        ref_indices = np.array(\n            [\n                p[0].ImagePositionPatient\n                for p in seg.get_volume().get_plane_positions()\n            ]\n        )\n    except Exception as e:\n        # probably fractional\n        svg = seg.get_volume_geometry()\n\n        if svg is None:\n            raise SegmentReferenceIndicesError(\n                \"Unable to get volume geometry from segmentation.\"\n            ) from e\n\n        ref_indices = svg.map_indices_to_reference(\n            np.array([[p, 0, 0] for p in range(int(seg.NumberOfFrames))])\n        )\n\n    return ref_indices\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/","title":"Structureset","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset","title":"structureset","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ContourPointsAcrossSlicesError","title":"ContourPointsAcrossSlicesError","text":"<pre><code>ContourPointsAcrossSlicesError(\n    roi_name: str,\n    contour_num: int,\n    slice_points_shape: tuple,\n    z_values: list,\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when contour points span across multiple slices.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def __init__(\n    self,\n    roi_name: str,\n    contour_num: int,\n    slice_points_shape: tuple,\n    z_values: list,\n) -&gt; None:\n    self.roi_name = roi_name\n    self.contour_num = contour_num\n    self.slice_points_shape = slice_points_shape\n    self.z_values = z_values\n    super().__init__(self._generate_message())\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.MaskArrayOutOfBoundsError","title":"MaskArrayOutOfBoundsError","text":"<pre><code>MaskArrayOutOfBoundsError(\n    roi_name: str,\n    contour_num: int,\n    z_int: int,\n    mask_shape: tuple,\n    slice_points_shape: tuple,\n    z_values: list,\n    reference_image: imgtools.coretypes.MedImage,\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when a mask array index is out of bounds for the reference image.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def __init__(\n    self,\n    roi_name: str,\n    contour_num: int,\n    z_int: int,\n    mask_shape: tuple,\n    slice_points_shape: tuple,\n    z_values: list,\n    reference_image: MedImage,\n) -&gt; None:\n    self.roi_name = roi_name\n    self.contour_num = contour_num\n    self.z_int = z_int\n    self.mask_shape = mask_shape\n    self.slice_points_shape = slice_points_shape\n    self.z_values = z_values\n    self.reference_image = reference_image\n    super().__init__(self._generate_message())\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.NonIntegerZSliceIndexError","title":"NonIntegerZSliceIndexError","text":"<pre><code>NonIntegerZSliceIndexError(\n    roi_name: str, contour_num: int, z_idx: float\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when the Z-slice index is not an integer.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def __init__(self, roi_name: str, contour_num: int, z_idx: float) -&gt; None:\n    self.roi_name = roi_name\n    self.contour_num = contour_num\n    self.z_idx = z_idx\n    super().__init__(self._generate_message())\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError","title":"ROIContourExtractionError","text":"<pre><code>ROIContourExtractionError(\n    message: str,\n    roi_index: int,\n    roi_name: str,\n    referenced_roi_number: int | None = None,\n    additional_info: dict | None = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.exceptions.ROIContourError</code></p> <p>Exception raised when extracting ROI contour data fails with details about the specific ROI.</p> <p>This exception contains additional information about the ROI index, name, and referenced ROI number to provide context for the error.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Base error message describing what went wrong</p> required <code>int</code> <p>Index of the ROI in the ROIContourSequence</p> required <code>str</code> <p>Name of the ROI (for context in error messages)</p> required <code>int</code> <p>Referenced ROI number from the DICOM file, by default None</p> <code>None</code> <code>dict</code> <p>Any additional information to include in the error message, by default None</p> <code>None</code> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    roi_index: int,\n    roi_name: str,\n    referenced_roi_number: int | None = None,\n    additional_info: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize with detailed information about the ROI extraction failure.\n\n    Parameters\n    ----------\n    message : str\n        Base error message describing what went wrong\n    roi_index : int\n        Index of the ROI in the ROIContourSequence\n    roi_name : str\n        Name of the ROI (for context in error messages)\n    referenced_roi_number : int, optional\n        Referenced ROI number from the DICOM file, by default None\n    additional_info : dict, optional\n        Any additional information to include in the error message, by default None\n    \"\"\"\n    self.roi_index = roi_index\n    self.roi_name = roi_name\n    self.referenced_roi_number = referenced_roi_number or (roi_index + 1)\n    self.additional_info = additional_info or {}\n\n    # Build detailed error message\n    roi_identifier = (\n        f\"ROI at index {self.roi_index}, \"\n        f\"(ReferencedROINumber={self.referenced_roi_number})\"\n        f\" with name '{self.roi_name}'\"\n    )\n    detailed_message = f\"{roi_identifier}: {message}\"\n\n    # Add any additional info to the message\n    if self.additional_info:\n        details = \", \".join(\n            f\"{k}={v}\" for k, v in self.additional_info.items()\n        )\n        detailed_message += f\" [Additional info: {details}]\"\n\n    super().__init__(detailed_message)\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError(message)","title":"<code>message</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError(roi_index)","title":"<code>roi_index</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError(roi_name)","title":"<code>roi_name</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError(referenced_roi_number)","title":"<code>referenced_roi_number</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.ROIContourExtractionError(additional_info)","title":"<code>additional_info</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet","title":"RTStructureSet  <code>dataclass</code>","text":"<pre><code>RTStructureSet(\n    metadata: dict[str, typing.Any],\n    roi_map: dict[str, pydicom.sequence.Sequence] = dict(),\n)\n</code></pre> <p>Represents the entire structure set, containing multiple ROIs.</p> <p>Attributes:</p> Name Type Description <code>roi_names</code> <code>typing.List[str]</code> <p>List of ROI names extracted from the RTSTRUCT.</p> <code>metadata</code> <code>dict[str, typing.Any]</code> <p>Metadata extracted from the RTSTRUCT DICOM file.</p> <code>roi_map</code> <code>dict[str, ROI]</code> <p>Dictionary mapping ROI names to their corresponding <code>ROI</code> objects.</p> <code>roi_map_errors</code> <code>dict[str, ROIExtractionErrorMsg]</code> <p>Dictionary mapping ROI names to any extraction errors encountered.</p> <p>Methods:</p> Name Description <code>from_dicom</code> <p>Create a <code>RTStructureSet</code> object from a DICOM file.</p> <code>match_roi</code> <p>Search for ROI names matching a given pattern.</p> <code>__getitem__</code> <p>Access ROI objects by name, index, or slice.</p>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.plogger","title":"plogger  <code>property</code> <code>writable</code>","text":"<pre><code>plogger\n</code></pre> <p>Return the logger for this class.</p>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.from_dicom","title":"from_dicom  <code>classmethod</code>","text":"<pre><code>from_dicom(\n    dicom: imgtools.dicom.DicomInput,\n    metadata: typing.Dict[str, typing.Any] | None = None,\n) -&gt; (\n    imgtools.coretypes.masktypes.structureset.RTStructureSet\n)\n</code></pre> <p>Create a RTStructureSet object from an RTSTRUCT DICOM file.</p> <p>Lazy loads by default, by giving access to ROI Names and metadata and then loads the ROIs on demand. See Notes.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path | bytes | pydicom.dataset.FileDataset</code> <ul> <li>Either a path to the DICOM file, a directory containing a single DICOM file, or a <code>FileDataset</code> object.</li> <li>If a directory is provided, it will be searched for a single DICOM file. If multiple files are found, an error will be raised.</li> <li>If a <code>FileDataset</code> object is provided, it will be used directly.</li> </ul> required <code>dict[str, typing.Any] | None</code> <p>If provided, this metadata will be used instead of extracting it from the DICOM file. This is useful because our crawler extracts and processes the metadata, possibly doing some better remapping to figure out the correct ReferencedSeriesInstanceUID.</p> <code>None</code> <p>Returns:</p> Type Description <code>imgtools.coretypes.masktypes.structureset.RTStructureSet</code> <p>The structure set data extracted from the RTSTRUCT.</p> Notes <p>Compared to the old implementation, we dont extract the numpy arrays for the ROIs immediately. Instead, we just extract the metadata and then store the weak-refs to the <code>ROIContourSequence</code> objects.</p> <p>This allows us to avoid the computation of the numpy arrays until we actually ask for them (i.e converting to <code>sitk.Image</code>) which might use some regex pattern matching to only process the ROIs that we want.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>@classmethod\ndef from_dicom(\n    cls,\n    dicom: DicomInput,\n    metadata: Dict[str, Any] | None = None,\n) -&gt; RTStructureSet:\n    \"\"\"Create a RTStructureSet object from an RTSTRUCT DICOM file.\n\n    Lazy loads by default, by giving access to ROI Names and metadata\n    and then loads the ROIs on demand. See Notes.\n\n    Parameters\n    ----------\n    dicom : str | Path | bytes | FileDataset\n        - Either a path to the DICOM file, a directory containing\n        *a single* DICOM file, or a `FileDataset` object.\n        - If a directory is provided, it will be searched for a single DICOM\n        file. If multiple files are found, an error will be raised.\n        - If a `FileDataset` object is provided, it will be used directly.\n    metadata : dict[str, Any] | None, optional\n        If provided, this metadata will be used instead of extracting it\n        from the DICOM file. This is useful because our crawler\n        extracts and processes the metadata, possibly doing some better\n        remapping to figure out the correct ReferencedSeriesInstanceUID.\n\n    Returns\n    -------\n    RTStructureSet\n        The structure set data extracted from the RTSTRUCT.\n\n    Notes\n    -----\n    Compared to the old implementation, we dont extract the numpy arrays\n    for the ROIs immediately. Instead, we just extract the metadata and\n    then store the weak-refs to the `ROIContourSequence` objects.\n\n    This allows us to avoid the computation of the numpy arrays until we\n    actually ask for them (i.e converting to `sitk.Image`) which might\n    use some regex pattern matching to only process the ROIs that\n    we want.\n    \"\"\"\n    if isinstance(dicom, (str, Path)):\n        dicom = Path(dicom)\n        if dicom.is_dir():\n            if len(list(dicom.glob(\"*.dcm\"))) == 1:\n                dicom = list(dicom.glob(\"*.dcm\"))[0]\n            else:\n                errmsg = (\n                    f\"Directory `{dicom}` contains multiple DICOM files. \"\n                    f\"Cannot determine which one to load.\"\n                )\n                raise ValueError(errmsg)\n\n    # logger.debug(\"Loading RTSTRUCT DICOM file.\", dicom=dicom)\n    dicom_rt: FileDataset = load_dicom(dicom)\n    metadata = metadata or extract_metadata(\n        dicom_rt,\n        \"RTSTRUCT\",\n        extra_tags=None,\n    )\n    rt = cls(\n        metadata={k: v for k, v in metadata.items() if v},\n    )\n\n    # Extract ROI contour points for each ROI and\n    for roi_index, roi_name in enumerate(metadata[\"ROINames\"]):  # type: ignore[arg-type]\n        try:\n            extracted_roi: Sequence = cls._get_roi_points(\n                dicom_rt,\n                roi_index=roi_index,\n                roi_name=roi_name,\n            )\n        except ROIContourError as ae:\n            rt.roi_map_errors[roi_name] = ae\n        else:\n            rt.roi_map[roi_name] = extracted_roi\n            rt.roi_names.append(roi_name)\n\n    # tag the logger with the original RTSTRUCT file name\n    rt.plogger = logger.bind(\n        PatientID=metadata.get(\"PatientID\", \"Unknown\"),\n        SeriesInstanceUID=metadata.get(\"SeriesInstanceUID\", \"Unknown\")[  # type: ignore[index]\n            -8:\n        ],\n        filepath=Path(dicom) if isinstance(dicom, (str, Path)) else dicom,\n    )\n    return rt\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.from_dicom(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.from_dicom(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_mask_ndarray","title":"get_mask_ndarray","text":"<pre><code>get_mask_ndarray(\n    reference_image: imgtools.coretypes.MedImage,\n    roi_name: str,\n    mask_img_size: tuple[int, int, int, int],\n    continuous: bool = False,\n) -&gt; numpy.ndarray\n</code></pre> <p>Get the mask as a numpy array for the specified ROI.</p> <p>Extracts &amp; rasterizes the contour points for the specified ROI from the RTSTRUCT into a 3D binary, numpy array aligned with the reference image geometry.</p> <p>Internally, we convert the physical coordinates of each ROI's contours (stored in the <code>ROIContourSequence[roi_index].ContourSequence.ContourData</code> attribute) into pixel indices representing the boundaries of the contour using <code>physical_points_to_idxs</code> and then fill the mask using <code>skimage.draw.polygon2mask</code>.</p> <p>One key assumption here is that each 2D contour lies on a single axial slice. We raise explicit errors if we detect contours that span multiple slices or odd Z-index behavior, to be aligned with our assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.coretypes.MedImage</code> <p>The reference image that defines the spatial geometry and metadata (e.g., direction, spacing, origin).</p> required <code>str</code> <p>Name of the ROI to extract.</p> required <code>tuple[int, int, int, int]</code> <p>Target shape of the binary mask, including depth (z), height (y), width (x), and number of channels (usually 1 here).</p> required <code>bool</code> <p>If True, use continuous interpolation for index conversion. This can lead to non-integer Z-slice indices, which will raise errors. Set to False (default) for safety.</p> <code>False</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>3D binary mask with shape (Z, Y, X), where the ROI has value 1 and background 0.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def get_mask_ndarray(\n    self,\n    reference_image: MedImage,\n    roi_name: str,\n    mask_img_size: tuple[int, int, int, int],\n    continuous: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Get the mask as a numpy array for the specified ROI.\n\n    Extracts &amp; rasterizes the contour points for the specified ROI from\n    the RTSTRUCT into a 3D binary, numpy array aligned with the reference\n    image geometry.\n\n    Internally, we convert the physical coordinates of each ROI's contours\n    (stored in the `ROIContourSequence[roi_index].ContourSequence.ContourData`\n    attribute) into pixel indices representing the boundaries of the\n    contour using `physical_points_to_idxs` and then fill the mask\n    using `skimage.draw.polygon2mask`.\n\n    One key assumption here is that each 2D contour lies on a single axial slice.\n    We raise explicit errors if we detect contours that span multiple slices or\n    odd Z-index behavior, to be aligned with our assumptions.\n\n    Parameters\n    ----------\n    reference_image : MedImage\n        The reference image that defines the spatial geometry and metadata\n        (e.g., direction, spacing, origin).\n    roi_name : str\n        Name of the ROI to extract.\n    mask_img_size : tuple[int, int, int, int]\n        Target shape of the binary mask, including depth (z), height (y), width (x),\n        and number of channels (usually 1 here).\n    continuous : bool, optional\n        If True, use continuous interpolation for index conversion. This can lead to\n        non-integer Z-slice indices, which will raise errors. Set to False (default)\n        for safety.\n\n    Returns\n    -------\n    np.ndarray\n        3D binary mask with shape (Z, Y, X), where the ROI has value 1 and background 0.\n\n    Raises\n    ------\n    ContourPointsAcrossSlicesError\n        If a single contour spans multiple Z slices.\n    MaskArrayOutOfBoundsError\n        If a Z-index falls outside of expected image bounds.\n    UnexpectedContourPointsError\n        If Z-values are malformed or structurally unexpected.\n    NonIntegerZSliceIndexError\n        If continuous=True and Z-index is not an integer.\n    \"\"\"\n    if roi_name in self._roi_cache:\n        return self._roi_cache[roi_name]\n\n    slices = [\n        np.array(slc.ContourData).reshape(-1, 3)\n        for slc in self.roi_map[roi_name]\n    ]\n\n    mask_points = physical_points_to_idxs(\n        reference_image, slices, continuous=continuous\n    )\n\n    mask_array_3d = np.zeros(\n        mask_img_size[0:3],\n        dtype=np.uint8,\n    )\n\n    for contour_num, contour in enumerate(mask_points, start=0):\n        # split the contour into z values and the points\n        uniq_z_vals = list(np.unique(contour[:, 0]))\n        slice_points = contour[:, 1:]\n\n        # lets make sure that z is 1 unique value\n        match uniq_z_vals:\n            # make sure single z value is not negative\n            case [z] if (z &lt; 0) or (z &gt;= mask_img_size[0]):\n                raise MaskArrayOutOfBoundsError(\n                    roi_name,\n                    contour_num,\n                    z_int=z,\n                    mask_shape=mask_img_size,\n                    slice_points_shape=slice_points.shape,\n                    z_values=uniq_z_vals,\n                    reference_image=reference_image,\n                )\n            case [z] if not float(z).is_integer():\n                raise NonIntegerZSliceIndexError(\n                    roi_name,\n                    contour_num,\n                    z_idx=z,\n                )\n            case [z] if len(uniq_z_vals) == 1:\n                z_idx = z\n            case [*z_values]:\n                raise ContourPointsAcrossSlicesError(\n                    roi_name,\n                    contour_num,\n                    slice_points.shape,\n                    z_values,\n                )\n            case _:\n                raise UnexpectedContourPointsError(\n                    roi_name,\n                    contour_num,\n                    uniq_z_vals,\n                )\n\n        filled_mask_array = polygon2mask(\n            mask_img_size[1:3],\n            slice_points,\n        )\n\n        mask_array_3d[z_idx, :, :] = np.logical_or(\n            mask_array_3d[z_idx, :, :], filled_mask_array\n        )\n\n    # Store the mask in the cache\n    self._roi_cache[roi_name] = mask_array_3d\n\n    return mask_array_3d\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_mask_ndarray(reference_image)","title":"<code>reference_image</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_mask_ndarray(roi_name)","title":"<code>roi_name</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_mask_ndarray(mask_img_size)","title":"<code>mask_img_size</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_mask_ndarray(continuous)","title":"<code>continuous</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_vector_mask","title":"get_vector_mask","text":"<pre><code>get_vector_mask(\n    reference_image: imgtools.coretypes.MedImage,\n    roi_matcher: imgtools.coretypes.masktypes.roi_matching.ROIMatcher,\n) -&gt; imgtools.coretypes.base_masks.VectorMask | None\n</code></pre> <p>Contruct multi-channel (vector) mask using ROI matching.</p> <p>This function applies the given <code>ROIMatcher</code> to select ROIs and stack the resulting binary masks into a single 4D array (Z, Y, X, C), where C is the number of matched ROI keys. The logic is designed to be interpretable and efficient, avoiding full extraction until needed.</p> <p>Each ROI group is resolved according to the matching strategy in <code>ROIMatcher</code>. - MERGE: all matching ROIs are squashed into a single mask - KEEP_FIRST: only the first match is used - SEPARATE: each match becomes its own label (preferred for downstream tasks)</p> <p>We ensure that each voxel in the output mask belongs to at most one structure per channel. However, we do not check for inter-channel overlap at this stage\u2014that responsibility is left to the caller.</p> <p>The returned image is a <code>sitk.Image</code> with vector pixel type (<code>sitk.sitkVectorUInt8</code>) and can be converted to our <code>VectorMask</code> class for further manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.coretypes.MedImage</code> <p>The image whose geometry defines the spatial alignment of the masks.</p> required <code>imgtools.coretypes.masktypes.roi_matching.ROIMatcher</code> <p>Matcher used to resolve user-defined keys to actual ROI names.</p> required <p>Returns:</p> Type Description <code>tuple[SimpleITK.Image, dict[int, imgtools.coretypes.base_masks.ROIMaskMapping]]</code> <p>A SimpleITK vector image containing all extracted ROIs in separate channels, and a dictionary mapping channel indices to <code>ROIMaskMapping</code>, a named tuple containing the roi_key and roi_names.</p> Notes <p>This method is designed to be the bridge between raw RTSTRUCT metadata and a usable segmentation mask, encoded in a vector-friendly format. Its companion class <code>VectorMask</code> offers high-level access to individual ROIs, label conversion, and overlap inspection.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def get_vector_mask(\n    self,\n    reference_image: MedImage,\n    roi_matcher: ROIMatcher,\n) -&gt; VectorMask | None:\n    # ) -&gt; tuple[sitk.Image | None, dict[int, ROIMaskMapping]]:\n    \"\"\"Contruct multi-channel (vector) mask using ROI matching.\n\n    This function applies the given `ROIMatcher` to select ROIs and stack\n    the resulting binary masks into a single 4D array (Z, Y, X, C), where C is\n    the number of matched ROI keys. The logic is designed to be interpretable\n    and efficient, avoiding full extraction until needed.\n\n    Each ROI group is resolved according to the matching strategy in `ROIMatcher`.\n    - MERGE: all matching ROIs are squashed into a single mask\n    - KEEP_FIRST: only the first match is used\n    - SEPARATE: each match becomes its own label (preferred for downstream tasks)\n\n    We ensure that each voxel in the output mask belongs to at most one\n    structure per channel.\n    However, **we do not check for inter-channel overlap\n    at this stage**\u2014that responsibility is left to the caller.\n\n    The returned image is a `sitk.Image` with vector pixel type (`sitk.sitkVectorUInt8`)\n    and can be converted to our `VectorMask` class for further manipulation.\n\n    Parameters\n    ----------\n    reference_image : MedImage\n        The image whose geometry defines the spatial alignment of the masks.\n    roi_matcher : ROIMatcher\n        Matcher used to resolve user-defined keys to actual ROI names.\n\n    Returns\n    -------\n    tuple[sitk.Image, dict[int, ROIMaskMapping]]\n        A SimpleITK vector image containing all extracted ROIs in separate channels,\n        and a dictionary mapping channel indices to `ROIMaskMapping`,\n        a named tuple containing the roi_key and roi_names.\n\n    Raises\n    ------\n    MissingROIError\n        If no ROIs matched the specified patterns and the ROIMatchFailurePolicy is ERROR.\n\n    Notes\n    -----\n    This method is designed to be the bridge between raw RTSTRUCT metadata\n    and a usable segmentation mask, encoded in a vector-friendly format.\n    Its companion class `VectorMask` offers high-level access to\n    individual ROIs, label conversion, and overlap inspection.\n    \"\"\"\n    matched_rois: list[tuple[str, list[str]]] = roi_matcher.match_rois(\n        self.roi_names\n    )\n\n    # Handle the case where no matches were found, according to the policy\n    if not matched_rois:\n        message = \"No ROIs matched any patterns in the match_map.\"\n        match roi_matcher.on_missing_regex:\n            case ROIMatchFailurePolicy.IGNORE:\n                # Silently return None\n                pass\n            case ROIMatchFailurePolicy.WARN:\n                self.plogger.warning(\n                    message,\n                    roi_names=self.roi_names,\n                    roi_matching=roi_matcher.match_map,\n                )\n            case ROIMatchFailurePolicy.ERROR:\n                # Raise an error\n                errmsg = f\"{message} Available ROIs: {self.roi_names}, \"\n                raise ROIMatchingError(\n                    errmsg,\n                    roi_names=self.roi_names,\n                    match_patterns=roi_matcher.match_map,\n                )\n        return None\n\n    self.plogger.debug(\"Matched ROIs\", matched_rois=matched_rois)\n\n    ref_size = reference_image.size\n    mask_img_size: tuple[int, int, int, int] = (\n        ref_size.depth,\n        ref_size.height,\n        ref_size.width,\n        len(matched_rois),\n    )\n\n    mask_array_4d = np.zeros(\n        mask_img_size,\n        dtype=np.uint8,\n    )\n\n    # we need something to store the mapping\n    # so that we can keep track of what the 3D mask matches to\n    # the original roi name(s)\n    mapping: dict[int, ROIMaskMapping] = {}\n\n    for iroi, (roi_key, matches) in enumerate(matched_rois):\n        self.plogger.debug(\n            f\"Processing {roi_key=} &amp; {matches=} : ({iroi + 1}/{len(matched_rois)})\",\n        )\n        match matches:\n            case [*many_rois]:\n                # most likely handle type MERGE\n                for roi_name in many_rois:\n                    mask_3d = self.get_mask_ndarray(\n                        reference_image,\n                        roi_name,\n                        mask_img_size,\n                        continuous=False,\n                    )\n                    # here we want to combine the masks in the same 4th dimension\n                    mask_array_4d[:, :, :, iroi] = np.logical_or(\n                        mask_array_4d[:, :, :, iroi], mask_3d\n                    )\n                # image_id depends on the roi_matcher.handling_strategy\n                # if merging, image_id is just the key\n                # if separate, image_id is {roi_key}__[{roi_name}]\n                # if keeping first, image_id is {roi_key}__[{roi_name}]\n\n                mapping[iroi] = ROIMaskMapping(\n                    roi_key=roi_key,\n                    roi_names=many_rois,\n                    image_id=roi_key\n                    if roi_matcher.handling_strategy.value == \"merge\"\n                    else f\"{roi_key}__[{many_rois[0]}]\",\n                )\n\n    # convert to sitk image\n    mask_image = sitk.GetImageFromArray(mask_array_4d, isVector=True)\n    mask_image.CopyInformation(reference_image)\n\n    assert mask_image.GetPixelIDValue() == 13\n    assert mask_image.GetNumberOfComponentsPerPixel() == len(matched_rois)\n\n    return VectorMask(\n        mask_image,\n        mapping,\n        metadata=self.metadata,\n        errors=self.roi_map_errors,\n    )\n</code></pre>"},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_vector_mask(reference_image)","title":"<code>reference_image</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.RTStructureSet.get_vector_mask(roi_matcher)","title":"<code>roi_matcher</code>","text":""},{"location":"reference/coretypes/masktypes/structureset/#imgtools.coretypes.masktypes.structureset.UnexpectedContourPointsError","title":"UnexpectedContourPointsError","text":"<pre><code>UnexpectedContourPointsError(\n    roi_name: str, contour_num: int, z_values: list\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when contour points have an unexpected structure.</p> Source code in <code>src/imgtools/coretypes/masktypes/structureset.py</code> <pre><code>def __init__(\n    self,\n    roi_name: str,\n    contour_num: int,\n    z_values: list,\n) -&gt; None:\n    self.roi_name = roi_name\n    self.contour_num = contour_num\n    self.z_values = z_values\n    super().__init__(self._generate_message())\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/","title":"Coord types","text":""},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types","title":"coord_types","text":"<p>helper_types.py: Intuitive and reusable types for 3D spatial operations.</p> <p>This module defines foundational types to represent and manipulate 3D spatial concepts intuitively, focusing on clarity, usability, and alignment with medical imaging workflows. These types aim to simplify operations like spatial transformations, bounding box calculations, and metadata representation.</p>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types--goals","title":"Goals:","text":"<ul> <li>Intuitive Design: Variable names and methods should be easy to understand   and reflect their real-world meaning.</li> <li>Reusability: Types should be generic enough to apply to various spatial   operations across domains, especially medical imaging.</li> </ul>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Coordinate3D","title":"Coordinate3D  <code>dataclass</code>","text":"<pre><code>Coordinate3D(*args: int)\n</code></pre> <p>Represent a point in 3D space.</p> <p>Can add and subtract other Point3D or Size3D objects.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>int</code> <p>X-component of the vector.</p> <code>y</code> <code>int</code> <p>Y-component of the vector.</p> <code>z</code> <code>int</code> <p>Z-component of the vector.</p> <p>Methods:</p> Name Description <code>__add__</code> <p>Add another Coordinate3D to this vector.</p> <code>__sub__</code> <p>Subtract another Coordinate3D from this vector.</p> <code>__iter__</code> <p>Iterate over the components (x, y, z).</p> <code>__getitem__</code> <p>Access components via index.</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def __init__(self, *args: int) -&gt; None:\n    \"\"\"Initialize a Coordinate3D with x, y, z components.\"\"\"\n    match args:\n        case [x, y, z]:\n            self.x, self.y, self.z = x, y, z\n        case [tuple_points] if isinstance(tuple_points, tuple):\n            self.x, self.y, self.z = tuple_points\n        case _:\n            errmsg = (\n                f\"{self.__class__.__name__} expects 3 values for x, y, z.\"\n                f\" Got {len(args)} values for {args}.\"\n            )\n            raise ValueError(errmsg)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Coordinate3D.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple() -&gt; tuple[int, int, int]\n</code></pre> <p>Return the (x, y, z) tuple representation.</p> <p>Returns:</p> Type Description <code>tuple of int</code> <p>The coordinate as (x, y, z).</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def to_tuple(self) -&gt; tuple[int, int, int]:\n    \"\"\"Return the (x, y, z) tuple representation.\n\n    Returns\n    -------\n    tuple of int\n        The coordinate as (x, y, z).\n    \"\"\"\n    return (self.x, self.y, self.z)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Size3D","title":"Size3D  <code>dataclass</code>","text":"<pre><code>Size3D(*args: int)\n</code></pre> <p>Represent the size (width, height, depth) of a 3D object.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>The width of the 3D object.</p> <code>height</code> <code>int</code> <p>The height of the 3D object.</p> <code>depth</code> <code>int</code> <p>The depth of the 3D object.</p> <p>Methods:</p> Name Description <code>volume:</code> <p>Calculate the volume of the 3D object.</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def __init__(self, *args: int) -&gt; None:\n    match args:\n        case [int() as width, int() as height, int() as depth]:\n            self.width, self.height, self.depth = width, height, depth\n        case [tuple_points] if isinstance(tuple_points, tuple):\n            self.width, self.height, self.depth = map(int, tuple_points)\n        case _:\n            errmsg = (\n                f\"{self.__class__.__name__} expects 3 integer values for width, height, depth.\"\n                f\" Got {len(args)} values for {args}.\"\n            )\n            raise ValueError(errmsg)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Size3D.volume","title":"volume  <code>property</code>","text":"<pre><code>volume: int\n</code></pre> <p>Calculate the volume of the 3D object.</p>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Size3D.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple() -&gt; tuple[int, int, int]\n</code></pre> <p>Return the (width, height, depth) tuple representation.</p> <p>Returns:</p> Type Description <code>tuple of int</code> <p>The size as (width, height, depth).</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def to_tuple(self) -&gt; tuple[int, int, int]:\n    \"\"\"Return the (width, height, depth) tuple representation.\n\n    Returns\n    -------\n    tuple of int\n        The size as (width, height, depth).\n    \"\"\"\n    return (self.width, self.height, self.depth)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Spacing3D","title":"Spacing3D","text":"<pre><code>Spacing3D(*args: float)\n</code></pre> <p>Represent the spacing in 3D space.</p> <p>Methods:</p> Name Description <code>to_tuple</code> <p>Return the (x, y, z) spacing as a tuple.</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def __init__(self, *args: float) -&gt; None:\n    \"\"\"Initialize a Spacing3D with x, y, z components.\"\"\"\n    match args:\n        case [x, y, z]:\n            self.x, self.y, self.z = x, y, z\n        case [tuple_points] if isinstance(tuple_points, tuple):\n            if len(tuple_points) != 3:\n                errmsg = (\n                    f\"{self.__class__.__name__} expects 3 values for x, y, z.\"\n                    f\" Got {len(tuple_points)} values for {tuple_points}.\"\n                )\n                raise ValueError(errmsg)\n            self.x, self.y, self.z = tuple_points\n        case _:\n            errmsg = (\n                f\"{self.__class__.__name__} expects 3 values for x, y, z.\"\n                f\" Got {len(args)} values for {args}.\"\n            )\n            raise ValueError(errmsg)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/coord_types/#imgtools.coretypes.spatial_types.coord_types.Spacing3D.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple() -&gt; tuple[float, float, float]\n</code></pre> <p>Return the (x, y, z) spacing as a tuple.</p> <p>Returns:</p> Type Description <code>tuple of float</code> <p>The spacing as (x, y, z).</p> Source code in <code>src/imgtools/coretypes/spatial_types/coord_types.py</code> <pre><code>def to_tuple(self) -&gt; tuple[float, float, float]:\n    \"\"\"Return the (x, y, z) spacing as a tuple.\n\n    Returns\n    -------\n    tuple of float\n        The spacing as (x, y, z).\n    \"\"\"\n    return (self.x, self.y, self.z)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/direction/","title":"Direction","text":""},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction","title":"direction","text":"<p>This module defines the Direction class, which stores a 3x3 orientation matrix as a flattened tuple of nine floats (row-major order). You can convert it to a 3x3 structure, normalize row vectors, or check if rows are normalized.</p>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction","title":"Direction  <code>dataclass</code>","text":"<pre><code>Direction(\n    matrix: imgtools.coretypes.spatial_types.direction.Matrix3DFlat,\n)\n</code></pre> <p>Represent a directional matrix for image orientation.</p> <p>Supports 3D (3x3) directional matrices in row-major format as 9 floats. It's often useful when you need to keep track of orientation data in a compact way.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>imgtools.coretypes.spatial_types.direction.Matrix3DFlat</code> <p>Flattened representation of a 3x3 matrix.</p> <p>Methods:</p> Name Description <code>flip_axis</code> <p>Flip the matrix along a specified axis.</p> <code>from_matrix</code> <p>Create a Direction instance from a nested 3x3 tuple.</p> <code>is_normalized</code> <p>Check if all values are (almost) 1, given a tolerance.</p> <code>normalize</code> <p>Return a new Direction with normalized row vectors.</p> <code>to_matrix</code> <p>Convert the flattened row-major array back to a 3D matrix.</p>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.flip_axis","title":"flip_axis","text":"<pre><code>flip_axis(\n    axis: int,\n) -&gt; imgtools.coretypes.spatial_types.direction.Direction\n</code></pre> <p>Flip the matrix along a specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The axis to flip (0, 1, or 2).</p> required <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.direction.Direction</code> <p>A new instance with the flipped matrix.</p> Source code in <code>src/imgtools/coretypes/spatial_types/direction.py</code> <pre><code>def flip_axis(self, axis: int) -&gt; Direction:\n    \"\"\"Flip the matrix along a specified axis.\n\n    Parameters\n    ----------\n    axis : int\n        The axis to flip (0, 1, or 2).\n\n    Returns\n    -------\n    Direction\n        A new instance with the flipped matrix.\n    \"\"\"\n    if axis not in (0, 1, 2):\n        raise ValueError(\"Axis must be 0 (x), 1 (y), or 2 (z).\")\n    matrix = self.to_matrix()\n    matrix[axis] = [-v for v in matrix[axis]]\n    return Direction.from_matrix(\n        tuple(tuple(row) for row in matrix)  # type: ignore\n    )\n</code></pre>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.flip_axis(axis)","title":"<code>axis</code>","text":""},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.from_matrix","title":"from_matrix  <code>classmethod</code>","text":"<pre><code>from_matrix(\n    matrix: imgtools.coretypes.spatial_types.direction.Matrix3D,\n) -&gt; imgtools.coretypes.spatial_types.direction.Direction\n</code></pre> <p>Create a Direction instance from a nested 3x3 tuple.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.coretypes.spatial_types.direction.Matrix3D</code> <p>A tuple of 3 rows, each row having 3 floats.</p> required <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.direction.Direction</code> <p>An instance with the flattened matrix.</p> Source code in <code>src/imgtools/coretypes/spatial_types/direction.py</code> <pre><code>@classmethod\ndef from_matrix(\n    cls,\n    matrix: Matrix3D,\n) -&gt; Direction:\n    \"\"\"\n    Create a Direction instance from a nested 3x3 tuple.\n\n    Parameters\n    ----------\n    matrix : Matrix3D\n        A tuple of 3 rows, each row having 3 floats.\n\n    Returns\n    -------\n    Direction\n        An instance with the flattened matrix.\n\n    Raises\n    ------\n    ValueError\n        If the input isn't a 3x3 structure.\n    \"\"\"\n    if (size := len(matrix)) != 3:\n        msg = f\"Matrix must be 3x3. Got {size=}.\"\n        raise ValueError(msg)\n    for row in matrix:\n        if len(row) != size:\n            raise ValueError(\"Matrix must be square (3x3).\")\n    flattened: FlattenedMatrix = tuple(\n        value for row in matrix for value in row\n    )  # type: ignore\n    return cls(matrix=flattened)\n</code></pre>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.from_matrix(matrix)","title":"<code>matrix</code>","text":""},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.is_normalized","title":"is_normalized","text":"<pre><code>is_normalized(tol: float = 1e-06) -&gt; bool\n</code></pre> <p>Check if all values are (almost) 1, given a tolerance.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>Acceptable deviation from 1.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all rows meet the norm requirement, else False.</p> Source code in <code>src/imgtools/coretypes/spatial_types/direction.py</code> <pre><code>def is_normalized(self, tol: float = 1e-6) -&gt; bool:\n    \"\"\"Check if all values are (almost) 1, given a tolerance.\n\n    Parameters\n    ----------\n    tol : float, optional\n        Acceptable deviation from 1.\n\n    Returns\n    -------\n    bool\n        True if all rows meet the norm requirement, else False.\n    \"\"\"\n    matrix = self.to_matrix()\n    for row in matrix:\n        if not np.isclose(np.linalg.norm(row), 1.0, atol=tol):\n            return False\n    return True\n</code></pre>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.is_normalized(tol)","title":"<code>tol</code>","text":""},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.normalize","title":"normalize","text":"<pre><code>normalize() -&gt; (\n    imgtools.coretypes.spatial_types.direction.Direction\n)\n</code></pre> <p>Return a new Direction with normalized row vectors.</p> <p>Zero rows remain unchanged.</p> <p>Returns:</p> Type Description <code>imgtools.coretypes.spatial_types.direction.Direction</code> <p>A new instance with normalized rows.</p> Source code in <code>src/imgtools/coretypes/spatial_types/direction.py</code> <pre><code>def normalize(self) -&gt; Direction:\n    \"\"\"Return a new Direction with normalized row vectors.\n\n    Zero rows remain unchanged.\n\n    Returns\n    -------\n    Direction\n        A new instance with normalized rows.\n    \"\"\"\n    matrix = self.to_matrix()\n    normalized_matrix = [\n        list(np.array(row) / np.linalg.norm(row)) for row in matrix\n    ]\n    return Direction.from_matrix(\n        tuple(tuple(row) for row in normalized_matrix)  # type: ignore\n    )\n</code></pre>"},{"location":"reference/coretypes/spatial_types/direction/#imgtools.coretypes.spatial_types.direction.Direction.to_matrix","title":"to_matrix","text":"<pre><code>to_matrix() -&gt; list[list[float]]\n</code></pre> <p>Convert the flattened row-major array back to a 3D matrix.</p> <p>Returns:</p> Type Description <code>list of list of float</code> <p>The 3x3 data, row by row.</p> Source code in <code>src/imgtools/coretypes/spatial_types/direction.py</code> <pre><code>def to_matrix(self) -&gt; list[list[float]]:\n    \"\"\"Convert the flattened row-major array back to a 3D matrix.\n\n    Returns\n    -------\n    list of list of float\n        The 3x3 data, row by row.\n    \"\"\"\n    dim = 3\n    return [list(self.matrix[i * dim : (i + 1) * dim]) for i in range(dim)]\n</code></pre>"},{"location":"reference/coretypes/spatial_types/image_geometry/","title":"Image geometry","text":""},{"location":"reference/coretypes/spatial_types/image_geometry/#imgtools.coretypes.spatial_types.image_geometry","title":"image_geometry","text":""},{"location":"reference/coretypes/spatial_types/image_geometry/#imgtools.coretypes.spatial_types.image_geometry.ImageGeometry","title":"ImageGeometry  <code>dataclass</code>","text":"<pre><code>ImageGeometry(\n    size: imgtools.coretypes.spatial_types.Size3D,\n    origin: imgtools.coretypes.spatial_types.Coordinate3D,\n    direction: imgtools.coretypes.spatial_types.Direction,\n    spacing: imgtools.coretypes.spatial_types.Spacing3D,\n)\n</code></pre> <p>Represents the geometry of a 3D image.</p>"},{"location":"reference/dicom/dicom_find/","title":"Dicom find","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find","title":"dicom_find","text":"<p>Functions:</p> Name Description <code>convert_to_case_insensitive</code> <p>Convert the file extension to a case-insensitive format.</p> <code>find_dicoms</code> <p>Locate DICOM files in a specified directory.</p>"},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.convert_to_case_insensitive","title":"convert_to_case_insensitive","text":"<pre><code>convert_to_case_insensitive(extension: str) -&gt; str\n</code></pre> <p>Convert the file extension to a case-insensitive format.</p> <p>This is done by converting each character in the extension to a case-insensitive pattern using character sets. For example, 'dcm' becomes '[dD][cC][mM]'.</p> Source code in <code>src/imgtools/dicom/dicom_find.py</code> <pre><code>def convert_to_case_insensitive(extension: str) -&gt; str:\n    \"\"\"Convert the file extension to a case-insensitive format.\n\n    This is done by converting each character in the extension to a\n    case-insensitive pattern using character sets. For example, 'dcm'\n    becomes '[dD][cC][mM]'.\n    \"\"\"\n    if not extension:\n        return \"\"\n\n    # Convert the extension to lowercase\n    lower_extension = extension.lower()\n\n    # permutate combinations of each letter in the extension\n    # to create a case-insensitive pattern\n    case_insensitive_extension = \"\".join(\n        f\"[{char.lower()}{char.upper()}]\" for char in lower_extension\n    )\n    return case_insensitive_extension\n</code></pre>"},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms","title":"find_dicoms","text":"<pre><code>find_dicoms(\n    directory: pathlib.Path,\n    recursive: bool = True,\n    check_header: bool = False,\n    extension: str = \"dcm\",\n    case_sensitive: bool = False,\n    limit: int | None = None,\n    search_input: typing.List[str] | None = None,\n) -&gt; typing.List[pathlib.Path]\n</code></pre> <p>Locate DICOM files in a specified directory.</p> <p>This function scans a directory for files matching the specified extension and validates them as DICOM files based on the provided options. It supports recursive search and optional header validation to confirm file validity.</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The directory in which to search for DICOM files.</p> required <code>bool</code> <p>Whether to include subdirectories in the search</p> <code>True</code> <code>bool</code> <p>Whether to validate files by checking for a valid DICOM header.     - If <code>True</code>, perform DICOM header validation (slower but more accurate).     - If <code>False</code>, skip header validation and rely on extension.</p> <code>False</code> <code>str</code> <p>File extension to search for (e.g., \"dcm\"). If <code>None</code>, consider all files regardless of extension.</p> <code>\"dcm\"</code> <code>bool</code> <p>Whether to perform a case-sensitive search for the file extension. If <code>False</code>, the search is case-insensitive.</p> <code>False</code> <code>int</code> <p>Maximum number of DICOM files to return. If <code>None</code>, return all found files.</p> <code>None</code> <code>typing.List[str]</code> <p>List of terms to filter files by. Only files containing all terms in their paths will be included. If <code>None</code>, no filtering is applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>typing.List[pathlib.Path]</code> <p>A list of valid DICOM file paths found in the directory.</p> Notes <ul> <li>If <code>check_header</code> is enabled, the function checks each file for a valid     DICOM header, which may slow down the search process.</li> </ul> <p>Examples:</p> <p>Setup</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from imgtools.dicom.dicom_find import find_dicoms\n</code></pre> <p>Find DICOM files recursively without header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), PosixPath('/data/subdir/scan3.DCM')]\n</code></pre> <p>Suppose that <code>scan3.DCM</code> is not a valid DICOM file. Find DICOM files with header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=True,\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files without recursion:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=False,\n...     check_header=False,\n... )\n[PosixPath('/data/scan1.dcm')]\n</code></pre> <p>Find DICOM files with a specific extension:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     extension=\"dcm\",\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files with a search input (substring match):</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     search_input=[\"1\", \"scan2\"],\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files with a limit:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     limit=1,\n... )\n[PosixPath('/data/scan1.dcm')]\n</code></pre> <p>Find DICOM files with all options:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=True,\n...     extension=\"dcm\",\n...     limit=2,\n...     search_input=[\"scan\"],\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> Source code in <code>src/imgtools/dicom/dicom_find.py</code> <pre><code>def find_dicoms(\n    directory: Path,\n    recursive: bool = True,\n    check_header: bool = False,\n    extension: str = \"dcm\",\n    case_sensitive: bool = False,\n    limit: int | None = None,\n    search_input: List[str] | None = None,\n) -&gt; List[Path]:\n    \"\"\"Locate DICOM files in a specified directory.\n\n    This function scans a directory for files matching the specified extension\n    and validates them as DICOM files based on the provided options. It supports\n    recursive search and optional header validation to confirm file validity.\n\n    Parameters\n    ----------\n    directory : Path\n        The directory in which to search for DICOM files.\n    recursive : bool\n        Whether to include subdirectories in the search\n    check_header : bool\n        Whether to validate files by checking for a valid DICOM header.\n            - If `True`, perform DICOM header validation (slower but more accurate).\n            - If `False`, skip header validation and rely on extension.\n    extension : str, default=\"dcm\"\n        File extension to search for (e.g., \"dcm\"). If `None`, consider all files\n        regardless of extension.\n    case_sensitive : bool, default=False\n        Whether to perform a case-sensitive search for the file extension.\n        If `False`, the search is case-insensitive.\n    limit : int, optional\n        Maximum number of DICOM files to return. If `None`, return all found files.\n    search_input : List[str], optional\n        List of terms to filter files by. Only files containing all terms\n        in their paths will be included. If `None`, no filtering is applied.\n\n    Returns\n    -------\n    List[Path]\n        A list of valid DICOM file paths found in the directory.\n\n    Notes\n    -----\n    - If `check_header` is enabled, the function checks each file for a valid\n        DICOM header, which may slow down the search process.\n\n    Examples\n    --------\n    Setup\n\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; from imgtools.dicom.dicom_find import find_dicoms\n\n    Find DICOM files recursively without header validation:\n\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), \\\nPosixPath('/data/subdir/scan3.DCM')]\n\n    Suppose that `scan3.DCM` is not a valid DICOM file. Find DICOM files with \\\nheader validation:\n\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=True,\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files without recursion:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=False,\n    ...     check_header=False,\n    ... )\n    [PosixPath('/data/scan1.dcm')]\n\n    Find DICOM files with a specific extension:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     extension=\"dcm\",\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files with a search input (substring match):\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     search_input=[\"1\", \"scan2\"],\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files with a limit:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     limit=1,\n    ... )\n    [PosixPath('/data/scan1.dcm')]\n\n    Find DICOM files with all options:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=True,\n    ...     extension=\"dcm\",\n    ...     limit=2,\n    ...     search_input=[\"scan\"],\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n    \"\"\"\n\n    files = filter_valid_dicoms(\n        directory,\n        check_header,\n        case_sensitive,\n        search_input,\n        extension or \"\",\n        recursive,\n    )\n\n    return list(islice(files, limit)) if limit else list(files)\n</code></pre>"},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(directory)","title":"<code>directory</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(check_header)","title":"<code>check_header</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(extension)","title":"<code>extension</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(case_sensitive)","title":"<code>case_sensitive</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(limit)","title":"<code>limit</code>","text":""},{"location":"reference/dicom/dicom_find/#imgtools.dicom.dicom_find.find_dicoms(search_input)","title":"<code>search_input</code>","text":""},{"location":"reference/dicom/dicom_reader/","title":"Dicom reader","text":""},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader","title":"dicom_reader","text":"<p>Functions:</p> Name Description <code>load_dicom</code> <p>Load a DICOM file and return the parsed FileDataset object.</p> <code>path_from_pathlike</code> <p>Return the string representation if file_object is path-like,</p>"},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.load_dicom","title":"load_dicom","text":"<pre><code>load_dicom(\n    dicom_input: imgtools.dicom.dicom_reader.DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n    **kwargs: typing.Any\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Load a DICOM file and return the parsed FileDataset object.</p> <p>This function supports various input types including file paths, byte streams, and file-like objects. It uses the <code>pydicom.dcmread</code> function to read the DICOM file.</p> Notes <ul> <li>If <code>dicom_input</code> is already a <code>FileDataset</code>, it is returned as is.</li> <li>If <code>dicom_input</code> is a file path or file-like object, it is read using <code>pydicom.dcmread</code>.</li> <li>If <code>dicom_input</code> is a byte stream, it is wrapped in a <code>BytesIO</code> object and then read.</li> <li>An <code>InvalidDicomError</code> is raised if the input type is unsupported.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.FileDataset | str | pathlib.Path | bytes | typing.BinaryIO</code> <p>Input DICOM file as a <code>pydicom.FileDataset</code>, file path, byte stream, or file-like object.</p> required <code>bool</code> <p>Whether to allow reading DICOM files missing the File Meta Information header, by default True.</p> <code>True</code> <code>bool</code> <p>Whether to stop reading the DICOM file before loading pixel data, by default True.</p> <code>True</code> <code>typing.Any</code> <p>Additional keyword arguments to pass to <code>pydicom.dcmread</code>. i.e <code>specific_tags</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pydicom.dataset.FileDataset</code> <p>Parsed DICOM dataset.</p> Source code in <code>src/imgtools/dicom/dicom_reader.py</code> <pre><code>def load_dicom(\n    dicom_input: DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n    **kwargs: Any,  # noqa: ANN401\n) -&gt; FileDataset:\n    \"\"\"Load a DICOM file and return the parsed FileDataset object.\n\n    This function supports various input types including file paths, byte streams,\n    and file-like objects. It uses the `pydicom.dcmread` function to read the DICOM file.\n\n    Notes\n    -----\n    - If `dicom_input` is already a `FileDataset`, it is returned as is.\n    - If `dicom_input` is a file path or file-like object, it is read using `pydicom.dcmread`.\n    - If `dicom_input` is a byte stream, it is wrapped in a `BytesIO` object and then read.\n    - An `InvalidDicomError` is raised if the input type is unsupported.\n\n    Parameters\n    ----------\n    dicom_input : FileDataset | str | Path | bytes | BinaryIO\n        Input DICOM file as a `pydicom.FileDataset`, file path, byte stream, or file-like object.\n    force : bool, optional\n        Whether to allow reading DICOM files missing the *File Meta Information*\n        header, by default True.\n    stop_before_pixels : bool, optional\n        Whether to stop reading the DICOM file before loading pixel data, by default True.\n    **kwargs\n        Additional keyword arguments to pass to `pydicom.dcmread`.\n        i.e `specific_tags`.\n    Returns\n    -------\n    FileDataset\n        Parsed DICOM dataset.\n\n    Raises\n    ------\n    InvalidDicomError\n        If the input is of an unsupported type or cannot be read as a DICOM file.\n    \"\"\"\n    match dicom_input:\n        case FileDataset():\n            return dicom_input\n        case str() | Path() | BinaryIO():\n            dicom_source = path_from_pathlike(dicom_input)\n            return dcmread(\n                dicom_source,\n                force=force,\n                stop_before_pixels=stop_before_pixels,\n                **kwargs,\n            )\n        case bytes():\n            return dcmread(\n                BytesIO(dicom_input),\n                force=force,\n                stop_before_pixels=stop_before_pixels,\n                **kwargs,\n            )\n        case _:\n            msg = (\n                f\"Invalid input type for 'dicom_input': {type(dicom_input)}. \"\n                \"Must be a FileDataset, str, Path, bytes, or BinaryIO object.\"\n            )\n            raise InvalidDicomError(msg)\n</code></pre>"},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.load_dicom(dicom_input)","title":"<code>dicom_input</code>","text":""},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.load_dicom(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.load_dicom(stop_before_pixels)","title":"<code>stop_before_pixels</code>","text":""},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.load_dicom(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.path_from_pathlike","title":"path_from_pathlike","text":"<pre><code>path_from_pathlike(\n    file_object: str | pathlib.Path | typing.BinaryIO,\n) -&gt; str | typing.BinaryIO\n</code></pre> <p>Return the string representation if file_object is path-like, otherwise return the object itself.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path | typing.BinaryIO</code> <p>File path or file-like object.</p> required <p>Returns:</p> Type Description <code>str | typing.BinaryIO</code> <p>String representation of the path or the original file-like object.</p> Source code in <code>src/imgtools/dicom/dicom_reader.py</code> <pre><code>def path_from_pathlike(file_object: str | Path | BinaryIO) -&gt; str | BinaryIO:\n    \"\"\"Return the string representation if file_object is path-like,\n    otherwise return the object itself.\n\n    Parameters\n    ----------\n    file_object : str | Path | BinaryIO\n        File path or file-like object.\n\n    Returns\n    -------\n    str | BinaryIO\n        String representation of the path or the original file-like object.\n    \"\"\"\n    try:\n        return os.fspath(file_object)  # type: ignore[arg-type]\n    except TypeError:\n        return cast(\"BinaryIO\", file_object)\n</code></pre>"},{"location":"reference/dicom/dicom_reader/#imgtools.dicom.dicom_reader.path_from_pathlike(file_object)","title":"<code>file_object</code>","text":""},{"location":"reference/dicom/interlacer/","title":"Interlacer","text":""},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer","title":"interlacer","text":"<p>Interlacer Module</p> <p>This module defines the <code>Interlacer</code> class, which constructs and queries a hierarchical forest of DICOM series based on their metadata relationships. It enables efficient grouping, querying, and visualization of medical imaging series.</p> <p>The Interlacer provides tools for analyzing complex DICOM relationships, such as connections between different imaging modalities (CT, MR, PT) and derived objects (RTSTRUCT, RTDOSE, SEG). It enables validation of relationships based on DICOM standards and medical imaging workflows.</p> <p>Classes:</p> Name Description <code>SeriesNode</code> <p>Represents an individual DICOM series and its hierarchical relationships.</p> <code>Interlacer</code> <p>Builds the hierarchy, processes queries, and visualizes the relationships.</p> <code>InterlacerQueryError</code> <p>Base exception for query validation errors.</p> <code>UnsupportedModalityError</code> <p>Raised when an unsupported modality is specified in a query.</p> <code>MissingDependencyModalityError</code> <p>Raised when modalities in a query are missing required dependencies.</p> <code>ModalityHighlighter</code> <p>Rich text highlighter for pretty-printing DICOM modalities.</p> Features <ul> <li>Hierarchical representation of DICOM relationships</li> <li>Query for specific combinations of modalities with dependency validation</li> <li>Interactive visualization of DICOM series relationships</li> <li>Rich text console display of patient/series hierarchies</li> <li>Validation of modality dependencies based on DICOM standards</li> </ul>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.DuplicateRowError","title":"DuplicateRowError","text":"<pre><code>DuplicateRowError()\n</code></pre> <p>               Bases: <code>imgtools.dicom.interlacer.InterlacerQueryError</code></p> <p>Raised when the index.csv file contains duplicate rows.</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def __init__(self) -&gt; None:\n    msg = \"The input file contains duplicate rows.\"\n    super().__init__(msg)\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer","title":"Interlacer  <code>dataclass</code>","text":"<pre><code>Interlacer(\n    crawl_index: str | pathlib.Path | pandas.DataFrame,\n)\n</code></pre> <p>Builds and queries a forest of SeriesNode objects from DICOM series data.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path | pandas.DataFrame</code> <p>Path to the CSV file or DataFrame containing the series data</p> required <p>Attributes:</p> Name Type Description <code>crawl_df</code> <code>pandas.DataFrame</code> <p>DataFrame containing the data loaded from the CSV file or passed in <code>crawl_index</code></p> <code>series_nodes</code> <code>dict[str, imgtools.utils.interlacer_utils.SeriesNode]</code> <p>Maps SeriesInstanceUID to SeriesNode objects</p> <code>root_nodes</code> <code>list[imgtools.utils.interlacer_utils.SeriesNode]</code> <p>List of root nodes in the forest</p> <p>Methods:</p> Name Description <code>print_tree</code> <p>Print a representation of the forest.</p> <code>query</code> <p>Query the forest for specific modalities.</p> <code>query_all</code> <p>Simply return ALL possible matches</p> <code>visualize_forest</code> <p>Visualize the forest as an interactive network graph.</p>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer(crawl_index)","title":"<code>crawl_index</code>","text":""},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.valid_queries","title":"valid_queries  <code>property</code>","text":"<pre><code>valid_queries: list[str]\n</code></pre> <p>Compute all valid queries based on the current forest. Mostly for debugging and informing the user of what we have determined what we can query.</p> <p>Essentially, traverse each possible branch path and add the modalities to a set, afterwards, permutate each element's subpaths that might be valid</p>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.print_tree","title":"print_tree","text":"<pre><code>print_tree(input_directory: pathlib.Path | None) -&gt; None\n</code></pre> <p>Print a representation of the forest.</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def print_tree(self, input_directory: Path | None) -&gt; None:\n    \"\"\"Print a representation of the forest.\"\"\"\n    print_interlacer_tree(self.root_nodes, input_directory)\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.query","title":"query","text":"<pre><code>query(\n    query_string: str, group_by_root: bool = True\n) -&gt; list[list[imgtools.utils.interlacer_utils.SeriesNode]]\n</code></pre> <p>Query the forest for specific modalities.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Comma-separated string of modalities to query (e.g., 'CT,MR')</p> required <code>bool</code> <p>If True, group the returned SeriesNodes by their root CT/MR/PT node (i.e., avoid duplicate root nodes across results).</p> <code>True</code> <p>Returns:</p> Type Description <code>list[list[dict[str, str]]]</code> <p>List of matched series groups where each series is represented by a dict containing 'Series' and 'Modality' keys</p> Notes <p>Supported modalities: - CT: Computed Tomography - PT: Positron Emission Tomography - MR: Magnetic Resonance Imaging - SEG: Segmentation - RTSTRUCT: Radiotherapy Structure - RTDOSE: Radiotherapy Dose</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def query(\n    self,\n    query_string: str,\n    group_by_root: bool = True,\n) -&gt; list[list[SeriesNode]]:\n    \"\"\"\n    Query the forest for specific modalities.\n\n    Parameters\n    ----------\n    query_string : str\n        Comma-separated string of modalities to query (e.g., 'CT,MR')\n\n    group_by_root : bool, default=True\n        If True, group the returned SeriesNodes by their root CT/MR/PT\n        node (i.e., avoid duplicate root nodes across results).\n\n    Returns\n    -------\n    list[list[dict[str, str]]]\n        List of matched series groups where each series is represented by a\n        dict containing 'Series' and 'Modality' keys\n\n    Notes\n    -----\n    Supported modalities:\n    - CT: Computed Tomography\n    - PT: Positron Emission Tomography\n    - MR: Magnetic Resonance Imaging\n    - SEG: Segmentation\n    - RTSTRUCT: Radiotherapy Structure\n    - RTDOSE: Radiotherapy Dose\n    \"\"\"\n    if query_string in [\"*\", \"all\"]:\n        query_results = self.query_all()\n    else:\n        queried_modalities = self._get_valid_query(query_string.split(\",\"))\n        query_results = self._query(queried_modalities)\n\n    if not group_by_root:\n        return query_results\n\n    grouped: dict[SeriesNode, set[SeriesNode]] = defaultdict(set)\n    # pretty much start with the root node, then add all branches\n    for path in query_results:\n        root = path[0]\n        grouped[root].update(path[1:])\n\n    # break each item into a list starting with key, then all the values\n    return [[key] + list(value) for key, value in grouped.items()]\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.query(query_string)","title":"<code>query_string</code>","text":""},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.query(group_by_root)","title":"<code>group_by_root</code>","text":""},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.query_all","title":"query_all","text":"<pre><code>query_all() -&gt; (\n    list[list[imgtools.utils.interlacer_utils.SeriesNode]]\n)\n</code></pre> <p>Simply return ALL possible matches Note this has a different approach than query, since we dont care about the order of the modalities, just that they exist in the Branch</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def query_all(self) -&gt; list[list[SeriesNode]]:\n    \"\"\"Simply return ALL possible matches\n    Note this has a different approach than query, since we dont care\n    about the order of the modalities, just that they exist in the\n    Branch\n    \"\"\"\n    results: list[list[SeriesNode]] = []\n\n    def dfs(node: SeriesNode, path: list[SeriesNode]) -&gt; None:\n        path.append(node)\n        if len(node.children) == 0:\n            # If this is a leaf node, check if the path is unique\n            # but first, if the path has any 'RTPLAN' nodes, remove them\n            # TODO:: create a global VALID_MODALITIES list instead of hardcoding\n            cleaned_path = [n for n in path if n.Modality != \"RTPLAN\"]\n            if cleaned_path not in results:\n                results.append(cleaned_path)\n\n        for child in node.children:\n            dfs(child, path.copy())\n\n    for root in self.root_nodes:\n        dfs(root, [])\n    return results\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.visualize_forest","title":"visualize_forest","text":"<pre><code>visualize_forest(\n    save_path: str | pathlib.Path,\n) -&gt; pathlib.Path\n</code></pre> <p>Visualize the forest as an interactive network graph.</p> <p>Creates an HTML visualization showing nodes for each SeriesNode and edges for parent-child relationships.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>Path to save the HTML visualization.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Path to the saved HTML visualization</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def visualize_forest(self, save_path: str | Path) -&gt; Path:\n    \"\"\"\n    Visualize the forest as an interactive network graph.\n\n    Creates an HTML visualization showing nodes for each SeriesNode and\n    edges for parent-child relationships.\n\n    Parameters\n    ----------\n    save_path : str | Path\n        Path to save the HTML visualization.\n\n    Returns\n    -------\n    Path\n        Path to the saved HTML visualization\n\n    Raises\n    ------\n    OptionalImportError\n        If pyvis package is not installed\n    \"\"\"\n    return visualize_forest(\n        self.root_nodes, save_path=save_path\n    )  # call external method.\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.Interlacer.visualize_forest(save_path)","title":"<code>save_path</code>","text":""},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.InterlacerQueryError","title":"InterlacerQueryError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for Interlacer query errors.</p>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.MissingDependencyModalityError","title":"MissingDependencyModalityError","text":"<pre><code>MissingDependencyModalityError(\n    missing_dependencies: dict[str, set[str]],\n    query_set: set[str],\n)\n</code></pre> <p>               Bases: <code>imgtools.dicom.interlacer.InterlacerQueryError</code></p> <p>Raised when modalities are missing their required dependencies.</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def __init__(\n    self, missing_dependencies: dict[str, set[str]], query_set: set[str]\n) -&gt; None:\n    self.missing_dependencies = missing_dependencies\n    self.query_set = query_set\n    message = self._build_error_message()\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/interlacer/#imgtools.dicom.interlacer.UnsupportedModalityError","title":"UnsupportedModalityError","text":"<pre><code>UnsupportedModalityError(\n    query_set: set[str], valid_order: list[str]\n)\n</code></pre> <p>               Bases: <code>imgtools.dicom.interlacer.InterlacerQueryError</code></p> <p>Raised when an unsupported modality is specified in the query.</p> Source code in <code>src/imgtools/dicom/interlacer.py</code> <pre><code>def __init__(self, query_set: set[str], valid_order: list[str]) -&gt; None:\n    self.unsupported_modalities = query_set - set(valid_order)\n    self.valid_order = valid_order\n    msg = (\n        f\"Invalid query: [{', '.join(query_set)}]. \"\n        f\"The provided modalities [{', '.join(self.unsupported_modalities)}] \"\n        f\"are not supported. \"\n        f\"Supported modalities are: {', '.join(valid_order)}\"\n    )\n    super().__init__(msg)\n</code></pre>"},{"location":"reference/dicom/read_tags/","title":"Read tags","text":""},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags","title":"read_tags","text":"<p>DICOM Sorting Utilities.</p> <p>This module provides utilities for truncating UIDs, and reading specific tags from DICOM files.</p> <p>Functions:</p> Name Description <code>read_tags</code> <p>Read specified tags from a DICOM file.</p> <p>Examples:</p> <p>Read tags from a DICOM file:     &gt;&gt;&gt; from pathlib import (     ...     Path,     ... )     &gt;&gt;&gt; tags = [     ...     \"PatientID\",     ...     \"StudyInstanceUID\",     ... ]     &gt;&gt;&gt; read_tags(     ...     Path(\"sample.dcm\"),     ...     tags,     ... )</p>"},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags","title":"read_tags","text":"<pre><code>read_tags(\n    file: pathlib.Path,\n    tags: typing.List[str],\n    truncate: int = 5,\n    default: typing.Optional[str] = \"\",\n    force: bool = False,\n) -&gt; typing.Dict[str, str]\n</code></pre> <p>Read the specified tags from a DICOM file.</p> <p>Reads a set of tags from a DICOM file and applies optional sanitization and truncation for UIDs. Handles cases where specific tags may be missing.</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>Path to the DICOM file.</p> required <code>list of str</code> <p>List of DICOM tags to read.</p> required <code>int</code> <p>Number of characters to keep at the end of UIDs (default is 5). 0 or negative values will keep the entire UID.</p> <code>5</code> <code>str</code> <p>Default value to use for missing tags (default is \"\").</p> <code>''</code> <code>bool</code> <p>If True, force reading the file even if it is not a valid DICOM file (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict of str : str</code> <p>A dictionary mapping tags to their values.</p> Notes <p>For RTSTRUCT files, missing 'InstanceNumber' tags default to '1'.</p> <p>Examples:</p> <p>Read tags from a valid DICOM file with truncation:</p> <pre><code>&gt;&gt;&gt; from pathlib import (\n...     Path,\n... )\n&gt;&gt;&gt; read_tags(\n...     Path(\"sample.dcm\"),\n...     [\n...         \"PatientID\",\n...         \"StudyInstanceUID\",\n...     ],\n... )\n{'PatientID': '12345', 'StudyInstanceUID': '1.2.3.4.5'}\n</code></pre> <p>Read tags without truncating UIDs:</p> <pre><code>&gt;&gt;&gt; read_tags(\n...     Path(\"sample.dcm\"),\n...     [\n...         \"PatientID\",\n...         \"StudyInstanceUID\",\n...     ],\n...     truncate=False,\n... )\n{'PatientID': '12345', 'StudyInstanceUID': '1.2.840.10008.1.2.1'}\n</code></pre> <p>Handle missing tags:</p> <pre><code>&gt;&gt;&gt; read_tags(\n...     Path(\"sample.dcm\"),\n...     [\"NonexistentTag\"],\n... )\n[warn] No value for tag: NonexistentTag in file: sample.dcm\n{'NonexistentTag': 'UNKNOWN'}\n</code></pre> Source code in <code>src/imgtools/dicom/read_tags.py</code> <pre><code>def read_tags(\n    file: Path,\n    tags: List[str],\n    truncate: int = 5,\n    default: Optional[str] = \"\",\n    force: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Read the specified tags from a DICOM file.\n\n    Reads a set of tags from a DICOM file and applies optional sanitization\n    and truncation for UIDs. Handles cases where specific tags may be missing.\n\n    Parameters\n    ----------\n    file : Path\n        Path to the DICOM file.\n    tags : list of str\n        List of DICOM tags to read.\n    truncate : int, optional\n        Number of characters to keep at the end of UIDs (default is 5).\n        0 or negative values will keep the entire UID.\n    default : str, optional\n        Default value to use for missing tags (default is \"\").\n    force : bool, optional\n        If True, force reading the file even if it is not a valid DICOM file\n        (default is False).\n\n    Returns\n    -------\n    dict of str : str\n        A dictionary mapping tags to their values.\n\n    Raises\n    ------\n    TypeError\n        If there is a type error while reading the DICOM file.\n    InvalidDicomError\n        If the file is not a valid DICOM file.\n    ValueError\n        If there is a value error while reading the file.\n\n    Notes\n    -----\n    For RTSTRUCT files, missing 'InstanceNumber' tags default to '1'.\n\n    Examples\n    --------\n    Read tags from a valid DICOM file with truncation:\n    &gt;&gt;&gt; from pathlib import (\n    ...     Path,\n    ... )\n    &gt;&gt;&gt; read_tags(\n    ...     Path(\"sample.dcm\"),\n    ...     [\n    ...         \"PatientID\",\n    ...         \"StudyInstanceUID\",\n    ...     ],\n    ... )\n    {'PatientID': '12345', 'StudyInstanceUID': '1.2.3.4.5'}\n\n    Read tags without truncating UIDs:\n    &gt;&gt;&gt; read_tags(\n    ...     Path(\"sample.dcm\"),\n    ...     [\n    ...         \"PatientID\",\n    ...         \"StudyInstanceUID\",\n    ...     ],\n    ...     truncate=False,\n    ... )\n    {'PatientID': '12345', 'StudyInstanceUID': '1.2.840.10008.1.2.1'}\n\n    Handle missing tags:\n    &gt;&gt;&gt; read_tags(\n    ...     Path(\"sample.dcm\"),\n    ...     [\"NonexistentTag\"],\n    ... )\n    [warn] No value for tag: NonexistentTag in file: sample.dcm\n    {'NonexistentTag': 'UNKNOWN'}\n    \"\"\"\n    assert isinstance(file, Path)\n    assert (\n        isinstance(tags, list)\n        and all(isinstance(tag, str) for tag in tags)\n        and tags is not None\n    )\n\n    dicom = load_dicom(\n        file, force=force, stop_before_pixels=True, specific_tags=tags\n    )\n\n    result = {}\n\n    for tag in tags:\n        value = str(dicom.get(tag, default=default))\n\n        if tag.endswith(\"UID\") and truncate &gt; 0:\n            value = truncate_uid(value, last_digits=truncate)\n\n        result[tag] = value\n    return result\n</code></pre>"},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags(file)","title":"<code>file</code>","text":""},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags(tags)","title":"<code>tags</code>","text":""},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags(truncate)","title":"<code>truncate</code>","text":""},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags(default)","title":"<code>default</code>","text":""},{"location":"reference/dicom/read_tags/#imgtools.dicom.read_tags.read_tags(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom/utils/","title":"Utils","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils","title":"utils","text":"<p>DICOM Utilities.</p> <p>This module provides utilities for: - Searching and validating DICOM files in directories. - Looking up DICOM tags by keywords with optional hexadecimal formatting. - Checking the existence of DICOM tags. - Finding similar DICOM tags.</p> <p>Functions:</p> Name Description <code>lookup_tag</code> <p>Lookup the tag for a given DICOM keyword.</p> <code>similar_tags</code> <p>Find similar DICOM tags for a given keyword.</p> <code>tag_exists</code> <p>Boolean check if a DICOM tag exists for a given keyword.</p>"},{"location":"reference/dicom/utils/#imgtools.dicom.utils.lookup_tag","title":"lookup_tag  <code>cached</code>","text":"<pre><code>lookup_tag(\n    keyword: str, hex_format: bool = False\n) -&gt; typing.Optional[str]\n</code></pre> <p>Lookup the tag for a given DICOM keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to look up.</p> required <code>bool</code> <p>If True, return the tag in hexadecimal format (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str or None</code> <p>The DICOM tag as a string, or None if the keyword is invalid.</p> <p>Examples:</p> <p>Lookup a DICOM tag in decimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\"PatientID\")\n'1048608'\n</code></pre> <p>Lookup a DICOM tag in hexadecimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\n...     \"PatientID\",\n...     hex_format=True,\n... )\n'0x100020'\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef lookup_tag(keyword: str, hex_format: bool = False) -&gt; Optional[str]:\n    \"\"\"\n    Lookup the tag for a given DICOM keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to look up.\n    hex_format : bool, optional\n        If True, return the tag in hexadecimal format (default is False).\n\n    Returns\n    -------\n    str or None\n        The DICOM tag as a string, or None if the keyword is invalid.\n\n    Examples\n    --------\n\n    Lookup a DICOM tag in decimal format:\n\n    &gt;&gt;&gt; lookup_tag(\"PatientID\")\n    '1048608'\n\n    Lookup a DICOM tag in hexadecimal format:\n\n    &gt;&gt;&gt; lookup_tag(\n    ...     \"PatientID\",\n    ...     hex_format=True,\n    ... )\n    '0x100020'\n    \"\"\"\n    if (tag := tag_for_keyword(keyword)) is None:\n        return None\n    return f\"0x{tag:X}\" if hex_format else str(tag)\n</code></pre>"},{"location":"reference/dicom/utils/#imgtools.dicom.utils.lookup_tag(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils.lookup_tag(hex_format)","title":"<code>hex_format</code>","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils.similar_tags","title":"similar_tags  <code>cached</code>","text":"<pre><code>similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; typing.List[str]\n</code></pre> <p>Find similar DICOM tags for a given keyword.</p> <p>Useful for User Interface to suggest similar tags based on a misspelled keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The keyword to search for similar tags.</p> required <code>int</code> <p>Maximum number of similar tags to return (default is 3).</p> <code>3</code> <code>float</code> <p>Minimum similarity ratio (default is 0.6).</p> <code>0.6</code> <p>Returns:</p> Type Description <code>typing.List[str]</code> <p>A list of up to <code>n</code> similar DICOM tags.</p> <p>Examples:</p> <p>Find similar tags for a misspelled keyword:</p> <pre><code>&gt;&gt;&gt; similar_tags(\"PatinetID\")\n['PatientID', 'PatientName', 'PatientBirthDate']\n</code></pre> <p>Adjust the number of results and threshold:</p> <pre><code>&gt;&gt;&gt; similar_tags(\n...     \"PatinetID\",\n...     n=5,\n...     threshold=0.7,\n... )\n['PatientID', 'PatientName']\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; List[str]:\n    \"\"\"Find similar DICOM tags for a given keyword.\n\n    Useful for User Interface to suggest similar tags based on a misspelled keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The keyword to search for similar tags.\n    n : int, optional\n        Maximum number of similar tags to return (default is 3).\n    threshold : float, optional\n        Minimum similarity ratio (default is 0.6).\n\n    Returns\n    -------\n    List[str]\n        A list of up to `n` similar DICOM tags.\n\n    Examples\n    --------\n    Find similar tags for a misspelled keyword:\n\n    &gt;&gt;&gt; similar_tags(\"PatinetID\")\n    ['PatientID', 'PatientName', 'PatientBirthDate']\n\n    Adjust the number of results and threshold:\n\n    &gt;&gt;&gt; similar_tags(\n    ...     \"PatinetID\",\n    ...     n=5,\n    ...     threshold=0.7,\n    ... )\n    ['PatientID', 'PatientName']\n    \"\"\"\n    return difflib.get_close_matches(keyword, ALL_DICOM_TAGS, n, threshold)\n</code></pre>"},{"location":"reference/dicom/utils/#imgtools.dicom.utils.similar_tags(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils.similar_tags(n)","title":"<code>n</code>","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils.similar_tags(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/dicom/utils/#imgtools.dicom.utils.tag_exists","title":"tag_exists  <code>cached</code>","text":"<pre><code>tag_exists(keyword: str) -&gt; bool\n</code></pre> <p>Boolean check if a DICOM tag exists for a given keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tag exists, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tag_exists(\"PatientID\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\nFalse\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef tag_exists(keyword: str) -&gt; bool:\n    \"\"\"Boolean check if a DICOM tag exists for a given keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to check.\n\n    Returns\n    -------\n    bool\n        True if the tag exists, False otherwise.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; tag_exists(\"PatientID\")\n    True\n\n    &gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\n    False\n    \"\"\"\n    return dictionary_has_tag(keyword)\n</code></pre>"},{"location":"reference/dicom/utils/#imgtools.dicom.utils.tag_exists(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom/crawl/crawler/","title":"Crawler","text":""},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler","title":"crawler","text":""},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.CrawlResultsNotAvailableError","title":"CrawlResultsNotAvailableError","text":"<pre><code>CrawlResultsNotAvailableError(method_name: str = '')\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when crawler results are accessed before crawling.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def __init__(self, method_name: str = \"\") -&gt; None:\n    message = \"Crawl results not available. Please run crawl() first.\"\n    if method_name:\n        message = f\"{message} (Called from: {method_name})\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler","title":"Crawler  <code>dataclass</code>","text":"<pre><code>Crawler(\n    dicom_dir: pathlib.Path,\n    output_dir: pathlib.Path | None = None,\n    dataset_name: str | None = None,\n    n_jobs: int = 1,\n    force: bool = False,\n)\n</code></pre> <p>Crawl a DICOM directory and extract metadata.</p> <p>Methods:</p> Name Description <code>crawl</code> <p>Crawl the DICOM directory and extract metadata.</p> <code>get_folder</code> <p>Get the folder for a given series UID.</p> <code>get_modality</code> <p>Get the modality for a given series UID.</p> <code>get_series_info</code> <p>Get the series information for a given series UID.</p>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.crawl_db","title":"crawl_db  <code>property</code>","text":"<pre><code>crawl_db: list[dict[str, str]]\n</code></pre> <p>Return the crawl database.</p>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.crawl_db_raw","title":"crawl_db_raw  <code>property</code>","text":"<pre><code>crawl_db_raw: (\n    imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap\n)\n</code></pre> <p>Return the crawl database raw.</p>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.crawl_results","title":"crawl_results  <code>property</code>","text":"<pre><code>crawl_results: (\n    imgtools.dicom.crawl.parse_dicoms.ParseDicomDirResult\n)\n</code></pre> <p>Get the crawl results, validating they're available first.</p>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.index","title":"index  <code>property</code>","text":"<pre><code>index: pandas.DataFrame\n</code></pre> <p>Return the index of the crawl results.</p>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.crawl","title":"crawl","text":"<pre><code>crawl() -&gt; None\n</code></pre> <p>Crawl the DICOM directory and extract metadata.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def crawl(self) -&gt; None:\n    \"\"\"Crawl the DICOM directory and extract metadata.\"\"\"\n    self.output_dir = (\n        self.output_dir or self.dicom_dir.parent / \".imgtools\"\n    )\n    validate_output_dir(self.output_dir)\n\n    logger.info(\n        \"Starting DICOM crawl.\",\n        dicom_dir=self.dicom_dir,\n        output_dir=self.output_dir,\n        dataset_name=self.dataset_name,\n    )\n\n    with tqdm_logging_redirect():\n        crawldb = parse_dicom_dir(\n            dicom_dir=self.dicom_dir,\n            output_dir=self.output_dir,\n            dataset_name=self.dataset_name,\n            n_jobs=self.n_jobs,\n            force=self.force,\n        )\n    self._crawl_results = crawldb\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.get_folder","title":"get_folder","text":"<pre><code>get_folder(series_uid: str) -&gt; str\n</code></pre> <p>Get the folder for a given series UID.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def get_folder(self, series_uid: str) -&gt; str:\n    \"\"\"Get the folder for a given series UID.\"\"\"\n    if series_uid not in self.crawl_results.crawl_db_raw:\n        msg = f\"Series UID {series_uid} not found in crawl results.\"\n        raise ValueError(msg)\n\n    data = self.crawl_results.crawl_db_raw[series_uid]\n    first_subseries = next(iter(data.values()))\n    return first_subseries[\"folder\"]\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.get_modality","title":"get_modality","text":"<pre><code>get_modality(series_uid: str) -&gt; str\n</code></pre> <p>Get the modality for a given series UID.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def get_modality(self, series_uid: str) -&gt; str:\n    \"\"\"Get the modality for a given series UID.\"\"\"\n    if series_uid not in self.crawl_results.crawl_db_raw:\n        msg = f\"Series UID {series_uid} not found in crawl results.\"\n        raise ValueError(msg)\n\n    data = self.crawl_results.crawl_db_raw[series_uid]\n    first_subseries = next(iter(data.values()))\n    return first_subseries[\"modality\"]\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.Crawler.get_series_info","title":"get_series_info","text":"<pre><code>get_series_info(series_uid: str) -&gt; dict[str, str]\n</code></pre> <p>Get the series information for a given series UID.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def get_series_info(self, series_uid: str) -&gt; dict[str, str]:\n    \"\"\"Get the series information for a given series UID.\"\"\"\n    if series_uid not in self.crawl_results.crawl_db_raw:\n        msg = f\"Series UID {series_uid} not found in crawl results.\"\n        raise ValueError(msg)\n\n    data = self.crawl_results.crawl_db_raw[series_uid]\n    first_subseries = next(iter(data.values()))\n    return first_subseries\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.CrawlerOutputDirError","title":"CrawlerOutputDirError","text":"<pre><code>CrawlerOutputDirError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception for errors related to the output directory.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    super().__init__(f\"Output directory error: {message}\")\n</code></pre>"},{"location":"reference/dicom/crawl/crawler/#imgtools.dicom.crawl.crawler.validate_output_dir","title":"validate_output_dir","text":"<pre><code>validate_output_dir(output_dir: pathlib.Path) -&gt; None\n</code></pre> <p>Validate the output directory.</p> Source code in <code>src/imgtools/dicom/crawl/crawler.py</code> <pre><code>def validate_output_dir(output_dir: Path) -&gt; None:\n    \"\"\"Validate the output directory.\"\"\"\n    output_dir = output_dir.expanduser().resolve()\n    errmsg = \"\"\n\n    if not output_dir.exists():\n        logger.debug(f\"Output path {output_dir} does not exist. Creating it.\")\n        try:\n            output_dir.mkdir(parents=True, exist_ok=True)\n        except OSError as e:\n            errmsg = f\"Failed to create output directory {output_dir}: {e}\"\n    elif output_dir.exists() and not output_dir.is_dir():\n        errmsg = f\"Output path {output_dir} is not a directory.\"\n    elif not os.access(output_dir, os.W_OK):\n        # should only get here if the directory exists, but is not writable\n        errmsg = f\"Output directory {output_dir} is not writable.\"\n\n        # do some more investigation to give user some more information\n        if not os.access(output_dir, os.R_OK):\n            errmsg += \" It is also not readable.\"\n\n        # get the owner and permissions of the directory\n        try:\n            stat_info = output_dir.stat()\n            owner = stat_info.st_uid\n            permissions = oct(stat_info.st_mode)[-3:]\n            errmsg += f\" Owner: {owner}, Permissions: {permissions}\"\n        except OSError as e:\n            errmsg += f\" Failed to retrieve directory stats for more information: {e}\"\n\n    if errmsg:\n        raise CrawlerOutputDirError(errmsg)\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/","title":"Parse dicoms","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms","title":"parse_dicoms","text":"<p>Functions:</p> Name Description <code>parse_dicom_dir</code> <p>Parse all DICOM files in a directory and return the metadata.</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap","title":"SeriesMetaMap  <code>module-attribute</code>","text":"<pre><code>SeriesMetaMap: typing.TypeAlias = dict[\n    imgtools.dicom.crawl.parse_dicoms.SeriesUID,\n    dict[\n        imgtools.dicom.crawl.parse_dicoms.SubSeriesID, dict\n    ],\n]\n</code></pre> <p>Datatype represents: {<code>Series</code>: {<code>SubSeries</code>: <code>dict</code>}}</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.SeriesUID","title":"SeriesUID  <code>module-attribute</code>","text":"<pre><code>SeriesUID: typing.TypeAlias = str\n</code></pre> <p>Represent the <code>SeriesInstanceUID</code> of a DICOM file.</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.SopSeriesMap","title":"SopSeriesMap  <code>module-attribute</code>","text":"<pre><code>SopSeriesMap: typing.TypeAlias = dict[\n    imgtools.dicom.crawl.parse_dicoms.SopUID,\n    imgtools.dicom.crawl.parse_dicoms.SeriesUID,\n]\n</code></pre> <p>Datatype represents: {<code>SOPInstanceUID</code>: <code>SeriesInstanceUID</code>}</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.SopUID","title":"SopUID  <code>module-attribute</code>","text":"<pre><code>SopUID: typing.TypeAlias = str\n</code></pre> <p>Represent the <code>SOPInstanceUID</code> of a DICOM file.</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.SubSeriesID","title":"SubSeriesID  <code>module-attribute</code>","text":"<pre><code>SubSeriesID: typing.TypeAlias = str\n</code></pre> <p>Represent the <code>AcquisitionNumber</code> of a DICOM file.</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.ParseDicomDirResult","title":"ParseDicomDirResult","text":"<p>               Bases: <code>typing.NamedTuple('ParseDicomDirResult', [('crawl_db', list[dict[str, str]]), ('index', pandas.DataFrame), ('crawl_db_raw', imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap), ('crawl_db_path', pathlib.Path), ('index_csv_path', pathlib.Path), ('crawl_cache_path', pathlib.Path), ('sop_map_path', pathlib.Path)])</code></p> <p>A NamedTuple containing the result of parsing a DICOM directory.</p> <p>Attributes:</p> Name Type Description <code>crawl_db</code> <code>list[dict[str, str]]</code> <p>A list of dictionaries containing the simplified crawl database.</p> <code>index</code> <code>pandas.DataFrame</code> <p>A DataFrame from which the relationships between DICOMs in the directory can be constructed.</p> <code>crawl_db_raw</code> <code>imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap</code> <p>A mapping of each series to the corresponding subseries in the directory.</p> <code>crawl_db_path</code> <code>pathlib.Path</code> <p>Path to the simplified crawl database JSON file.</p> <code>index_csv_path</code> <code>pathlib.path</code> <p>Path to the index file.</p> <code>crawl_cache_path</code> <code>pathlib.Path</code> <p>Path to the crawl cache.</p> <code>sop_map_path</code> <code>pathlib.Path</code> <p>Path to the SOP map, which maps SOP UIDs to Series UIDs.</p>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.construct_barebones_dict","title":"construct_barebones_dict","text":"<pre><code>construct_barebones_dict(\n    series_meta_raw: imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap,\n) -&gt; list[dict[str, str]]\n</code></pre> <p>Construct a simplified dictionary from the series metadata.</p> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def construct_barebones_dict(\n    series_meta_raw: SeriesMetaMap,\n) -&gt; list[dict[str, str]]:\n    \"\"\"Construct a simplified dictionary from the series metadata.\"\"\"\n    barebones_dict = []\n    for seriesuid, subsseries_map in series_meta_raw.items():\n        for subseriesid, meta in subsseries_map.items():\n            match meta.get(\"ReferencedSeriesUID\", None):\n                case None | \"\":\n                    ref_series = \"\"\n                    meta[\"ReferencedModality\"] = \"\"\n                case [*multiple_refs]:  # only SR can have multiple references\n                    ref_series = \";\".join(multiple_refs)\n                    ref_modalities = []\n                    for ref in multiple_refs:\n                        if ref in series_meta_raw:\n                            ref_modalities.append(\n                                series2modality(ref, series_meta_raw)\n                            )\n                    meta[\"ReferencedSeriesUID\"] = ref_series\n                    meta[\"ReferencedModality\"] = \";\".join(ref_modalities)\n                case single_ref:\n                    ref_series = single_ref\n                    meta[\"ReferencedModality\"] = series2modality(\n                        ref_series, series_meta_raw\n                    )\n\n            barebones_dict.append(\n                {\n                    \"PatientID\": meta[\"PatientID\"],\n                    \"StudyInstanceUID\": meta[\"StudyInstanceUID\"],\n                    \"SeriesInstanceUID\": seriesuid,\n                    \"SubSeries\": subseriesid or \"1\",\n                    \"Modality\": meta[\"Modality\"],\n                    \"ReferencedModality\": meta.get(\"ReferencedModality\", \"\"),\n                    \"ReferencedSeriesUID\": ref_series,\n                    \"instances\": len(meta.get(\"instances\", [])),\n                    \"folder\": meta[\"folder\"],\n                }\n            )\n\n    return barebones_dict\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.extract_metadata_wrapper","title":"extract_metadata_wrapper","text":"<pre><code>extract_metadata_wrapper(\n    dicom: pathlib.Path,\n) -&gt; dict[str, object | list[object]]\n</code></pre> <p>Wrapper for extract_metadata to avoid lambda in parallel processing.</p> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def extract_metadata_wrapper(\n    dicom: pathlib.Path,\n) -&gt; dict[str, object | list[object]]:\n    \"\"\"Wrapper for extract_metadata to avoid lambda in parallel processing.\"\"\"\n    return extract_metadata(dicom, None, [\"SOPInstanceUID\"])\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_all_dicoms","title":"parse_all_dicoms","text":"<pre><code>parse_all_dicoms(\n    dicom_files: list[pathlib.Path],\n    top: pathlib.Path,\n    n_jobs: int = -1,\n) -&gt; tuple[\n    imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap,\n    imgtools.dicom.crawl.parse_dicoms.SopSeriesMap,\n]\n</code></pre> <p>Parse a list of DICOM files in parallel and return the metadata.</p> <p>Given a list of dicom files, this function will parse the metadata of each file in parallel and return two dictionaries: 1. <code>series_meta_raw</code>: A dictionary mapping <code>SeriesInstanceUID</code> to     1 or more <code>SubSeriesID</code> and the metadata of each SubSeries.     where <code>SubSeriesID</code> is the <code>AcquisitionNumber</code> of the DICOM file. 2. <code>sop_map</code>: A dictionary mapping <code>SOPInstanceUID</code> to <code>SeriesInstanceUID</code>.</p> <pre><code>{\n    &lt;SeriesInstanceUID&gt;: {\n        &lt;SubSeriesID&gt;: {\n            'Modality': &lt;Modality&gt;,\n            ...\n            'folder': &lt;folder&gt;,\n            'instances': {\n                    &lt;SOPInstanceUID&gt;: &lt;filename&gt;,\n                    ...\n                },\n            }\n        }\n}\n\n{\n    &lt;SOPInstanceUID&gt;: &lt;SeriesInstanceUID&gt;,\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>list[str]</code> <p>List of paths to DICOM files to process</p> required <code>pathlib.Path</code> <p>Top directory path (used for relative path calculation)</p> required <code>int</code> <p>Number of parallel jobs to run</p> <code>-1</code> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>@timer(\"Parsing all DICOMs\")\ndef parse_all_dicoms(\n    dicom_files: list[pathlib.Path],\n    top: pathlib.Path,\n    n_jobs: int = -1,\n) -&gt; tuple[SeriesMetaMap, SopSeriesMap]:\n    \"\"\"Parse a list of DICOM files in parallel and return the metadata.\n\n    Given a list of dicom files, this function will parse the metadata of each file\n    in parallel and return two dictionaries:\n    1. `series_meta_raw`: A dictionary mapping `SeriesInstanceUID` to\n        1 or more `SubSeriesID` and the metadata of each SubSeries.\n        where `SubSeriesID` is the `AcquisitionNumber` of the DICOM file.\n    2. `sop_map`: A dictionary mapping `SOPInstanceUID` to `SeriesInstanceUID`.\n\n    ```\n    {\n        &lt;SeriesInstanceUID&gt;: {\n            &lt;SubSeriesID&gt;: {\n                'Modality': &lt;Modality&gt;,\n                ...\n                'folder': &lt;folder&gt;,\n                'instances': {\n                        &lt;SOPInstanceUID&gt;: &lt;filename&gt;,\n                        ...\n                    },\n                }\n            }\n    }\n\n    {\n        &lt;SOPInstanceUID&gt;: &lt;SeriesInstanceUID&gt;,\n    }\n    ```\n    Parameters\n    ----------\n    dicom_files : list[str]\n        List of paths to DICOM files to process\n    top : pathlib.Path\n        Top directory path (used for relative path calculation)\n    n_jobs : int, default=-1\n        Number of parallel jobs to run\n    \"\"\"\n\n    series_meta_raw: SeriesMetaMap = defaultdict(lambda: defaultdict(dict))\n    sop_map: SopSeriesMap = {}\n    description = f\"Parsing {len(dicom_files)} DICOM files\"\n\n    for dcm, result in zip(\n        dicom_files,\n        Parallel(n_jobs=n_jobs, return_as=\"generator\")(\n            delayed(extract_metadata_wrapper)(dicom)\n            for dicom in tqdm(\n                dicom_files,\n                desc=description,\n                mininterval=1,\n                leave=False,\n                colour=\"green\",\n            )\n        ),\n        strict=False,\n    ):\n        series_uid = result[\"SeriesInstanceUID\"]\n        sop_uid = result[\"SOPInstanceUID\"]\n\n        # we cant let the subseries id be None or \"None\"\n        subseries_id = SubSeriesID(result.get(\"AcquisitionNumber\") or \"1\")\n        if subseries_id == \"None\":\n            subseries_id = \"1\"\n\n        series_entry = series_meta_raw[series_uid][subseries_id]\n        filepath: pathlib.Path = pathlib.Path(dcm).relative_to(top.parent)\n\n        # Initialize metadata if not already set\n        if \"instances\" not in series_entry:\n            # Copy only the metadata you want to retain\n            series_entry.update(\n                {\n                    k: v\n                    for k, v in result.items()\n                    if k\n                    not in (\n                        \"SOPInstanceUID\",\n                    )  # exclude instance-specific keys\n                }\n            )\n            series_entry[\"folder\"] = str(filepath.parent.as_posix())\n            series_entry[\"instances\"] = {}\n\n        # Append current instance info\n        series_entry[\"instances\"][sop_uid] = filepath.name  # type: ignore\n\n        # Add the SOP UID to the sop_map dictionary\n        sop_map[sop_uid] = series_uid\n\n    return series_meta_raw, sop_map\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_all_dicoms(dicom_files)","title":"<code>dicom_files</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_all_dicoms(top)","title":"<code>top</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_all_dicoms(n_jobs)","title":"<code>n_jobs</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir","title":"parse_dicom_dir","text":"<pre><code>parse_dicom_dir(\n    dicom_dir: str | pathlib.Path,\n    output_dir: str | pathlib.Path,\n    dataset_name: str | None = None,\n    extension: str = \"dcm\",\n    n_jobs: int = -1,\n    force: bool = True,\n) -&gt; imgtools.dicom.crawl.parse_dicoms.ParseDicomDirResult\n</code></pre> <p>Parse all DICOM files in a directory and return the metadata.</p> <p>This function searches for DICOM files within the specified directory (including subdirectories), extracts metadata from each file, and organizes the metadata into structured dictionaries.  The results, including a simplified crawl database, the raw series metadata, a SOP map, and their corresponding JSON file paths, are returned as a <code>ParseDicomDirResult</code> namedtuple.</p> <p>Parameters:</p> Name Type Description Default <code>str | pathlib.Path</code> <p>The directory to search for DICOM files.</p> required <code>str | pathlib.Path</code> <p>The directory to save crawl outputs. See Notes for details.</p> required <code>str | None</code> <p>The name of the dataset. If None, the name of the top directory will be used. This is used to create a subdirectory in the <code>output_dir</code> to store the crawl database and SOP map JSON files.</p> <code>None</code> <code>str</code> <p>The file extension to look for when searching for DICOM files.</p> <code>\"dcm\"</code> <code>int</code> <p>The number of parallel jobs to use for parsing DICOM files.  If -1, all available cores will be used.</p> <code>-1</code> <code>bool</code> <p>If True, overwrite existing crawl database and SOP map JSON files. If False, load existing files if they exist.</p> <code>True</code> <p>Returns:</p> Type Description <code>imgtools.dicom.crawl.parse_dicoms.ParseDicomDirResult</code> <p>A namedtuple containing the following fields:     - crawl_db_path (pathlib.Path):         Path to the simplified crawl database JSON file.     - crawl_db (list[dict[str, str]]):         A list of dictionaries containing the simplified crawl database.</p> Notes <p>This function will create a directory structure for the crawl database <pre><code>output_dir\n    \u251c\u2500\u2500 &lt;dataset_name&gt;\n    \u2502   \u251c\u2500\u2500 crawl_db.json\n    \u2502   \u251c\u2500\u2500 crawl-cache.json\n    \u2502   \u251c\u2500\u2500 sop_map.json\n    \u2502   \u2514\u2500\u2500 index.csv\n    \u2514\u2500\u2500 ...\n</code></pre> The <code>crawl_db.json</code> file contains the simplified crawl database, while</p> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def parse_dicom_dir(\n    dicom_dir: str | pathlib.Path,\n    output_dir: str | pathlib.Path,\n    dataset_name: str | None = None,\n    extension: str = \"dcm\",\n    n_jobs: int = -1,\n    force: bool = True,\n) -&gt; ParseDicomDirResult:\n    \"\"\"Parse all DICOM files in a directory and return the metadata.\n\n    This function searches for DICOM files within the specified directory\n    (including subdirectories), extracts metadata from each file, and\n    organizes the metadata into structured dictionaries.  The results,\n    including a simplified crawl database, the raw series metadata,\n    a SOP map, and their corresponding JSON file paths, are returned\n    as a `ParseDicomDirResult` namedtuple.\n\n    Parameters\n    ----------\n    dicom_dir : str | pathlib.Path\n        The directory to search for DICOM files.\n    output_dir : str | pathlib.Path\n        The directory to save crawl outputs.\n        See Notes for details.\n    dataset_name : str | None, default=None\n        The name of the dataset. If None, the name of the top directory\n        will be used. This is used to create a subdirectory in the\n        `output_dir` to store the crawl database and SOP map JSON files.\n    extension : str, default=\"dcm\"\n        The file extension to look for when searching for DICOM files.\n    n_jobs : int, default=-1\n        The number of parallel jobs to use for parsing DICOM files.  If -1,\n        all available cores will be used.\n    force : bool, default=True\n        If True, overwrite existing crawl database and SOP map JSON files.\n        If False, load existing files if they exist.\n\n    Returns\n    -------\n    ParseDicomDirResult\n        A namedtuple containing the following fields:\n            - crawl_db_path (pathlib.Path):\n                Path to the simplified crawl database JSON file.\n            - crawl_db (list[dict[str, str]]):\n                A list of dictionaries containing the simplified crawl database.\n\n    Notes\n    -----\n    This function will create a directory structure for the crawl database\n    ```\n    output_dir\n        \u251c\u2500\u2500 &lt;dataset_name&gt;\n        \u2502   \u251c\u2500\u2500 crawl_db.json\n        \u2502   \u251c\u2500\u2500 crawl-cache.json\n        \u2502   \u251c\u2500\u2500 sop_map.json\n        \u2502   \u2514\u2500\u2500 index.csv\n        \u2514\u2500\u2500 ...\n    ```\n    The `crawl_db.json` file contains the simplified crawl database, while\n\n    \"\"\"\n\n    # resolve the search directory and determine the dataset name\n    search_directory = pathlib.Path(dicom_dir).resolve().absolute()\n    ds_name = dataset_name or search_directory.name\n\n    # ensure the output directory is a pathlib.Path object\n    output_dir = pathlib.Path(output_dir)\n\n    pathlib.Path(output_dir / ds_name).mkdir(\n        parents=True, exist_ok=True\n    )  # create the output directory if it doesn't exist\n\n    # determine the output directory paths\n    crawl_db_path: pathlib.Path = output_dir / ds_name / \"crawl_db.json\"\n    crawl_cache: pathlib.Path = output_dir / ds_name / \"crawl-cache.json\"\n    sop_map_json: pathlib.Path = output_dir / ds_name / \"sop_map.json\"\n    index_csv: pathlib.Path = output_dir / ds_name / \"index.csv\"\n\n    if (crawl_cache.exists() and sop_map_json.exists()) and not force:\n        logger.info(f\"{crawl_cache} exists and {force=}. Loading from file.\")\n        with crawl_cache.open(\"r\") as f:\n            series_meta_raw = json.load(f)\n        logger.info(f\"{sop_map_json} exists and {force=}. Loading from file.\")\n        with sop_map_json.open(\"r\") as f:\n            sop_map = json.load(f)\n    else:\n        dicom_files = find_dicoms(search_directory, extension=extension)\n        if not dicom_files:\n            msg = f\"No DICOM files found in {search_directory} with extension {extension}\"\n            raise FileNotFoundError(msg)\n\n        logger.info(f\"Found {len(dicom_files)} DICOM files in {dicom_dir}\")\n\n        series_meta_raw, sop_map = parse_all_dicoms(\n            dicom_files, search_directory, n_jobs=n_jobs\n        )\n\n        with crawl_cache.open(\"w\") as f:\n            json.dump(series_meta_raw, f, indent=4)\n        logger.debug(\"Saved cache.\", crawl_cache=crawl_cache)\n        with sop_map_json.open(\"w\") as f:\n            json.dump(sop_map, f, indent=4)\n        logger.debug(\"Saved SOP map.\", sop_map_json=sop_map_json)\n\n    # we have to resolve the reference series mapping\n    # for the `FrameOfReferenceUID` to `SeriesInstanceUID` mapping\n    # this is a bit tricky, because the `FrameOfReferenceUID` will\n    # be the same for many series\n    with timed_context(\"Mapping FrameOfReferenceUID to SeriesInstanceUID\"):\n        frame_mapping = defaultdict(set)\n        for series_uid, subsseries_map in dpath_search(\n            series_meta_raw,\n            \"*/**/FrameOfReferenceUID\",\n        ).items():\n            for meta in subsseries_map.values():\n                if frame := meta.get(\"FrameOfReferenceUID\"):\n                    frame_mapping[frame].add(series_uid)\n\n    # we have to extract all the nested metadata dictionaries\n    # within each subseries\n    # to resolve the reference series mapping\n    _meta_gen = (\n        meta\n        for seriesuid in series_meta_raw\n        for meta in series_meta_raw[seriesuid].values()\n    )\n\n    # Using the function with the metadata generator\n    for meta in tqdm(\n        _meta_gen,\n        desc=\"Solving Reference Series Mapping\",\n        leave=False,\n    ):\n        resolve_reference_series(meta, sop_map, series_meta_raw, frame_mapping)\n\n    # add \"ReferencedModality\" to the metadata\n    # and extract the relevant fields for barebones_dict\n    slim_db = construct_barebones_dict(series_meta_raw)\n\n    # drop duplicate entries with different subseries.\n    slim_db = remove_duplicate_entries(slim_db)\n\n    # convert slimb_db to a pandas dataframe\n    index_df = pd.DataFrame.from_records(slim_db)\n\n    index_df.to_csv(index_csv, index=False)\n    logger.debug(\"Saved index CSV.\", index_csv=index_csv)\n\n    # save the crawl_db\n    with crawl_db_path.open(\"w\") as f:\n        json.dump(series_meta_raw, f, indent=4)\n    logger.debug(\"Saved crawl_db.\", crawl_db_path=crawl_db_path)\n\n    return ParseDicomDirResult(\n        crawl_db=slim_db,\n        index=index_df,\n        crawl_db_raw=series_meta_raw,\n        crawl_db_path=crawl_db_path,\n        index_csv_path=index_csv,\n        crawl_cache_path=crawl_cache,\n        sop_map_path=sop_map_json,\n    )\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(dicom_dir)","title":"<code>dicom_dir</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(output_dir)","title":"<code>output_dir</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(dataset_name)","title":"<code>dataset_name</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(extension)","title":"<code>extension</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(n_jobs)","title":"<code>n_jobs</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.parse_dicom_dir(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.remove_duplicate_entries","title":"remove_duplicate_entries","text":"<pre><code>remove_duplicate_entries(\n    slim_db: list[dict[str, str]],\n    ignore_keys: typing.Optional[list[str]] = None,\n) -&gt; list[dict[str, str]]\n</code></pre> <p>Removes duplicate entries from a barebones dict, ignoring the keys in <code>ignore_keys</code>.</p> <p>Returns:</p> Name Type Description <code>    list[dict[str, str]]</code> <ul> <li>the barebones dict with duplicates removed.</li> </ul> <code>Note</code> <code>I tried to make this as efficient as possible, sorry if it slows down everything.</code> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def remove_duplicate_entries(\n    slim_db: list[dict[str, str]], ignore_keys: Optional[list[str]] = None\n) -&gt; list[dict[str, str]]:\n    \"\"\"Removes duplicate entries from a barebones dict, ignoring the keys in `ignore_keys`.\n    Parameters\n    ----------\n\n        slim_db : list[dict[str, str]]\n            - The barebones dict to operate on. MUST contain the following keys:\n                    \"PatientID\"\n                    \"StudyInstanceUID\"\n                    \"SeriesInstanceUID\"\n                    \"SubSeries\"\n                    \"Modality\"\n                    \"ReferencedModality\"\n                    \"ReferencedSeriesUID\"\n                    \"instances\"\n                    \"folder\"\n        ignore_keys: Optional[list[str]], default = None\n            - The list of keys to ignore when searching for duplicates.\n            - If a row is exactly the same as another row, except for one of the keys listed in `ignore_keys`\n              the row will still be considered a duplicate.\n\n\n    Returns\n    -------\n\n        list[dict[str, str]]\n            - the barebones dict with duplicates removed.\n\n    Note: I tried to make this as efficient as possible, sorry if it slows down everything.\n    \"\"\"\n\n    hash_values = set()\n    # I find it more intuitive to pass in a list, rather than a set, but since sets are faster for our usecase I cast ignore_keys to a set.\n    if ignore_keys is None:\n        ignore_set = set([\"SubSeries\"])\n    else:\n        ignore_set = set(ignore_keys)\n    output = []\n    for record in slim_db:\n        hash_key = [record[key] for key in record if key not in ignore_set]\n        if (*hash_key,) not in hash_values:\n            output.append(record)\n            hash_values.add((*hash_key,))\n\n    return output\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.resolve_reference_series","title":"resolve_reference_series","text":"<pre><code>resolve_reference_series(\n    meta: dict,\n    sop_map: imgtools.dicom.crawl.parse_dicoms.SopSeriesMap,\n    series_meta_raw: imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap,\n    frame_mapping: dict[str, set[str]],\n) -&gt; None\n</code></pre> <p>Process reference mapping for a single metadata entry.</p> <p>The goal is to get the <code>ReferencedSeriesUID</code> for as many series as possible. Whereas some series directly reference the <code>SeriesInstanceUID</code> of another series, others reference one or more <code>SOPInstanceUID</code> of instances in another series. This method tries to resolve the latter case by mapping the <code>SOPInstanceUID</code> to the <code>SeriesInstanceUID</code> of the series it belongs to.</p> <p>Additionally, the <code>PT</code> modalities might reference a <code>CT</code>.</p> <p>Side effects: - as we iterate over the metadata dictionaries, we add the     <code>ReferencedSeriesUID</code> field to the metadata dictionaries in the crawldb.</p> Notes <p>This mutates the metadata dictionaries in the crawldb in place. i.e the <code>ReferencedSeriesUID</code> field is added to metadata dicts in the crawldb.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>Metadata entry to process</p> required <code>dict</code> <p>Dictionary mapping SOP UIDs to Series UIDs</p> required <code>dict</code> <p>Series metadata dictionary</p> required <code>dict</code> <p>Frame of reference mapping</p> required Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def resolve_reference_series(\n    meta: dict,\n    sop_map: SopSeriesMap,\n    series_meta_raw: SeriesMetaMap,\n    frame_mapping: dict[str, set[str]],\n) -&gt; None:\n    \"\"\"Process reference mapping for a single metadata entry.\n\n    The goal is to get the `ReferencedSeriesUID` for as many series as possible.\n    Whereas some series directly reference the `SeriesInstanceUID` of another series,\n    others reference one or more `SOPInstanceUID` of instances in another series.\n    This method tries to resolve the latter case by mapping the `SOPInstanceUID` to the\n    `SeriesInstanceUID` of the series it belongs to.\n\n    Additionally, the `PT` modalities might reference a `CT`.\n\n    Side effects:\n    - as we iterate over the metadata dictionaries, we add the\n        `ReferencedSeriesUID` field to the metadata dictionaries in the crawldb.\n\n    Notes\n    -----\n    This mutates the metadata dictionaries in the crawldb in place.\n    i.e the `ReferencedSeriesUID` field is added to metadata dicts in the crawldb.\n\n    Parameters\n    ----------\n    meta : dict\n        Metadata entry to process\n    sop_map : dict\n        Dictionary mapping SOP UIDs to Series UIDs\n    series_meta_raw : dict\n        Series metadata dictionary\n    frame_mapping : dict\n        Frame of reference mapping\n    \"\"\"\n\n    if meta.get(\"ReferencedSeriesUID\"):\n        return\n\n    match meta[\"Modality\"]:\n        case \"SEG\" | \"RTSTRUCT\" | \"RTDOSE\" | \"RTPLAN\":\n            if not (sop_refs := meta.get(\"ReferencedSOPUIDs\", [])):\n                return\n            # get the unique SeriesUIDs by looking up the SOP UIDs in the sop_map\n            _all_seg_refs = {\n                seriesuid\n                for ref in sop_refs\n                if (seriesuid := sop_map.get(ref))\n                and seriesuid in series_meta_raw\n            }\n\n            if not _all_seg_refs:\n                # unlikely to happen...\n                warnmsg = (\n                    f\"Referenced SOP UID {sop_refs} not found in series map\"\n                )\n                logger.warning(\n                    warnmsg,\n                    modality=meta[\"Modality\"],\n                    series=meta[\"SeriesInstanceUID\"],\n                )\n                return\n            elif len(_all_seg_refs) &gt; 1:\n                # even more unlikely to happen...\n                warnmsg = (\n                    \"Multiple series referenced\"\n                    f\" ({_all_seg_refs}). Taking the first one.\"\n                )\n                logger.warning(\n                    warnmsg,\n                    modality=meta[\"Modality\"],\n                    series=meta[\"SeriesInstanceUID\"],\n                )\n            ref_series = _all_seg_refs.pop()\n            meta[\"ReferencedSeriesUID\"] = ref_series\n        case \"PT\":\n            if (\n                not (ref_frame := meta.get(\"FrameOfReferenceUID\"))\n                or not isinstance(ref_frame, str)\n                or ref_frame not in frame_mapping\n            ) or not (ref_series_set := frame_mapping.get(ref_frame)):\n                return\n            for series_uid in ref_series_set:\n                if series_uid in series_meta_raw and (\n                    series2modality(series_uid, series_meta_raw) == \"CT\"\n                ):\n                    meta[\"ReferencedSeriesUID\"] = series_uid\n                    break\n    return\n</code></pre>"},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.resolve_reference_series(meta)","title":"<code>meta</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.resolve_reference_series(sop_map)","title":"<code>sop_map</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.resolve_reference_series(series_meta_raw)","title":"<code>series_meta_raw</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.resolve_reference_series(frame_mapping)","title":"<code>frame_mapping</code>","text":""},{"location":"reference/dicom/crawl/parse_dicoms/#imgtools.dicom.crawl.parse_dicoms.series2modality","title":"series2modality","text":"<pre><code>series2modality(\n    seriesuid: imgtools.dicom.crawl.parse_dicoms.SeriesUID,\n    series_meta_raw: imgtools.dicom.crawl.parse_dicoms.SeriesMetaMap,\n) -&gt; str\n</code></pre> <p>Get the modality of a series.</p> Source code in <code>src/imgtools/dicom/crawl/parse_dicoms.py</code> <pre><code>def series2modality(\n    seriesuid: SeriesUID, series_meta_raw: SeriesMetaMap\n) -&gt; str:\n    \"\"\"Get the modality of a series.\"\"\"\n    return list(series_meta_raw[seriesuid].values())[0].get(\"Modality\", \"\")\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/","title":"Extractor base","text":""},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base","title":"extractor_base","text":"<p>Base class for extracting modality-specific DICOM metadata.</p> <p>This module defines an extensible interface for building metadata extractors that handle both simple DICOM tags and complex computed metadata fields, such as references embedded in nested sequences.</p>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ComputedField","title":"ComputedField  <code>module-attribute</code>","text":"<pre><code>ComputedField = typing.Callable[\n    [pydicom.Dataset],\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedValue,\n]\n</code></pre> <p>Function that extracts a value from a DICOM dataset</p>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ComputedValue","title":"ComputedValue  <code>module-attribute</code>","text":"<pre><code>ComputedValue = object | list[object]\n</code></pre> <p>Single value or list of values extracted from a DICOM dataset.</p>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields","title":"ExtractedFields  <code>module-attribute</code>","text":"<pre><code>ExtractedFields = dict[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedValue,\n]\n</code></pre> <p>Collection of computed values keyed by field name</p>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor","title":"ModalityMetadataExtractor","text":"<p>               Bases: <code>abc.ABC</code></p> <p>Abstract base class for modality-specific DICOM metadata extractors.</p> <p>This class supports both standard DICOM tag retrieval and more complex field computations based on the full DICOM dataset. Subclasses must specify the modality they support and define additional metadata fields either as DICOM tag names or as custom computation functions.</p> <p>Attributes:</p> Name Type Description <code>base_tags</code> <code>typing.ClassVar[set[str]]</code> <p>Standard DICOM tags to always extract, regardless of modality.</p> <code>modality_tags</code> <code>typing.ClassVar[set[str]]</code> <p>Tags specific to the subclass's modality. These are merged with <code>base_tags</code> to form the list of tags to retrieve directly from the dataset.</p> <code>computed_fields</code> <code>typing.ClassVar[collections.abc.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]]</code> <p>A mapping of metadata field names to callables that compute values from the loaded <code>pydicom.Dataset</code>.</p> <p>Methods:</p> Name Description <code>metadata_keys</code> <p>Returns a predictable, sorted list of all metadata field names produced by this extractor.</p> <code>extract</code> <p>Extracts metadata tags and computed fields from a DICOM dataset. Returns a dictionary mapping metadata field names to values.</p> Notes <p>Subclasses MUST implement the following abstract methods/properties: - <code>modality() -&gt; str</code>: A class method that returns the DICOM modality string handled   (e.g., \"CT\", \"MR\", \"RTDOSE\"). - <code>modality_tags -&gt; set[str]</code>: A class property that defines the set of DICOM attribute   names (tags) specific to the modality. - <code>computed_fields -&gt; Mapping[str, Callable[[pydicom.Dataset], ComputedValue]]</code>: A class property   that defines a mapping of metadata field names to callables which compute their values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class CTExtractor(ModalityMetadataExtractor):\n&gt;&gt;&gt;     @classmethod\n&gt;&gt;&gt;     def modality(cls) -&gt; str:\n&gt;&gt;&gt;         return \"CT\"\n&gt;&gt;&gt;     @classproperty\n&gt;&gt;&gt;     def modality_tags(cls) -&gt; set[str]:\n&gt;&gt;&gt;         return {\"KVP\", \"ReconstructionAlgorithm\"}\n&gt;&gt;&gt;     @classproperty\n&gt;&gt;&gt;     def computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n&gt;&gt;&gt;         return {\n&gt;&gt;&gt;             \"CustomValue\": lambda ds: str(float(ds.SliceThickness) * 2),\n&gt;&gt;&gt;             \"DoublePatientAge\": lambda ds: str(ds.PatientAge * 2)\n&gt;&gt;&gt;         }\n</code></pre> <pre><code>&gt;&gt;&gt; # Using the extractor\n&gt;&gt;&gt; metadata = CTExtractor.extract(\"file.dcm\")\n&gt;&gt;&gt; # Returns: {'PatientID': '123', 'KVP': '120', 'CustomValue': '5.0', ...}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.computed_fields","title":"computed_fields  <code>abstractmethod</code>","text":"<pre><code>computed_fields() -&gt; collections.abc.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, typing.Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classproperty\n@abstractmethod\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:  # noqa: N805\n    \"\"\"\n    A mapping of metadata field names to callables that compute their values.\n\n    The callable should accept a pydicom Dataset and return a value.\n\n    Returns\n    -------\n    dict[str, Callable[[pydicom.Dataset], ComputedValue]]\n        Mapping of field names to computation functions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.modality","title":"modality  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef modality(cls) -&gt; str:\n    \"\"\"\n    The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").\n\n    Returns\n    -------\n    str\n        Modality name.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor.modality_tags","title":"modality_tags  <code>abstractmethod</code>","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classproperty\n@abstractmethod\ndef modality_tags(cls) -&gt; set[str]:  # noqa: N805\n    \"\"\"\n    A set of DICOM tags specific to the modality handled by this extractor.\n\n    Returns\n    -------\n    set[str]\n        Set of DICOM tag names.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractor_base/#imgtools.dicom.dicom_metadata.extractor_base.classproperty","title":"classproperty","text":"<p>               Bases: <code>property</code></p> <p>A decorator that behaves like @property, but on the class rather than instance.</p> <p>Useful for exposing computed class-level constants or derived metadata.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/","title":"Extractors","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors","title":"extractors","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor","title":"CTMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Metadata extractor for CT modality DICOM datasets.</p> <p>This subclass defines modality-specific tags and computed fields relevant to CT (Computed Tomography) imaging. It extends the base metadata extractor with CT-specific acquisition and reconstruction parameters.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>CT-specific computed fields.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Mapping of field names to functions that compute values from DICOM datasets.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    CT-specific computed fields.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Mapping of field names to functions that compute values from DICOM datasets.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"CT\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.CTMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> <code>CT-specific DICOM tag names.</code> <p>Returns:</p> Type Description <code>set[str]</code> <p>CT acquisition and reconstruction-related DICOM tags.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    \"\"\"\n    CT-specific DICOM tag names.\n\n    Returns\n    -------\n    set[str]\n        CT acquisition and reconstruction-related DICOM tags.\n    \"\"\"\n    return {\n        # Contrast &amp; Enhancement\n        \"ContrastFlowDuration\",\n        \"ContrastFlowRate\",\n        \"ContrastBolusAgent\",\n        \"ContrastBolusVolume\",\n        \"ContrastBolusStartTime\",\n        \"ContrastBolusStopTime\",\n        \"ContrastBolusIngredient\",\n        \"ContrastBolusIngredientConcentration\",\n        # X-ray Exposure &amp; Dose\n        \"KVP\",\n        \"XRayTubeCurrent\",\n        \"ExposureTime\",\n        \"Exposure\",\n        \"ExposureModulationType\",\n        \"CTDIvol\",\n        # Image Reconstruction &amp; Processing\n        \"ReconstructionAlgorithm\",\n        \"ReconstructionDiameter\",\n        \"ReconstructionMethod\",\n        \"ReconstructionTargetCenterPatient\",\n        \"ReconstructionFieldOfView\",\n        \"ConvolutionKernel\",\n        # Scan &amp; Acquisition Parameters\n        \"SpiralPitchFactor\",\n        \"SingleCollimationWidth\",\n        \"TotalCollimationWidth\",\n        \"TableSpeed\",\n        \"TableMotion\",\n        \"GantryDetectorTilt\",\n        \"DetectorType\",\n        \"DetectorConfiguration\",\n        \"DataCollectionCenterPatient\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor","title":"FallbackMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Generic fallback extractor for unsupported or uncommon DICOM modalities.</p> <p>This extractor uses only the base tags defined in the superclass and defines no modality-specific tags or computed fields. It allows graceful handling of modalities not yet explicitly supported.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>Returns an empty mapping since no computed fields are defined.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Empty mapping.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    Returns an empty mapping since no computed fields are defined.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Empty mapping.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"UNKNOWN\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.FallbackMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> <code>Returns an empty set since no modality-specific tags are defined.</code> <p>Returns:</p> Type Description <code>set[str]</code> <p>Empty set.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    \"\"\"\n    Returns an empty set since no modality-specific tags are defined.\n\n    Returns\n    -------\n    set[str]\n        Empty set.\n    \"\"\"\n    return set()\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor","title":"MRMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>MR-specific computed fields.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Mapping of field names to functions that compute values from DICOM datasets.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    MR-specific computed fields.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Mapping of field names to functions that compute values from DICOM datasets.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"MR\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.MRMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        # Magnetic Field &amp; RF Properties\n        \"MagneticFieldStrength\",\n        \"ImagingFrequency\",\n        \"TransmitCoilName\",\n        # Sequence &amp; Acquisition Parameters\n        \"SequenceName\",\n        \"ScanningSequence\",\n        \"SequenceVariant\",\n        \"AcquisitionContrast\",\n        \"AcquisitionType\",\n        \"EchoTime\",\n        \"RepetitionTime\",\n        \"InversionTime\",\n        \"EchoTrainLength\",\n        \"NumberOfAverages\",\n        \"FlipAngle\",\n        \"PercentSampling\",\n        \"PercentPhaseFieldOfView\",\n        \"PixelBandwidth\",\n        \"SpacingBetweenSlices\",\n        # Diffusion Imaging\n        \"DiffusionGradientDirectionSequence\",\n        \"DiffusionBMatrixSequence\",\n        # Parallel Imaging &amp; Acceleration\n        \"ParallelAcquisitionTechnique\",\n        \"ParallelReductionFactorInPlane\",\n        \"ParallelReductionFactorOutOfPlane\",\n        # Functional MRI (fMRI)\n        \"NumberOfTemporalPositions\",\n        \"TemporalResolution\",\n        \"FrameReferenceTime\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor","title":"PTMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>PET-specific computed fields.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Mapping of field names to functions that compute values from DICOM datasets.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    PET-specific computed fields.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Mapping of field names to functions that compute values from DICOM datasets.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"PT\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.PTMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        # Radiotracer &amp; Injection Information\n        \"Radiopharmaceutical\",\n        \"RadiopharmaceuticalStartTime\",\n        \"RadionuclideTotalDose\",\n        \"RadionuclideHalfLife\",\n        \"RadionuclidePositronFraction\",\n        \"RadiopharmaceuticalVolume\",\n        \"RadiopharmaceuticalSpecificActivity\",\n        \"RadiopharmaceuticalStartDateTime\",\n        \"RadiopharmaceuticalStopDateTime\",\n        \"RadiopharmaceuticalRoute\",\n        \"RadiopharmaceuticalCodeSequence\",\n        # PET Image Quantification\n        \"DecayCorrection\",\n        \"DecayFactor\",\n        \"AttenuationCorrectionMethod\",\n        \"ScatterCorrectionMethod\",\n        \"DecayCorrected\",\n        \"DeadTimeCorrectionFlag\",\n        \"ReconstructionMethod\",\n        # SUV (Standardized Uptake Value) Calculation\n        \"SUVType\",\n        # Acquisition Timing &amp; Dynamics\n        \"FrameReferenceTime\",\n        \"FrameTime\",\n        \"ActualFrameDuration\",\n        \"AcquisitionStartCondition\",\n        \"AcquisitionTerminationCondition\",\n        \"TimeSliceVector\",\n        # PET Detector &amp; Calibration\n        \"DetectorType\",\n        \"CoincidenceWindowWidth\",\n        \"EnergyWindowLowerLimit\",\n        \"EnergyWindowUpperLimit\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor","title":"RTDOSEMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Metadata extractor for RTDOSE modality DICOM datasets.</p> <p>Extracts direct and computed reference UIDs from dose DICOM files.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    from imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils import (\n        rtdose_reference_uids,\n    )\n\n    def get_sop_uids(ds: Dataset) -&gt; list[str]:\n        ref_pl, ref_struct, ref_series = rtdose_reference_uids(ds)\n        return [ref_struct or ref_pl]\n\n    return {\n        \"ReferencedSeriesUID\": lambda ds: rtdose_reference_uids(ds)[2],\n        \"ReferencedSeriesSOPUIDs\": get_sop_uids,\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"RTDOSE\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTDOSEMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        \"DoseType\",\n        \"DoseUnits\",\n        \"DoseSummationType\",\n        \"DoseGridScaling\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor","title":"RTPLANMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Metadata extractor for RTPLAN modality DICOM datasets.</p> <p>Extracts basic DICOM tags and reference to an RTSTRUCT UID.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    from imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils import (\n        rtplan_reference_uids,\n    )\n\n    return {\"ReferencedSOPUIDs\": lambda ds: [rtplan_reference_uids(ds)]}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"RTPLAN\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTPLANMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        \"SeriesInstanceUID\",\n        \"StudyInstanceUID\",\n        \"RTPlanLabel\",\n        \"RTPlanName\",\n        \"RTPlanDate\",\n        \"RTPlanTime\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor","title":"RTSTRUCTMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Metadata extractor for RTSTRUCT modality DICOM datasets.</p> <p>This class uses computed fields to extract ROI metadata and reference UIDs.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>RTSTRUCT-specific computed fields.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Field names mapped to functions that extract computed values.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    RTSTRUCT-specific computed fields.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Field names mapped to functions that extract computed values.\n    \"\"\"\n\n    from imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils import (\n        extract_roi_names,\n        rtstruct_reference_uids,\n    )\n\n    return {\n        \"ReferencedSeriesUID\": lambda ds: rtstruct_reference_uids(ds)[0],\n        \"ReferencedSOPUIDs\": lambda ds: rtstruct_reference_uids(ds)[1],\n        \"ROINames\": extract_roi_names,\n        \"NumROIs\": lambda ds: len(extract_roi_names(ds)),\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"RTSTRUCT\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.RTSTRUCTMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> <code>RTSTRUCT-specific direct tags (generally minimal).</code> <p>Returns:</p> Type Description <code>set[str]</code> <p>A set of directly accessible RTSTRUCT tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    \"\"\"\n    RTSTRUCT-specific direct tags (generally minimal).\n\n    Returns\n    -------\n    set[str]\n        A set of directly accessible RTSTRUCT tag names.\n    \"\"\"\n    return {\n        \"StructureSetLabel\",\n        \"StructureSetName\",\n        \"StructureSetDate\",\n        \"StructureSetTime\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor","title":"SEGMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> See Also <p>https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.8.20.2.html</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> <code>SEG-specific computed fields.</code> <code>Each computed field is a function that takes a pydicom.Dataset as input</code> <code>and returns a computed value. The functions are defined to extract</code> <code>relevant information from the DICOM dataset.</code> <p>Returns:</p> Type Description <code>typing.Mapping[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedField]</code> <p>Mapping of field names to functions that compute values from DICOM datasets.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    \"\"\"\n    SEG-specific computed fields.\n\n    Each computed field is a function that takes a pydicom.Dataset as input\n    and returns a computed value. The functions are defined to extract\n    relevant information from the DICOM dataset.\n\n    Returns\n    -------\n    Mapping[str, ComputedField]\n        Mapping of field names to functions that compute values from DICOM datasets.\n    \"\"\"\n    from imgtools.dicom.dicom_metadata.modality_utils.seg_utils import (\n        get_seg_direction,\n        get_seg_spacing,\n        seg_reference_uids,\n    )\n\n    def get_seg_ref_series(seg: Dataset) -&gt; str:\n        \"\"\"Get the reference series UID for the segmentation.\"\"\"\n        return seg_reference_uids(seg)[0]\n\n    def get_seg_ref_sop_uids(seg: Dataset) -&gt; list[str]:\n        \"\"\"Get the reference SOP instance UIDs for the segmentation.\"\"\"\n        return seg_reference_uids(seg)[1]\n\n    def get_seg_segmentlabels(seg: Dataset) -&gt; list[str]:\n        \"\"\"Get the segment labels from the segmentation.\"\"\"\n        return [\n            desc.get(\"SegmentLabel\", \"\")\n            for desc in seg.get(\"SegmentSequence\", [])\n        ]\n\n    def get_seg_descriptions(seg: Dataset) -&gt; list[str]:\n        \"\"\"Get the segment descriptions from the segmentation.\"\"\"\n        return [\n            desc.get(\"SegmentDescription\", \"\")\n            for desc in seg.get(\"SegmentSequence\", [])\n        ]\n\n    return {\n        # prefix with \"Seg\" to avoid collision sitk computed attrbutes\n        \"SegSpacing\": lambda ds: get_seg_spacing(ds) or \"\",\n        \"SegDirection\": lambda ds: get_seg_direction(ds) or \"\",\n        \"ROINames\": get_seg_segmentlabels,\n        \"ROIDescriptions\": get_seg_descriptions,\n        \"ReferencedSeriesUID\": get_seg_ref_series,\n        \"ReferencedSOPUIDs\": get_seg_ref_sop_uids,\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"SEG\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SEGMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        # DICOM-SEG tags\n        # BINARY, FRACTIONAL, or LABELMAP\n        # https://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.8.20.2.3.html\n        \"SegmentationType\",\n        \"SegmentationFractionalType\",\n        \"MaximumFractionalValue\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor","title":"SRMetadataExtractor","text":"<p>               Bases: <code>imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor</code></p> <p>Metadata extractor for SR (Structured Report) modality DICOM datasets.</p> <p>Extracts referenced SeriesInstanceUIDs and SOPInstanceUIDs from structured reports.</p> <p>Methods:</p> Name Description <code>computed_fields</code> <p>A mapping of metadata field names to callables that compute their values.</p> <code>extract</code> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <code>metadata_keys</code> <p>Return a predictable, sorted list of metadata field names.</p> <code>modality</code> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <code>modality_tags</code> <p>A set of DICOM tags specific to the modality handled by this extractor.</p>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.computed_fields","title":"computed_fields","text":"<pre><code>computed_fields() -&gt; typing.Mapping[\n    str,\n    imgtools.dicom.dicom_metadata.extractor_base.ComputedField,\n]\n</code></pre> <p>A mapping of metadata field names to callables that compute their values.</p> <p>The callable should accept a pydicom Dataset and return a value.</p> <p>Returns:</p> Type Description <code>dict[str, Callable[[pydicom.Dataset], imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]]</code> <p>Mapping of field names to computation functions.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef computed_fields(cls) -&gt; Mapping[str, ComputedField]:\n    from imgtools.dicom.dicom_metadata.modality_utils.sr_utils import (\n        sr_reference_uids,\n    )\n\n    def get_series_uids(ds: Dataset) -&gt; list[str]:\n        series, _ = sr_reference_uids(ds)\n        return list(series)\n\n    def get_sop_uids(ds: Dataset) -&gt; list[str]:\n        _, sops = sr_reference_uids(ds)\n        return list(sops)\n\n    return {\n        \"ReferencedSeriesUID\": get_series_uids,\n        \"ReferencedSOPUIDs\": get_sop_uids,\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.extract","title":"extract  <code>classmethod</code>","text":"<pre><code>extract(\n    dicom: imgtools.dicom.DicomInput,\n    extra_tags: list[str] | None = None,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.extractor_base.ExtractedFields\n)\n</code></pre> <p>Extract metadata tags and computed fields from a DICOM dataset.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.DicomInput</code> <p>A path, byte stream, or pydicom FileDataset.</p> required <code>list[str] | None</code> <p>Additional DICOM tags to extract, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, imgtools.dicom.dicom_metadata.extractor_base.ComputedValue]</code> <p>A dictionary mapping metadata field names to values. Values may be strings, numbers, dictionaries, or lists of these types. Missing tags or errors during computation will result in an empty string.</p> Notes <p>Be aware that using extra_tags may lead to unexpected results if the extra tags are not compatible with the modality or if they are not present in the DICOM file. The extractor will not validate the extra tags against the modality, so it's the user's responsibility to ensure that the extra tags are relevant and valid for the given DICOM file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef extract(\n    cls, dicom: DicomInput, extra_tags: list[str] | None = None\n) -&gt; ExtractedFields:\n    \"\"\"\n    Extract metadata tags and computed fields from a DICOM dataset.\n\n    Parameters\n    ----------\n    dicom : DicomInput\n        A path, byte stream, or pydicom FileDataset.\n    extra_tags : list[str] | None, optional\n        Additional DICOM tags to extract, by default None\n\n    Returns\n    -------\n    dict[str, ComputedValue]\n        A dictionary mapping metadata field names to values.\n        Values may be strings, numbers, dictionaries, or lists of these types.\n        Missing tags or errors during computation will result in an empty string.\n\n    Notes\n    -----\n    Be aware that using extra_tags may lead to unexpected results if the\n    extra tags are not compatible with the modality or if they are not\n    present in the DICOM file. The extractor will not validate the extra tags\n    against the modality, so it's the user's responsibility to ensure that\n    the extra tags are relevant and valid for the given DICOM file.\n    \"\"\"\n    ds = load_dicom(dicom)\n    output: ExtractedFields = {}\n\n    # Extract base and modality-specific tags\n    tags_to_extract = cls.base_tags.union(cls.modality_tags)\n    if extra_tags:\n        tags_to_extract = tags_to_extract.union(extra_tags)\n\n    for tag in tags_to_extract:\n        output[tag] = str(ds.get(tag, \"\"))\n\n    # Compute advanced fields\n    for key, fn in cls.computed_fields.items():\n        try:\n            # Store computed value directly without conversion to string\n            output[key] = fn(ds)\n        except Exception as e:\n            warnmsg = (\n                f\"Failed to compute field '{key}' for modality '{cls.modality()}'. \"\n                \"This may be due to missing or malformed data in the DICOM file.\"\n            )\n            warnmsg += f\" Error: {e}\"\n            logger.warning(warnmsg, file=str(dicom))\n            output[key] = \"\"\n\n    # sort all keys\n    return {k: output[k] for k in sorted(output.keys())}\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.extract(dicom)","title":"<code>dicom</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.extract(extra_tags)","title":"<code>extra_tags</code>","text":""},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.metadata_keys","title":"metadata_keys  <code>classmethod</code>","text":"<pre><code>metadata_keys() -&gt; list[str]\n</code></pre> <p>Return a predictable, sorted list of metadata field names.</p> <p>This includes both direct DICOM tag names and any computed metadata keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>All metadata keys produced by this extractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractor_base.py</code> <pre><code>@classmethod\ndef metadata_keys(cls) -&gt; list[str]:\n    \"\"\"\n    Return a predictable, sorted list of metadata field names.\n\n    This includes both direct DICOM tag names and any computed metadata keys.\n\n    Returns\n    -------\n    list[str]\n        All metadata keys produced by this extractor.\n    \"\"\"\n    # if no modality_tags or computed_fields are defined, return base_tags\n    if not cls.modality_tags and not cls.computed_fields:\n        return sorted(cls.base_tags)\n\n    all_tags = cls.base_tags.union(cls.modality_tags)\n    all_keys = all_tags.union(cls.computed_fields.keys())\n    return sorted(all_keys)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.modality","title":"modality  <code>classmethod</code>","text":"<pre><code>modality() -&gt; str\n</code></pre> <p>The DICOM modality handled by this extractor (e.g., \"CT\", \"MR\").</p> <p>Returns:</p> Type Description <code>str</code> <p>Modality name.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classmethod\ndef modality(cls) -&gt; str:\n    return \"SR\"\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/extractors/#imgtools.dicom.dicom_metadata.extractors.SRMetadataExtractor.modality_tags","title":"modality_tags","text":"<pre><code>modality_tags() -&gt; set[str]\n</code></pre> <p>A set of DICOM tags specific to the modality handled by this extractor.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of DICOM tag names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/extractors.py</code> <pre><code>@classproperty\ndef modality_tags(cls) -&gt; set[str]:\n    return {\n        \"SeriesInstanceUID\",\n        \"StudyInstanceUID\",\n        \"Modality\",\n        \"Manufacturer\",\n        \"ContentDate\",\n        \"ContentTime\",\n        \"SeriesDescription\",\n    }\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/registry/","title":"Registry","text":""},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry","title":"registry","text":"<p>Functions:</p> Name Description <code>get_extractor</code> <p>Retrieve a registered extractor for the given modality.</p> <code>register_extractor</code> <p>Register a modality extractor class in the global registry.</p> <code>supported_modalities</code> <p>List all registered modalities.</p>"},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.ExistingExtractorError","title":"ExistingExtractorError","text":"<pre><code>ExistingExtractorError(\n    modality: str,\n    existing_extractor: typing.Type[\n        imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor\n    ],\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when trying to register an extractor for an already registered modality.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/registry.py</code> <pre><code>def __init__(\n    self,\n    modality: str,\n    existing_extractor: Type[ModalityMetadataExtractor],\n) -&gt; None:\n    self.modality = modality\n    self.existing_extractor = existing_extractor\n    super().__init__(\n        f\"Modality '{modality}' already registered by {existing_extractor.__name__}\"\n    )\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.get_extractor","title":"get_extractor","text":"<pre><code>get_extractor(\n    modality: str,\n) -&gt; typing.Type[\n    imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor\n]\n</code></pre> <p>Retrieve a registered extractor for the given modality.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM modality string (e.g., \"CT\", \"MR\").</p> required <p>Returns:</p> Type Description <code>typing.Type[imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor]</code> <p>The corresponding registered extractor class. If no extractor is registered for the modality, returns a FallbackMetadataExtractor.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/registry.py</code> <pre><code>def get_extractor(modality: str) -&gt; Type[ModalityMetadataExtractor]:\n    \"\"\"\n    Retrieve a registered extractor for the given modality.\n\n    Parameters\n    ----------\n    modality : str\n        The DICOM modality string (e.g., \"CT\", \"MR\").\n\n    Returns\n    -------\n    Type[ModalityMetadataExtractor]\n        The corresponding registered extractor class.\n        If no extractor is registered for the modality, returns a FallbackMetadataExtractor.\n    \"\"\"\n    x = _EXTRACTOR_REGISTRY.get(modality.upper(), None)\n    if not x:\n        from imgtools.dicom.dicom_metadata.extractors import (\n            FallbackMetadataExtractor,\n        )\n\n        x = FallbackMetadataExtractor\n    return x\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.get_extractor(modality)","title":"<code>modality</code>","text":""},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.register_extractor","title":"register_extractor","text":"<pre><code>register_extractor(\n    cls: typing.Type[\n        imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor\n    ],\n) -&gt; typing.Type[\n    imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor\n]\n</code></pre> <p>Register a modality extractor class in the global registry.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Type[imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor]</code> <p>The subclass to register.</p> required <p>Returns:</p> Type Description <code>typing.Type[imgtools.dicom.dicom_metadata.extractor_base.ModalityMetadataExtractor]</code> <p>The class itself (unchanged), for use as a decorator.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/registry.py</code> <pre><code>def register_extractor(\n    cls: Type[ModalityMetadataExtractor],\n) -&gt; Type[ModalityMetadataExtractor]:\n    \"\"\"\n    Register a modality extractor class in the global registry.\n\n    Parameters\n    ----------\n    cls : Type[ModalityMetadataExtractor]\n        The subclass to register.\n\n    Returns\n    -------\n    Type[ModalityMetadataExtractor]\n        The class itself (unchanged), for use as a decorator.\n    \"\"\"\n    modality = cls.modality().upper()\n    if modality in _EXTRACTOR_REGISTRY:\n        raise ExistingExtractorError(modality, _EXTRACTOR_REGISTRY[modality])\n    _EXTRACTOR_REGISTRY[modality] = cls\n    return cls\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.register_extractor(cls)","title":"<code>cls</code>","text":""},{"location":"reference/dicom/dicom_metadata/registry/#imgtools.dicom.dicom_metadata.registry.supported_modalities","title":"supported_modalities","text":"<pre><code>supported_modalities() -&gt; list[str]\n</code></pre> <p>List all registered modalities.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of supported modality names.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/registry.py</code> <pre><code>def supported_modalities() -&gt; list[str]:\n    \"\"\"\n    List all registered modalities.\n\n    Returns\n    -------\n    list[str]\n        Sorted list of supported modality names.\n    \"\"\"\n    return sorted(_EXTRACTOR_REGISTRY.keys())\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/","title":"Rtdose utils","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils","title":"rtdose_utils","text":"<p>Functions:</p> Name Description <code>rtdose_reference_uids</code> <p>Extracts referenced SOPInstanceUIDs from an RTDOSE file.</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefPlanSOP","title":"RTDOSERefPlanSOP","text":"<p>               Bases: <code>str</code></p> <p>Represents a SOPInstanceUID to a RTPLAN file in a RTDOSE file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefSeries","title":"RTDOSERefSeries","text":"<p>               Bases: <code>str</code></p> <p>Sometimes... they reference the SERIESUID as well (Head-Neck-Pet-CT...) but they also call it the SOPInstanceUID for some reason lol</p> <p><code>dose.ReferencedImageSequence[0].ReferencedSOPInstanceUID</code></p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefStructSOP","title":"RTDOSERefStructSOP","text":"<p>               Bases: <code>str</code></p> <p>Represents a SOPInstanceUID to a RTSTRUCT file in a RTDOSE file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.rtdose_reference_uids","title":"rtdose_reference_uids","text":"<pre><code>rtdose_reference_uids(\n    rtdose: pydicom.dataset.Dataset,\n) -&gt; tuple[\n    imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefPlanSOP,\n    imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefStructSOP,\n    imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefSeries,\n]\n</code></pre> <p>Extracts referenced SOPInstanceUIDs from an RTDOSE file.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>DICOM RTDOSE dataset as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>tuple[imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefPlanSOP, imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefStructSOP, imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.RTDOSERefSeries]</code> <p>A tuple containing: - plan_uid: RTDOSERefPlanSOP - Referenced RTPLAN SOPInstanceUID (empty string if not found) - struct_uid: RTDOSERefStructSOP - Referenced RTSTRUCT SOPInstanceUID (empty string if not found) - series_uid: RTDOSERefSeries - Referenced Series SOPInstanceUID (empty string if not found)</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/rtdose_utils.py</code> <pre><code>def rtdose_reference_uids(\n    rtdose: Dataset,\n) -&gt; tuple[RTDOSERefPlanSOP, RTDOSERefStructSOP, RTDOSERefSeries]:\n    \"\"\"Extracts referenced SOPInstanceUIDs from an RTDOSE file.\n\n    Parameters\n    ----------\n    rtdose : Dataset\n        DICOM RTDOSE dataset as a pydicom Dataset.\n\n    Returns\n    -------\n    tuple[RTDOSERefPlanSOP, RTDOSERefStructSOP, RTDOSERefSeries]\n        A tuple containing:\n        - plan_uid: RTDOSERefPlanSOP - Referenced RTPLAN SOPInstanceUID (empty string if not found)\n        - struct_uid: RTDOSERefStructSOP - Referenced RTSTRUCT SOPInstanceUID (empty string if not found)\n        - series_uid: RTDOSERefSeries - Referenced Series SOPInstanceUID (empty string if not found)\n    \"\"\"\n\n    # Extract plan UID\n    plan_uid = RTDOSERefPlanSOP(\"\")\n    if (\n        \"ReferencedRTPlanSequence\" in rtdose\n        and rtdose.ReferencedRTPlanSequence\n    ):\n        plan_uid = RTDOSERefPlanSOP(rtdose.ReferencedRTPlanSequence[0].ReferencedSOPInstanceUID)  # fmt: skip\n\n    # Extract structure set UID\n    struct_uid = RTDOSERefStructSOP(\"\")\n    if (\"ReferencedStructureSetSequence\" in rtdose and rtdose.ReferencedStructureSetSequence):  # fmt: skip\n        struct_uid = RTDOSERefStructSOP(rtdose.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID)  # fmt: skip\n\n    # Extract series UID\n    series_uid = RTDOSERefSeries(\"\")\n    if \"ReferencedImageSequence\" in rtdose and rtdose.ReferencedImageSequence:\n        series_uid = RTDOSERefSeries(rtdose.ReferencedImageSequence[0].ReferencedSOPInstanceUID)  # fmt: skip\n\n    return plan_uid, struct_uid, series_uid\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtdose_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtdose_utils.rtdose_reference_uids(rtdose)","title":"<code>rtdose</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtplan_utils/","title":"Rtplan utils","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtplan_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils","title":"rtplan_utils","text":"<p>Functions:</p> Name Description <code>rtplan_reference_uids</code> <p>Get the ReferencedSOPInstanceUIDs from an RTPLAN file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtplan_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils.RTPLANRefStructSOP","title":"RTPLANRefStructSOP","text":"<p>               Bases: <code>str</code></p> <p>Represents a <code>SOPInstanceUID</code> pointing to a <code>RTSTRUCT</code> referenced by a <code>RTPLAN</code> file</p> <p>extracted via <code>ReferencedStructureSetSequence.ReferencedSOPInstanceUID</code></p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtplan_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils.rtplan_reference_uids","title":"rtplan_reference_uids","text":"<pre><code>rtplan_reference_uids(\n    rtplan: pydicom.dataset.Dataset,\n) -&gt; (\n    imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils.RTPLANRefStructSOP\n)\n</code></pre> <p>Get the ReferencedSOPInstanceUIDs from an RTPLAN file</p> <p>We assume RTPLAN only references a <code>RTSTRUCT</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>The RTPLAN file to extract the reference UIDs from Must be a <code>pydicom.Dataset</code> object</p> required Example <p>match rtplan_reference_uids(rtplan): ...     case RTPLANRefStructSOP(uid): ...         print(f\"SOPInstanceUID: {uid=}\")</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/rtplan_utils.py</code> <pre><code>def rtplan_reference_uids(\n    rtplan: Dataset,\n) -&gt; RTPLANRefStructSOP:\n    \"\"\"Get the ReferencedSOPInstanceUIDs from an RTPLAN file\n\n    We assume RTPLAN only references a `RTSTRUCT` file.\n\n    Parameters\n    ----------\n    rtplan : Dataset\n        The RTPLAN file to extract the reference UIDs from\n        Must be a `pydicom.Dataset` object\n\n    Example\n    -------\n    &gt;&gt;&gt; match rtplan_reference_uids(rtplan):\n    ...     case RTPLANRefStructSOP(uid):\n    ...         print(f\"SOPInstanceUID: {uid=}\")\n    \"\"\"\n    if \"ReferencedStructureSetSequence\" in rtplan:\n        refs = [\n            RTPLANRefStructSOP(seq.ReferencedSOPInstanceUID)\n            for seq in rtplan.ReferencedStructureSetSequence\n        ]\n        if len(refs) &gt; 1:\n            warnmsg = (\n                f\"Found {len(refs)} RTSTRUCT references in {rtplan=}. \"\n                \"Only the first one will be used.\"\n            )\n            logger.warning(warnmsg)\n        return refs[0]\n\n    return RTPLANRefStructSOP(\"\")\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtplan_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtplan_utils.rtplan_reference_uids(rtplan)","title":"<code>rtplan</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/","title":"Rtstruct utils","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils","title":"rtstruct_utils","text":"<p>Functions:</p> Name Description <code>extract_roi_meta</code> <p>Extract ROI metadata from an RTSTRUCT DICOM file.</p> <code>extract_roi_names</code> <p>Extract a list of ROI names from an RTSTRUCT DICOM file.</p> <code>rtstruct_reference_uids</code> <p>Retrieve the referenced SeriesInstanceUID and SOP UIDs from an RTSTRUCT.</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSOP","title":"RTSTRUCTRefSOP","text":"<p>               Bases: <code>list[str]</code></p> <p>A list subclass representing the SOPInstanceUIDs referenced by an RTSTRUCT file.</p> <p>Contains the UIDs of individual DICOM instances that the structure set references.</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSeries","title":"RTSTRUCTRefSeries","text":"<p>               Bases: <code>str</code></p> <p>A string subclass representing the SeriesInstanceUID referenced by an RTSTRUCT file.</p> <p>Used for type annotations and to distinguish this particular reference type.</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.extract_roi_meta","title":"extract_roi_meta","text":"<pre><code>extract_roi_meta(\n    rtstruct: pydicom.dataset.Dataset,\n) -&gt; list[dict[str, str]]\n</code></pre> <p>Extract ROI metadata from an RTSTRUCT DICOM file.</p> <p>Iterate over the <code>StructureSetROISequence</code> in the RTSTRUCT file and extract:     - \"ROINumber\": Unique identifier for the ROI.     - \"ROIName\": Name of the ROI.     - \"ROIGenerationAlgorithm\": Algorithm used to generate the ROI.</p> <p>Parameters:</p> Name Type Description Default <code>`pydicom.dataset.Dataset`</code> required <p>Returns:</p> Type Description <code>list of dict[str, str]</code> <p>A list of dictionaries, each containing metadata for an ROI.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/rtstruct_utils.py</code> <pre><code>def extract_roi_meta(rtstruct: Dataset) -&gt; list[dict[str, str]]:\n    \"\"\"Extract ROI metadata from an RTSTRUCT DICOM file.\n\n    Iterate over the `StructureSetROISequence` in the RTSTRUCT file and extract:\n        - \"ROINumber\": Unique identifier for the ROI.\n        - \"ROIName\": Name of the ROI.\n        - \"ROIGenerationAlgorithm\": Algorithm used to generate the ROI.\n\n    Parameters\n    ----------\n    rtstruct : `pydicom.dataset.Dataset`\n\n    Returns\n    -------\n    list of dict[str, str]\n        A list of dictionaries, each containing metadata for an ROI.\n\n    Raises\n    ------\n    RTSTRUCTAttributeError\n        If the RTSTRUCT file does not contain the required `StructureSetROISequence`.\n    \"\"\"\n\n    try:\n        roi_sequence = rtstruct.StructureSetROISequence\n    except AttributeError as e:\n        errmsg = \"Failed to extract ROISequence from the RTSTRUCT file.\"\n        raise RTSTRUCTAttributeError(errmsg) from e\n    roi_metas = []\n    for roi in roi_sequence:\n        roi_meta = {}\n        roi_meta[\"ROINumber\"] = getattr(roi, \"ROINumber\", \"\")\n        roi_meta[\"ROIName\"] = getattr(roi, \"ROIName\", \"\")\n        roi_meta[\"ROIGenerationAlgorithm\"] = getattr(\n            roi, \"ROIGenerationAlgorithm\", \"\"\n        )\n        roi_metas.append(roi_meta)\n    return roi_metas\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.extract_roi_meta(rtstruct)","title":"<code>rtstruct</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.extract_roi_names","title":"extract_roi_names","text":"<pre><code>extract_roi_names(\n    rtstruct: pydicom.dataset.Dataset,\n) -&gt; list[str]\n</code></pre> <p>Extract a list of ROI names from an RTSTRUCT DICOM file.</p> <p>Parameters:</p> Name Type Description Default <code>`pydicom.dataset.Dataset`</code> required <p>Returns:</p> Type Description <code>list of str</code> <p>A list of ROI names extracted from the RTSTRUCT file.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/rtstruct_utils.py</code> <pre><code>def extract_roi_names(rtstruct: Dataset) -&gt; list[str]:\n    \"\"\"Extract a list of ROI names from an RTSTRUCT DICOM file.\n\n    Parameters\n    ----------\n    rtstruct : `pydicom.dataset.Dataset`\n\n    Returns\n    -------\n    list of str\n        A list of ROI names extracted from the RTSTRUCT file.\n\n    Raises\n    ------\n    RTSTRUCTAttributeError\n        If the RTSTRUCT file does not contain the required `StructureSetROISequence`.\n    \"\"\"\n\n    roi_metas = extract_roi_meta(rtstruct)\n    roi_names = [roi_meta[\"ROIName\"] for roi_meta in roi_metas]\n    return roi_names\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.extract_roi_names(rtstruct)","title":"<code>rtstruct</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.rtstruct_reference_uids","title":"rtstruct_reference_uids","text":"<pre><code>rtstruct_reference_uids(\n    rtstruct: pydicom.dataset.Dataset,\n) -&gt; tuple[\n    imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSeries,\n    imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSOP,\n]\n</code></pre> <p>Retrieve the referenced SeriesInstanceUID and SOP UIDs from an RTSTRUCT.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>DICOM RTSTRUCT dataset as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>tuple[imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSeries, imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.RTSTRUCTRefSOP]</code> <ul> <li>Referenced SeriesInstanceUID as RTSTRUCTRefSeries (empty string if unavailable)</li> <li>Referenced SOPInstanceUIDs as RTSTRUCTRefSOP (empty list if unavailable)</li> </ul> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/rtstruct_utils.py</code> <pre><code>def rtstruct_reference_uids(\n    rtstruct: Dataset,\n) -&gt; tuple[RTSTRUCTRefSeries, RTSTRUCTRefSOP]:\n    \"\"\"Retrieve the referenced SeriesInstanceUID and SOP UIDs from an RTSTRUCT.\n\n    Parameters\n    ----------\n    rtstruct : Dataset\n        DICOM RTSTRUCT dataset as a pydicom Dataset.\n\n    Returns\n    -------\n    tuple[RTSTRUCTRefSeries, RTSTRUCTRefSOP]\n        - Referenced SeriesInstanceUID as RTSTRUCTRefSeries (empty string if unavailable)\n        - Referenced SOPInstanceUIDs as RTSTRUCTRefSOP (empty list if unavailable)\n    \"\"\"\n    import contextlib\n\n    series_uid = \"\"\n    sop_uids: list[str] = []\n\n    with contextlib.suppress(AttributeError, IndexError):\n        # Direct access attempt - if any part fails, we'll catch the exception\n        ref_sequence = rtstruct.ReferencedFrameOfReferenceSequence[0]\n        rt_ref_series = ref_sequence.RTReferencedStudySequence[\n            0\n        ].RTReferencedSeriesSequence[0]\n\n        # Extract series UID if available\n        series_uid = rt_ref_series.get(\"SeriesInstanceUID\", \"\")\n\n        cis = rt_ref_series.ContourImageSequence[0]\n        sop_uids.extend(\n            [\n                ci.ReferencedSOPInstanceUID\n                for ci in cis\n                if \"ReferencedSOPInstanceUID\" in ci\n            ]\n        )\n\n    return RTSTRUCTRefSeries(series_uid), RTSTRUCTRefSOP(sop_uids)\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/rtstruct_utils/#imgtools.dicom.dicom_metadata.modality_utils.rtstruct_utils.rtstruct_reference_uids(rtstruct)","title":"<code>rtstruct</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/","title":"Seg utils","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils","title":"seg_utils","text":"<p>Functions:</p> Name Description <code>seg_reference_uids</code> <p>Get the ReferencedSeriesInstanceUID or ReferencedSOPInstanceUIDs from a SEG file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSOPs","title":"SEGRefSOPs","text":"<p>               Bases: <code>list[str]</code></p> <p>A list representing all the ReferencedSOPInstanceUIDs for a SEG file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSeries","title":"SEGRefSeries","text":"<p>               Bases: <code>str</code></p> <p>A single string to store the ReferencedSeriesInstanceUID for a SEG file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.get_seg_direction","title":"get_seg_direction","text":"<pre><code>get_seg_direction(\n    seg: pydicom.dataset.Dataset,\n) -&gt; list[float] | None\n</code></pre> <p>Get the direction cosines (orientation) from a SEG file.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>Input DICOM segmentation object as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>list[float] | None</code> <p>A list of six floats representing the direction cosines if available, or None if the orientation information is not found.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/seg_utils.py</code> <pre><code>def get_seg_direction(seg: Dataset) -&gt; list[float] | None:\n    \"\"\"\n    Get the direction cosines (orientation) from a SEG file.\n\n    Parameters\n    ----------\n    seg : Dataset\n        Input DICOM segmentation object as a pydicom Dataset.\n\n    Returns\n    -------\n    list[float] | None\n        A list of six floats representing the direction cosines if available,\n        or None if the orientation information is not found.\n    \"\"\"\n    if not (\n        (sharedseq := seg.SharedFunctionalGroupsSequence)\n        and (pos := sharedseq[0].PlaneOrientationSequence)\n        and (direction := pos[0].get(\"ImageOrientationPatient\"))\n    ):\n        return None\n    return [float(v) for v in direction]\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.get_seg_direction(seg)","title":"<code>seg</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.get_seg_spacing","title":"get_seg_spacing","text":"<pre><code>get_seg_spacing(\n    seg: pydicom.dataset.Dataset,\n) -&gt; list[float] | None\n</code></pre> <p>Get the pixel spacing and slice spacing or thickness from a SEG file.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>Input DICOM segmentation object as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>list[float] | None</code> <p>A list of three floats representing [x_spacing, y_spacing, z_spacing] if available, or None if the spacing information is not found.</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/seg_utils.py</code> <pre><code>def get_seg_spacing(seg: Dataset) -&gt; list[float] | None:\n    \"\"\"\n    Get the pixel spacing and slice spacing or thickness from a SEG file.\n\n    Parameters\n    ----------\n    seg : Dataset\n        Input DICOM segmentation object as a pydicom Dataset.\n\n    Returns\n    -------\n    list[float] | None\n        A list of three floats representing [x_spacing, y_spacing, z_spacing] if available,\n        or None if the spacing information is not found.\n    \"\"\"\n    if not (\n        (sharedseq := seg.SharedFunctionalGroupsSequence)\n        and (pms := sharedseq[0].PixelMeasuresSequence)\n        and (pixelspacing := pms[0].PixelSpacing)\n        and (\n            spacing := pms[0].get(\"SpacingBetweenSlices\")\n            or pms[0].get(\"SliceThickness\")\n        )\n    ):\n        return None\n\n    return [\n        float(pixelspacing[0]),\n        float(pixelspacing[1]),\n        float(spacing),\n    ]\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.get_seg_spacing(seg)","title":"<code>seg</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.seg_reference_uids","title":"seg_reference_uids","text":"<pre><code>seg_reference_uids(\n    seg: pydicom.dataset.Dataset,\n) -&gt; tuple[\n    imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSeries,\n    imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSOPs,\n]\n</code></pre> <p>Get the ReferencedSeriesInstanceUID or ReferencedSOPInstanceUIDs from a SEG file</p> <p>Modern Segmentation objects have a <code>ReferencedSeriesSequence</code> attribute which contains the <code>SeriesInstanceUID</code> of the referenced series and a <code>ReferencedInstanceSequence</code> attribute which contains the SOPInstanceUIDs of the referenced instances.</p> <p>Older Segmentation objects have a <code>SourceImageSequence</code> attribute which only contains the <code>SOPInstanceUIDs</code> of the referenced instances.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>Input DICOM segmentation object as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>tuple[imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSeries, imgtools.dicom.dicom_metadata.modality_utils.seg_utils.SEGRefSOPs]</code> <p>Always returns a tuple containing: - ReferencedSeriesInstanceUID (empty string if not available) - ReferencedSOPInstanceUIDs (empty list if not available)</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/seg_utils.py</code> <pre><code>def seg_reference_uids(\n    seg: Dataset,\n) -&gt; tuple[SEGRefSeries, SEGRefSOPs]:\n    \"\"\"Get the ReferencedSeriesInstanceUID or ReferencedSOPInstanceUIDs from a SEG file\n\n    Modern Segmentation objects have a `ReferencedSeriesSequence` attribute\n    which contains the `SeriesInstanceUID` of the referenced series and\n    a `ReferencedInstanceSequence` attribute which contains the SOPInstanceUIDs\n    of the referenced instances.\n\n    Older Segmentation objects have a `SourceImageSequence` attribute which\n    only contains the `SOPInstanceUIDs` of the referenced instances.\n\n    Parameters\n    ----------\n    seg : Dataset\n        Input DICOM segmentation object as a pydicom Dataset.\n\n    Returns\n    -------\n    tuple[SEGRefSeries, SEGRefSOPs]\n        Always returns a tuple containing:\n        - ReferencedSeriesInstanceUID (empty string if not available)\n        - ReferencedSOPInstanceUIDs (empty list if not available)\n    \"\"\"\n\n    assert seg.Modality == \"SEG\", (\n        \"Input DICOM file is not a Segmentation object\"\n    )\n\n    if \"ReferencedSeriesSequence\" in seg:\n        ref_series: list[SEGRefSeries] = [\n            SEGRefSeries(ref_series.SeriesInstanceUID)\n            for ref_series in seg.ReferencedSeriesSequence\n        ]\n        ref_sop = SEGRefSOPs(\n            [\n                seq.ReferencedSOPInstanceUID\n                for seq in seg.ReferencedSeriesSequence[\n                    0\n                ].ReferencedInstanceSequence\n            ]\n        )\n        # Check if there is only one ReferencedSeriesInstanceUID\n        if len(ref_series) == 1:\n            return ref_series[0], ref_sop\n        else:\n            errmsg = (\n                \"Multiple ReferencedSeriesInstanceUIDs found in ReferencedSeriesSequence\"\n                \" This is unexpected and may cause issues. \"\n                f\"Found {len(ref_series)} ReferencedSeriesInstanceUIDs.\"\n            )\n            raise ValueError(errmsg)\n    elif \"SourceImageSequence\" in seg:\n        ref_sop_list = [\n            seq.ReferencedSOPInstanceUID for seq in seg.SourceImageSequence\n        ]\n\n        return SEGRefSeries(\"\"), SEGRefSOPs(ref_sop_list)\n\n    # Return empty values if no reference information is found\n    return SEGRefSeries(\"\"), SEGRefSOPs([])\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/seg_utils/#imgtools.dicom.dicom_metadata.modality_utils.seg_utils.seg_reference_uids(seg)","title":"<code>seg</code>","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/","title":"Sr utils","text":""},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/#imgtools.dicom.dicom_metadata.modality_utils.sr_utils","title":"sr_utils","text":"<p>Functions:</p> Name Description <code>sr_reference_uids</code> <p>Get the <code>ReferencedSeriesInstanceUID</code>s from an SR file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/#imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSOPs","title":"SR_RefSOPs","text":"<p>               Bases: <code>list</code></p> <p>Represents a list of unique <code>SOPInstanceUID</code>s referenced in by a <code>SR</code> file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/#imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSeries","title":"SR_RefSeries","text":"<p>               Bases: <code>list</code></p> <p>Represents a list of <code>SeriesInstanceUID</code>s referenced by a <code>SR</code> file</p>"},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/#imgtools.dicom.dicom_metadata.modality_utils.sr_utils.sr_reference_uids","title":"sr_reference_uids","text":"<pre><code>sr_reference_uids(\n    sr: pydicom.dataset.Dataset,\n) -&gt; tuple[\n    imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSeries,\n    imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSOPs,\n]\n</code></pre> <p>Get the <code>ReferencedSeriesInstanceUID</code>s from an SR file</p> <p>SR Dicom files can reference multiple SeriesInstanceUIDs and many SOPInstanceUIDs</p> <p>Since we might need to match on SOP Instance UIDs if the reference is a MR, we also get the SOP Instance UIDs This function extracts all unique references from the CurrentRequestedProcedureEvidenceSequence.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.Dataset</code> <p>DICOM Structured Report dataset as a pydicom Dataset.</p> required <p>Returns:</p> Type Description <code>tuple[imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSeries, imgtools.dicom.dicom_metadata.modality_utils.sr_utils.SR_RefSOPs]</code> <p>A tuple containing: - series_uids: SR_RefSeries - List of unique referenced SeriesInstanceUIDs (empty list if none) - sop_uids: SR_RefSOPs - List of unique referenced SOPInstanceUIDs (empty list if none)</p> Source code in <code>src/imgtools/dicom/dicom_metadata/modality_utils/sr_utils.py</code> <pre><code>def sr_reference_uids(\n    sr: Dataset,\n) -&gt; tuple[SR_RefSeries, SR_RefSOPs]:\n    \"\"\"Get the `ReferencedSeriesInstanceUID`s from an SR file\n\n    SR Dicom files can reference multiple SeriesInstanceUIDs and many SOPInstanceUIDs\n\n    Since we might need to match on SOP Instance UIDs if the reference is\n    a MR, we also get the SOP Instance UIDs\n    This function extracts all unique references from the\n    CurrentRequestedProcedureEvidenceSequence.\n\n    Parameters\n    ----------\n    sr : Dataset\n        DICOM Structured Report dataset as a pydicom Dataset.\n\n    Returns\n    -------\n    tuple[SR_RefSeries, SR_RefSOPs]\n        A tuple containing:\n        - series_uids: SR_RefSeries - List of unique referenced SeriesInstanceUIDs (empty list if none)\n        - sop_uids: SR_RefSOPs - List of unique referenced SOPInstanceUIDs (empty list if none)\n    \"\"\"\n\n    series_uids = set()\n    sop_uids = set()\n    if \"CurrentRequestedProcedureEvidenceSequence\" not in sr:\n        return SR_RefSeries([]), SR_RefSOPs([])\n\n    for evidence_seq in sr.CurrentRequestedProcedureEvidenceSequence:\n        if \"ReferencedSeriesSequence\" not in evidence_seq:\n            continue\n\n        for series_seq in evidence_seq.ReferencedSeriesSequence:\n            series_uids.add(series_seq.SeriesInstanceUID)\n\n            if \"ReferencedSOPSequence\" not in series_seq:\n                continue\n            for ref_seq in series_seq.ReferencedSOPSequence:\n                sop_uids.add(ref_seq.ReferencedSOPInstanceUID)\n\n    series_list = SR_RefSeries(series_uids)\n    sop_list = SR_RefSOPs(sop_uids)\n\n    return series_list, sop_list\n</code></pre>"},{"location":"reference/dicom/dicom_metadata/modality_utils/sr_utils/#imgtools.dicom.dicom_metadata.modality_utils.sr_utils.sr_reference_uids(sr)","title":"<code>sr</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/","title":"Dicomsorter","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter","title":"dicomsorter","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter","title":"DICOMSorter","text":"<pre><code>DICOMSorter(\n    source_directory: pathlib.Path,\n    target_pattern: str,\n    pattern_parser: typing.Pattern = imgtools.dicom.sort.dicomsorter.DEFAULT_PATTERN_PARSER,\n)\n</code></pre> <p>               Bases: <code>imgtools.dicom.sort.SorterBase</code></p> <p>A specialized implementation of the <code>SorterBase</code> for sorting DICOM files by metadata.</p> <p>This class resolves paths for DICOM files based on specified target patterns, using metadata extracted from the files. The filename of each source file is preserved during this process.</p> <p>Attributes:</p> Name Type Description <code>source_directory</code> <code>pathlib.Path</code> <p>The directory containing the files to be sorted.</p> <code>logger</code> <code>Logger</code> <p>The instance logger bound with the source directory context.</p> <code>dicom_files</code> <code>list of Path</code> <p>The list of DICOM files found in the <code>source_directory</code>.</p> <code>format</code> <code>str</code> <p>The parsed format string with placeholders for DICOM tags.</p> <code>keys</code> <code>typing.Set[str]</code> <p>DICOM tags extracted from the target pattern.</p> <code>invalid_keys</code> <code>typing.Set[str]</code> <p>DICOM tags from the pattern that are invalid.</p> <code>force_dcmread</code> <code>bool</code> <p>If True, force the use of <code>pydicom.dcmread</code> for reading DICOM files.</p> <p>Methods:</p> Name Description <code>execute</code> <p>Execute the file action on DICOM files.</p> <code>print_tree</code> <p>Display the pattern structure as a tree visualization.</p> <code>validate_keys</code> <p>Validate extracted keys. Subclasses should implement this method</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def __init__(\n    self,\n    source_directory: Path,\n    target_pattern: str,\n    pattern_parser: Pattern = DEFAULT_PATTERN_PARSER,\n) -&gt; None:\n    super().__init__(\n        source_directory=source_directory,\n        target_pattern=target_pattern,\n        pattern_parser=pattern_parser,\n    )\n    self.logger.debug(\n        \"All DICOM Keys are Valid in target pattern\", keys=self.keys\n    )\n</code></pre>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.format","title":"format  <code>property</code>","text":"<pre><code>format: str\n</code></pre> <p>Get the formatted pattern string.</p>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.invalid_keys","title":"invalid_keys  <code>property</code>","text":"<pre><code>invalid_keys: typing.Set[str]\n</code></pre> <p>Get the set of invalid keys.</p> <p>Essentially, this will check <code>pydicom.dictionary_has_tag</code> for each key in the pattern and return the set of keys that are invalid.</p> <p>Returns:</p> Type Description <code>typing.Set[str]</code> <p>The set of invalid keys.</p>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: typing.Set[str]\n</code></pre> <p>Get the set of keys extracted from the pattern.</p>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.pattern_preview","title":"pattern_preview  <code>property</code>","text":"<pre><code>pattern_preview: str\n</code></pre> <p>Returns a human readable preview of the pattern.</p> <p>Useful for visualizing the pattern structure and can be highlighted using Rich Console.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; target_pattern = \"%key1/%key2/%key3\"\n&gt;&gt;&gt; pattern_preview = \"{key1}/{key2}/{key3}\"\n</code></pre>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute","title":"execute","text":"<pre><code>execute(\n    action: (\n        imgtools.dicom.sort.FileAction | str\n    ) = imgtools.dicom.sort.FileAction.MOVE,\n    overwrite: bool = False,\n    dry_run: bool = False,\n    num_workers: int = 1,\n    truncate_uids: int = 5,\n) -&gt; None\n</code></pre> <p>Execute the file action on DICOM files.</p> <p>Users are encouraged to use FileAction.HARDLINK for efficient storage and performance for large dataset, as well as protection against lost data.</p> <p>Using hard links can save disk space and improve performance by creating multiple directory entries (links) for a single file instead of duplicating the file content. This is particularly useful when working with large datasets, such as DICOM files, where storage efficiency is crucial.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.sort.FileAction</code> <p>The action to apply to the DICOM files (e.g., move, copy).</p> <code>FileAction.MOVE</code> <code>bool</code> <p>If True, overwrite existing files at the destination.</p> <code>False</code> <code>bool</code> <p>If True, perform a dry run without making any changes.</p> <code>False</code> <code>int</code> <p>The number of worker threads to use for processing files.</p> <code>1</code> <code>int</code> <p>The number of characters to truncate from the UID.</p> <code>5</code> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def execute(\n    self,\n    action: FileAction | str = FileAction.MOVE,\n    overwrite: bool = False,\n    dry_run: bool = False,\n    num_workers: int = 1,\n    truncate_uids: int = 5,\n) -&gt; None:\n    \"\"\"Execute the file action on DICOM files.\n\n    Users are encouraged to use FileAction.HARDLINK for\n    efficient storage and performance for large dataset, as well as\n    protection against lost data.\n\n    Using hard links can save disk space and improve performance by\n    creating multiple directory entries (links) for a single file\n    instead of duplicating the file content. This is particularly\n    useful when working with large datasets, such as DICOM files,\n    where storage efficiency is crucial.\n\n    Parameters\n    ----------\n    action : FileAction, default: FileAction.MOVE\n        The action to apply to the DICOM files (e.g., move, copy).\n    overwrite : bool, default: False\n        If True, overwrite existing files at the destination.\n    dry_run : bool, default: False\n        If True, perform a dry run without making any changes.\n    num_workers : int, default: 1\n        The number of worker threads to use for processing files.\n    truncate_uids : int, default: 5\n        The number of characters to truncate from the UID.\n\n    Raises\n    ------\n    ValueError\n            If the provided action is not a valid FileAction.\n    \"\"\"\n    if not isinstance(action, FileAction):\n        action = FileAction.validate(action)\n\n    self.logger.debug(\n        f\"Mapping {len(self.dicom_files)} files to new paths\"\n    )\n\n    # Create a progress bar that can be used to track everything\n    with self._progress_bar() as progress_bar:\n        ################################################################################\n        # Resolve new paths\n        ################################################################################\n        file_map: Dict[Path, Path] = self._resolve_new_paths(\n            progress_bar=progress_bar,\n            num_workers=num_workers,\n            truncate_uids=truncate_uids,\n        )\n    self.logger.info(\"Finished resolving paths\")\n\n    ################################################################################\n    # Check if any of the resolved paths are duplicates\n    ################################################################################\n    file_map = self._check_duplicates(file_map)\n    self.logger.info(\"Finished checking for duplicates\")\n\n    ################################################################################\n    # Handle files\n    ################################################################################\n    if dry_run:\n        self._dry_run(file_map)\n        return\n\n    with self._progress_bar() as progress_bar:\n        task_files = progress_bar.add_task(\n            \"Handling files\", total=len(file_map)\n        )\n        new_paths: List[Path | None] = []\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            future_to_file = {\n                executor.submit(\n                    handle_file,\n                    source_path,\n                    resolved_path,\n                    action,\n                    overwrite,\n                ): source_path\n                for source_path, resolved_path in file_map.items()\n            }\n            for future in as_completed(future_to_file):\n                try:\n                    result = future.result()\n                    new_paths.append(result)\n                    progress_bar.update(task_files, advance=1)\n                except Exception as e:\n                    self.logger.exception(\n                        \"Failed to handle file\",\n                        exc_info=e,\n                        file=future_to_file[future],\n                    )\n</code></pre>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute(action)","title":"<code>action</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute(dry_run)","title":"<code>dry_run</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute(num_workers)","title":"<code>num_workers</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.execute(truncate_uids)","title":"<code>truncate_uids</code>","text":""},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.print_tree","title":"print_tree","text":"<pre><code>print_tree(base_dir: pathlib.Path | None = None) -&gt; None\n</code></pre> <p>Display the pattern structure as a tree visualization.</p> Notes <p>This only prints the target pattern, parsed and formatted. Performing a dry-run execute will display more information.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def print_tree(self, base_dir: Path | None = None) -&gt; None:\n    \"\"\"\n    Display the pattern structure as a tree visualization.\n\n    Notes\n    -----\n    This only prints the target pattern, parsed and formatted.\n    Performing a dry-run execute will display more information.\n\n    Raises\n    ------\n    SorterBaseError\n        If the tree visualization fails to generate.\n    \"\"\"\n    try:\n        base_dir = base_dir or Path().cwd().resolve()\n        tree = self._setup_tree(base_dir)\n        self._generate_tree_structure(self.pattern_preview, tree)\n        self._console.print(tree)\n    except Exception as e:\n        errmsg = \"Failed to generate tree visualization.\"\n        raise SorterBaseError(errmsg) from e\n</code></pre>"},{"location":"reference/dicom/sort/dicomsorter/#imgtools.dicom.sort.dicomsorter.DICOMSorter.validate_keys","title":"validate_keys","text":"<pre><code>validate_keys() -&gt; None\n</code></pre> <p>Validate extracted keys. Subclasses should implement this method to perform specific validations based on their context.</p> <p>Validate the DICOM keys in the target pattern.</p> <p>If any invalid keys are found, it suggests similar valid keys and raises an error.</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def validate_keys(self) -&gt; None:\n    \"\"\"Validate the DICOM keys in the target pattern.\n\n    If any invalid keys are found, it\n    suggests similar valid keys and raises an error.\n    \"\"\"\n    if not self.invalid_keys:\n        return\n\n    for key in sorted(self.invalid_keys):\n        # TODO: keep this logic, but make the suggestion more user-friendly/readable\n        similar = similar_tags(key)\n        suggestion = (\n            f\"\\n\\tDid you mean: [bold green]{', '.join(similar)}[/bold green]?\"\n            if similar\n            else \" And [bold red]no similar keys[/bold red] found.\"\n        )\n        _error = (\n            f\"Invalid DICOM key: [bold red]{key}[/bold red].{suggestion}\"\n        )\n        self._console.print(f\"{_error}\")\n    self._console.print(f\"Parsed Path: `{self.pattern_preview}`\")\n    errmsg = \"Invalid DICOM Keys found.\"\n    raise InvalidDICOMKeyError(errmsg)\n</code></pre>"},{"location":"reference/dicom/sort/exceptions/","title":"Exceptions","text":""},{"location":"reference/dicom/sort/exceptions/#imgtools.dicom.sort.exceptions","title":"exceptions","text":""},{"location":"reference/dicom/sort/exceptions/#imgtools.dicom.sort.exceptions.DICOMSortError","title":"DICOMSortError","text":"<pre><code>DICOMSortError(\n    message: str = \"An error occurred during DICOM sorting\",\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Base exception for DICOM sorting errors.</p> Source code in <code>src/imgtools/dicom/sort/exceptions.py</code> <pre><code>def __init__(\n    self, message: str = \"An error occurred during DICOM sorting\"\n) -&gt; None:\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/sort/exceptions/#imgtools.dicom.sort.exceptions.InvalidDICOMKeyError","title":"InvalidDICOMKeyError","text":"<pre><code>InvalidDICOMKeyError(key: str | None = None)\n</code></pre> <p>               Bases: <code>imgtools.dicom.sort.exceptions.DICOMSortError</code></p> <p>Raised when a DICOM key is invalid.</p> Source code in <code>src/imgtools/dicom/sort/exceptions.py</code> <pre><code>def __init__(self, key: str | None = None) -&gt; None:\n    message = f\"Invalid DICOM key: {key}\" if key else \"Invalid DICOM key\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/sort/exceptions/#imgtools.dicom.sort.exceptions.InvalidPatternError","title":"InvalidPatternError","text":"<pre><code>InvalidPatternError(pattern: str | None = None)\n</code></pre> <p>               Bases: <code>imgtools.dicom.sort.exceptions.DICOMSortError</code></p> <p>Raised when the target pattern is invalid.</p> Source code in <code>src/imgtools/dicom/sort/exceptions.py</code> <pre><code>def __init__(self, pattern: str | None = None) -&gt; None:\n    message = (\n        f\"Invalid target pattern: {pattern}\"\n        if pattern\n        else \"Invalid target pattern\"\n    )\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/sort/exceptions/#imgtools.dicom.sort.exceptions.SorterBaseError","title":"SorterBaseError","text":"<pre><code>SorterBaseError(\n    message: str = \"An error occurred during sorting\",\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Base exception for sorting errors.</p> Source code in <code>src/imgtools/dicom/sort/exceptions.py</code> <pre><code>def __init__(\n    self, message: str = \"An error occurred during sorting\"\n) -&gt; None:\n    super().__init__(message)\n</code></pre>"},{"location":"reference/dicom/sort/highlighter/","title":"Highlighter","text":""},{"location":"reference/dicom/sort/highlighter/#imgtools.dicom.sort.highlighter","title":"highlighter","text":""},{"location":"reference/dicom/sort/highlighter/#imgtools.dicom.sort.highlighter.TagHighlighter","title":"TagHighlighter","text":"<p>               Bases: <code>rich.highlighter.RegexHighlighter</code></p> <p>Highlights DICOM keys, forward slashes, and braces when printing with Rich.</p> <p>Attributes:</p> Name Type Description <code>base_style</code> <code>str</code> <p>The base style for highlighted elements.</p> <code>highlights</code> <code>list of str</code> <p>Regular expressions used for highlighting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from rich.console import (\n...     Console,\n... )\n&gt;&gt;&gt; highlighter = TagHighlighter()\n&gt;&gt;&gt; console = Console(\n                highlighter=highlighter,\n                theme=Theme(\n                        {\n                                \"example.Tag\": \"bold magenta\",\n                        }\n                ),\n        )\n&gt;&gt;&gt; console.print(\n...     \"%(PatientID)s/%(StudyID)s/{SomeValue}\"\n... )\n</code></pre>"},{"location":"reference/dicom/sort/sort_method/","title":"Sort method","text":""},{"location":"reference/dicom/sort/sort_method/#imgtools.dicom.sort.sort_method","title":"sort_method","text":"<p>File Handling Utility.</p> <p>This module provides functionality to handle files with different actions such as moving, copying, creating symbolic links, and creating hard links.</p> <p>Classes:</p> Name Description <code>FileAction</code> <p>Enum for file actions including MOVE, COPY, SYMLINK, and HARDLINK.</p> <p>Functions:</p> Name Description <code>handle_file</code> <p>Perform specified file operations (move, copy, symlink, hardlink) on a file.</p> Notes <p>Symlinks vs. Hardlinks: - Symlinks (Symbolic Links):     A symbolic link is a shortcut or reference to another file. It creates a new     file that points to the target file but does not duplicate the file's data.     If the target file is moved or deleted, the symlink becomes invalid.     Example: <code>ln -s target symlink</code></p> <ul> <li>Hardlinks:     A hard link is an additional reference to the same data on the disk. Both the     original file and the hard link share the same inode, meaning they are     indistinguishable. Deleting the original file does not affect the hard link,     and vice versa.     Example: <code>ln target hardlink</code></li> </ul> <p>Examples:</p> <p>Move a file:     &gt;&gt;&gt; from pathlib import (     ...     Path,     ... )     &gt;&gt;&gt; handle_file(     ...     Path(\"source.txt\"),     ...     Path(\"destination.txt\"),     ...     action=FileAction.MOVE,     ... )</p> <p>Create a symlink:     &gt;&gt;&gt; handle_file(     ...     Path(\"source.txt\"),     ...     Path(\"symlink.txt\"),     ...     action=FileAction.SYMLINK,     ... )</p> <p>Copy a file:     &gt;&gt;&gt; handle_file(     ...     Path(\"source.txt\"),     ...     Path(\"copy.txt\"),     ...     action=FileAction.COPY,     ...     overwrite=True,     ... )</p>"},{"location":"reference/dicom/sort/sort_method/#imgtools.dicom.sort.sort_method.FileAction","title":"FileAction","text":"<p>               Bases: <code>enum.Enum</code></p> <p>Methods:</p> Name Description <code>choices</code> <p>Return a list of valid file actions.</p>"},{"location":"reference/dicom/sort/sort_method/#imgtools.dicom.sort.sort_method.FileAction.choices","title":"choices  <code>staticmethod</code>","text":"<pre><code>choices() -&gt; typing.List[str]\n</code></pre> <p>Return a list of valid file actions.</p> Source code in <code>src/imgtools/dicom/sort/sort_method.py</code> <pre><code>@staticmethod\ndef choices() -&gt; List[str]:\n    \"\"\"Return a list of valid file actions.\"\"\"\n    return [action.value for action in FileAction]\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/","title":"Sorter base","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base","title":"sorter_base","text":"<p>Base module for sorting files based on customizable patterns.</p> <p>This module provides a foundation for implementing file sorting logic, particularly for handling DICOM files or other structured data.</p> <p>The <code>SorterBase</code> class serves as an abstract base class for: - Parsing and validating patterns used for organizing files. - Visualizing the target directory structure through a tree representation. - Allowing subclasses to implement specific validation and resolution logic.</p> <p>Important: While this module helps define the target directory structure for files based on customizable metadata-driven patterns, it does not alter the filename (basename) of the source files. The original filename is preserved during the sorting process. This ensures that files with the same metadata fields but different filenames are not overwritten, which is critical when dealing with fields like <code>InstanceNumber</code> that may have common values across different files.</p> <p>Examples:</p> <p>Given a source file: <code>/source_dir/HN-CHUS-082/1-1.dcm</code></p> <p>And a target pattern: <code>./data/dicoms/%PatientID/Study-%StudyInstanceUID/Series-%SeriesInstanceUID/%Modality/</code></p> <p>The resolved path will be: <code>./data/dicoms/HN-CHUS-082/Study-06980/Series-67882/RTSTRUCT/1-1.dcm</code></p> <p>The <code>SorterBase</code> class ensures that only the directory structure is adjusted based on metadata, leaving the original filename intact.</p> <p>Functions:</p> Name Description <code>resolve_path</code> <p>Worker function to resolve a single path.</p>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase","title":"SorterBase","text":"<pre><code>SorterBase(\n    source_directory: pathlib.Path,\n    target_pattern: str,\n    pattern_parser: typing.Pattern = imgtools.dicom.sort.sorter_base.DEFAULT_PATTERN_PARSER,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code></p> <p>Abstract base class for sorting files based on customizable patterns.</p> <p>This class provides functionalities for: - Pattern parsing and validation - Tree visualization of file structures - Extensibility for subclass-specific implementations</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The directory containing the files to be sorted.</p> required <code>str</code> <p>The pattern string for sorting files.</p> required <code>typing.Pattern</code> <p>Custom regex pattern for parsing patterns uses default that             matches placeholders in the format of <code>%KEY</code> or <code>{KEY}</code>: <code>re.compile(r\"%([A-Za-z]+)|\\\\{([A-Za-z]+)\\\\}\")</code>.</p> <code>imgtools.dicom.sort.sorter_base.DEFAULT_PATTERN_PARSER</code> <p>Attributes:</p> Name Type Description <code>source_directory</code> <code>pathlib.Path</code> <p>The directory containing the files to be sorted.</p> <code>format</code> <code>str</code> <p>The parsed format string with placeholders for keys.</p> <code>dicom_files</code> <code>list of Path</code> <p>The list of DICOM files to be sorted.</p> <p>Methods:</p> Name Description <code>print_tree</code> <p>Display the pattern structure as a tree visualization.</p> <code>validate_keys</code> <p>Validate extracted keys. Subclasses should implement this method</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def __init__(\n    self,\n    source_directory: Path,\n    target_pattern: str,\n    pattern_parser: Pattern = DEFAULT_PATTERN_PARSER,\n) -&gt; None:\n    if not source_directory.exists() or not source_directory.is_dir():\n        errmsg = f\"Source directory {source_directory} does not exist or is not a directory.\"\n        raise SorterBaseError(errmsg)\n\n    self.source_directory = source_directory\n    self._target_pattern = target_pattern\n    self._pattern_parser = pattern_parser\n    self._keys: Set[str] = set()\n    self._console: Console = self._initialize_console()\n    self.logger = logger.bind(source_directory=self.source_directory)\n\n    try:\n        self.dicom_files = find_dicoms(\n            directory=self.source_directory,\n            check_header=False,\n            recursive=True,\n            extension=\"dcm\",\n        )\n        self.logger.info(f\"Found {len(self.dicom_files)} files\")\n    except Exception as e:\n        errmsg = \"Failed to find files in the source directory.\"\n        raise SorterBaseError(errmsg) from e\n\n    try:\n        self._parser = PatternParser(\n            self._target_pattern, self._pattern_parser\n        )\n        self._format, parsed_keys = self._parser.parse()\n        self._keys = set(parsed_keys)\n    except Exception as e:\n        errmsg = \"Failed to initialize SorterBase.\"\n        raise SorterBaseError(errmsg) from e\n    self.validate_keys()\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase(source_directory)","title":"<code>source_directory</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase(target_pattern)","title":"<code>target_pattern</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase(pattern_parser)","title":"<code>pattern_parser</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase.format","title":"format  <code>property</code>","text":"<pre><code>format: str\n</code></pre> <p>Get the formatted pattern string.</p>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: typing.Set[str]\n</code></pre> <p>Get the set of keys extracted from the pattern.</p>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase.pattern_preview","title":"pattern_preview  <code>property</code>","text":"<pre><code>pattern_preview: str\n</code></pre> <p>Returns a human readable preview of the pattern.</p> <p>Useful for visualizing the pattern structure and can be highlighted using Rich Console.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; target_pattern = \"%key1/%key2/%key3\"\n&gt;&gt;&gt; pattern_preview = \"{key1}/{key2}/{key3}\"\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase.print_tree","title":"print_tree","text":"<pre><code>print_tree(base_dir: pathlib.Path | None = None) -&gt; None\n</code></pre> <p>Display the pattern structure as a tree visualization.</p> Notes <p>This only prints the target pattern, parsed and formatted. Performing a dry-run execute will display more information.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def print_tree(self, base_dir: Path | None = None) -&gt; None:\n    \"\"\"\n    Display the pattern structure as a tree visualization.\n\n    Notes\n    -----\n    This only prints the target pattern, parsed and formatted.\n    Performing a dry-run execute will display more information.\n\n    Raises\n    ------\n    SorterBaseError\n        If the tree visualization fails to generate.\n    \"\"\"\n    try:\n        base_dir = base_dir or Path().cwd().resolve()\n        tree = self._setup_tree(base_dir)\n        self._generate_tree_structure(self.pattern_preview, tree)\n        self._console.print(tree)\n    except Exception as e:\n        errmsg = \"Failed to generate tree visualization.\"\n        raise SorterBaseError(errmsg) from e\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.SorterBase.validate_keys","title":"validate_keys  <code>abstractmethod</code>","text":"<pre><code>validate_keys() -&gt; None\n</code></pre> <p>Validate extracted keys. Subclasses should implement this method to perform specific validations based on their context.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>@abstractmethod\ndef validate_keys(self) -&gt; None:\n    \"\"\"\n    Validate extracted keys. Subclasses should implement this method\n    to perform specific validations based on their context.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(\n    path: pathlib.Path,\n    keys: typing.Set[str],\n    format_str: str,\n    truncate: int = 5,\n    check_existing: bool = True,\n    force: bool = True,\n) -&gt; typing.Tuple[pathlib.Path, pathlib.Path]\n</code></pre> <p>Worker function to resolve a single path.</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The source file path.</p> required <code>typing.Set[str]</code> <p>The DICOM keys required for resolving the path.</p> required <code>str</code> <p>The format string for the resolved path.</p> required <code>bool</code> <p>If True, check if the resolved path already exists (default is True).</p> <code>True</code> <code>int</code> <p>The number of characters to trunctae UID values (default is 5).</p> <code>5</code> <code>bool</code> <p>passed to pydicom.dcmread() to force reading the file (default is False).</p> <code>True</code> <p>Returns:</p> Type Description <code>typing.Tuple[pathlib.Path, pathlib.Path]</code> <p>The source path and resolved path.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def resolve_path(\n    path: Path,\n    keys: Set[str],\n    format_str: str,\n    truncate: int = 5,\n    check_existing: bool = True,\n    force: bool = True,\n) -&gt; Tuple[Path, Path]:\n    \"\"\"\n    Worker function to resolve a single path.\n\n    Parameters\n    ----------\n    path : Path\n        The source file path.\n    keys : Set[str]\n        The DICOM keys required for resolving the path.\n    format_str : str\n        The format string for the resolved path.\n    check_existing : bool, optional\n        If True, check if the resolved path already exists (default is True).\n    truncate : int, optional\n        The number of characters to trunctae UID values (default is 5).\n    force : bool, optional\n        passed to pydicom.dcmread() to force reading the file (default is False).\n\n    Returns\n    -------\n    Tuple[Path, Path]\n        The source path and resolved path.\n    \"\"\"\n    tags: Dict[str, str] = read_tags(\n        path, list(keys), truncate=truncate, force=force, default=\"Unknown\"\n    )\n    resolved_path = Path(format_str % tags, path.name)\n    if check_existing and not resolved_path.exists():\n        resolved_path = resolved_path.resolve()\n    elif check_existing:\n        errmsg = f\"Path {resolved_path} already exists.\"\n        logger.error(errmsg, source_path=path, resolved_path=resolved_path)\n        raise FileExistsError(errmsg)\n\n    return path, resolved_path\n</code></pre>"},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(path)","title":"<code>path</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(keys)","title":"<code>keys</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(format_str)","title":"<code>format_str</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(check_existing)","title":"<code>check_existing</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(truncate)","title":"<code>truncate</code>","text":""},{"location":"reference/dicom/sort/sorter_base/#imgtools.dicom.sort.sorter_base.resolve_path(force)","title":"<code>force</code>","text":""},{"location":"reference/io/nnunet_output/","title":"Nnunet output","text":""},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output","title":"nnunet_output","text":""},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.MaskSavingStrategy","title":"MaskSavingStrategy","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enum for mask saving strategies.</p> <p>Attributes:</p> Name Type Description <code>LABEL_IMAGE</code> <code>str</code> <p>No overlaps allowed.</p> <code>SPARSE_MASK</code> <code>str</code> <p>Allows overlaps, but is lossy if overlaps exist.</p> <code>REGION_MASK</code> <code>str</code> <p>Work around that creates a new region for each overlap.</p>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.MaskSavingStrategyError","title":"MaskSavingStrategyError","text":"<p>               Bases: <code>imgtools.io.nnunet_output.nnUNetOutputError</code></p> <p>Raised when an invalid mask saving strategy is provided.</p>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.MissingROIsError","title":"MissingROIsError","text":"<pre><code>MissingROIsError(\n    sample_number: str,\n    expected_rois: list[str],\n    found_rois: list[list[str]],\n)\n</code></pre> <p>               Bases: <code>imgtools.io.nnunet_output.nnUNetOutputError</code></p> <p>Raised when a VectorMask does not contain all required ROI keys.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>def __init__(\n    self,\n    sample_number: str,\n    expected_rois: list[str],\n    found_rois: list[list[str]],\n) -&gt; None:\n    msg = (\n        f\"Not all required ROI names found in sample {sample_number}. \"\n        f\"Expected: {expected_rois}. Found: {found_rois}\"\n    )\n    super().__init__(msg)\n    self.sample_number = sample_number\n    self.expected_rois = expected_rois\n    self.found_rois = found_rois\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.NoSegmentationImagesError","title":"NoSegmentationImagesError","text":"<pre><code>NoSegmentationImagesError(sample_number: str)\n</code></pre> <p>               Bases: <code>imgtools.io.nnunet_output.nnUNetOutputError</code></p> <p>Raised when no segmentation images are found in a sample.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>def __init__(self, sample_number: str) -&gt; None:\n    msg = f\"No segmentation images found in sample {sample_number}\"\n    super().__init__(msg)\n    self.sample_number = sample_number\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput","title":"nnUNetOutput","text":"<p>               Bases: <code>pydantic.BaseModel</code></p> <p>Configuration model for saving medical imaging outputs in nnUNet format.</p> <p>This class provides a standardized configuration for saving medical images, supporting various file formats and output organization strategies.</p> <p>Attributes:</p> Name Type Description <code>directory</code> <code>pathlib.Path</code> <p>Directory where output files will be saved. Must exist and be writable.</p> <code>filename_format</code> <code>str</code> <p>Format string for output filenames with placeholders for metadata values.</p> <code>existing_file_mode</code> <code>imgtools.io.writers.ExistingFileMode</code> <p>How to handle existing files (FAIL, SKIP, OVERWRITE).</p> <code>extra_context</code> <code>typing.Dict[str, typing.Any]</code> <p>Additional metadata to include when saving files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from imgtools.io import nnUNetOutput\n&gt;&gt;&gt; from imgtools.io.writers import ExistingFileMode\n&gt;&gt;&gt; output = nnUNetOutput(\n...     directory=\"results/patient_scans\",\n...     existing_file_mode=ExistingFileMode.SKIP,\n... )\n&gt;&gt;&gt; output(scan_list)  # Save all scans in the list\n</code></pre> <p>Methods:</p> Name Description <code>default</code> <p>Create a default instance of SampleOutput.</p> <code>finalize_dataset</code> <p>Finalize dataset by generating preprocessing scripts and dataset JSON configuration.</p> <code>model_post_init</code> <p>Initialize the writer after model initialization.</p> <code>validate_directory</code> <p>Validate that the output directory exists or can be created, and is writable.</p>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput.writer","title":"writer  <code>property</code>","text":"<pre><code>writer: imgtools.io.writers.AbstractBaseWriter\n</code></pre> <p>Get the writer instance.</p>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; imgtools.io.nnunet_output.nnUNetOutput\n</code></pre> <p>Create a default instance of SampleOutput.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; nnUNetOutput:\n    \"\"\"Create a default instance of SampleOutput.\"\"\"\n    return cls(\n        directory=Path(\"output\"),\n        dataset_name=\"Dataset\",\n        roi_keys=[\"ROI_1\", \"ROI_2\"],\n        mask_saving_strategy=MaskSavingStrategy.LABEL_IMAGE,\n        existing_file_mode=ExistingFileMode.FAIL,\n        extra_context={},\n    )\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput.finalize_dataset","title":"finalize_dataset","text":"<pre><code>finalize_dataset() -&gt; None\n</code></pre> <p>Finalize dataset by generating preprocessing scripts and dataset JSON configuration.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>def finalize_dataset(self) -&gt; None:\n    \"\"\"Finalize dataset by generating preprocessing scripts and dataset JSON configuration.\"\"\"\n\n    generate_nnunet_scripts(self.directory, self.dataset_id)\n\n    index_df = pd.read_csv(self.writer.index_file)\n    _image_modalities = index_df[\"Modality\"].unique()\n\n    # Construct channel names mapping\n    channel_names = {\n        channel_num.lstrip(\"0\") or \"0\": modality\n        for modality, channel_num in MODALITY_MAP.items()\n        if modality in _image_modalities\n    }\n\n    # Count the number of training cases\n    num_training_cases = sum(\n        1\n        for file in (self.directory / \"imagesTr\").iterdir()\n        if file.is_file()\n    )\n\n    # Construct labels\n    labels: dict[str, int | list[int]] = {\"background\": 0}\n    if self.mask_saving_strategy is MaskSavingStrategy.REGION_MASK:\n        n_components = len(self.roi_keys)\n        max_val = 2**n_components\n\n        for component_index in range(n_components):\n            indices = [\n                value\n                for value in range(1, max_val)\n                if (value &gt;&gt; component_index) &amp; 1\n            ]\n            labels[self.roi_keys[component_index]] = indices\n        regions_class_order = tuple(idx + 1 for idx in range(n_components))\n    else:\n        labels = {\n            **{label: i + 1 for i, label in enumerate(self.roi_keys)},\n        }\n        regions_class_order = None\n\n    generate_dataset_json(\n        self.directory,\n        channel_names=channel_names,\n        labels=labels,\n        num_training_cases=num_training_cases,\n        file_ending=\".nii.gz\",\n        regions_class_order=regions_class_order,\n    )\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Initialize the writer after model initialization.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:  # type: ignore # noqa: ANN001\n    \"\"\"Initialize the writer after model initialization.\"\"\"\n    # Create required directories\n    for subdir in [\"nnUNet_results\", \"nnUNet_preprocessed\", \"nnUNet_raw\"]:\n        (self.directory / subdir).mkdir(parents=True, exist_ok=True)\n\n    # Determine the next available dataset ID\n    existing_ids = {\n        int(folder.name[7:10])\n        for folder in (self.directory / \"nnUNet_raw\").glob(\"Dataset*\")\n        if folder.name[7:10].isdigit()\n    }\n    self.dataset_id = min(set(range(1, 1000)) - existing_ids)\n\n    # Update root directory to the specific dataset folder\n    self.directory = (\n        self.directory\n        / \"nnUNet_raw\"\n        / f\"Dataset{self.dataset_id:03d}_{self.dataset_name}\"\n    )\n\n    self._file_name_format = (\n        \"{DirType}{SplitType}/{Dataset}_{SampleID}.nii.gz\"\n    )\n\n    self._writer = NIFTIWriter(\n        root_directory=self.directory,\n        existing_file_mode=self.existing_file_mode,\n        filename_format=self._file_name_format,\n        context=self.extra_context,\n    )\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutput.validate_directory","title":"validate_directory  <code>classmethod</code>","text":"<pre><code>validate_directory(v: str | pathlib.Path) -&gt; pathlib.Path\n</code></pre> <p>Validate that the output directory exists or can be created, and is writable.</p> Source code in <code>src/imgtools/io/nnunet_output.py</code> <pre><code>@field_validator(\"directory\")\n@classmethod\ndef validate_directory(cls, v: str | Path) -&gt; Path:\n    \"\"\"Validate that the output directory exists or can be created, and is writable.\"\"\"\n    return validate_directory(v)\n</code></pre>"},{"location":"reference/io/nnunet_output/#imgtools.io.nnunet_output.nnUNetOutputError","title":"nnUNetOutputError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors related to sample data.</p>"},{"location":"reference/io/readers/","title":"Readers","text":""},{"location":"reference/io/readers/#imgtools.io.readers","title":"readers","text":"<p>Functions:</p> Name Description <code>read_dicom_auto</code> <p>General DICOM reader that dispatches to the correct class based on modality.</p> <code>read_dicom_series</code> <p>Read DICOM series as SimpleITK Image.</p>"},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_auto","title":"read_dicom_auto","text":"<pre><code>read_dicom_auto(\n    path: str,\n    modality: typing.Optional[str] = None,\n    **kwargs: typing.Any\n) -&gt; imgtools.io.readers.MedImageT\n</code></pre> <p>General DICOM reader that dispatches to the correct class based on modality.</p> <p>This function automatically determines the appropriate class based on the modality and calls its from_dicom method with the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to the DICOM file or directory containing DICOM files.</p> required <code>str</code> <p>Explicitly specify the modality. If None, the function will try to determine the modality from the DICOM metadata.</p> <code>None</code> <code>typing.Any</code> <p>Additional keyword arguments to pass to the specific from_dicom method. Common parameters include:     - series_id: str | None - Series ID for DICOM series     - recursive: bool - Whether to search recursively for DICOM files     - file_names: list[str] | None - Specific file names to read     - pet_image_type: For PET images, specify SUV or ACT</p> <code>{}</code> <p>Returns:</p> Type Description <code>imgtools.io.readers.MedImageT</code> <p>The loaded image or mask object of the appropriate type.</p> Source code in <code>src/imgtools/io/readers.py</code> <pre><code>def read_dicom_auto(\n    path: str,\n    modality: Optional[str] = None,\n    **kwargs: Any,  # noqa\n) -&gt; MedImageT:\n    \"\"\"General DICOM reader that dispatches to the correct class based on modality.\n\n    This function automatically determines the appropriate class based on the\n    modality and calls its from_dicom method with the provided arguments.\n\n    Parameters\n    ----------\n    path : str\n        Path to the DICOM file or directory containing DICOM files.\n    modality : str, optional\n        Explicitly specify the modality. If None, the function will try to determine\n        the modality from the DICOM metadata.\n    **kwargs : Any\n        Additional keyword arguments to pass to the specific from_dicom method.\n        Common parameters include:\n            - series_id: str | None - Series ID for DICOM series\n            - recursive: bool - Whether to search recursively for DICOM files\n            - file_names: list[str] | None - Specific file names to read\n            - pet_image_type: For PET images, specify SUV or ACT\n\n    Returns\n    -------\n    MedImageT\n        The loaded image or mask object of the appropriate type.\n\n    Raises\n    ------\n    ValueError\n        If the modality is unknown or cannot be determined.\n    ImportError\n        If the required class for a modality cannot be imported.\n    \"\"\"\n    from pydicom import dcmread\n\n    from imgtools.dicom import find_dicoms\n\n    # Try to determine modality if not provided\n    if not modality:\n        # If it's a directory with DICOM files, read the first file\n        path_obj = Path(path)\n        if path_obj.is_dir():\n            first_file = find_dicoms(\n                directory=path_obj,\n                recursive=kwargs.pop(\"recursive\", False),\n                limit=1,\n            )\n            if not first_file:\n                errmsg = (\n                    f\"No DICOM files found in directory: {path_obj}. \"\n                    \"Please check the path and try again.\"\n                )\n                raise FileNotFoundError(errmsg)\n            # find_dicoms returns a list, even if limit=1\n            dcm = dcmread(first_file[0], stop_before_pixels=True)\n        else:\n            # It's a file\n            dcm = dcmread(path, stop_before_pixels=True)\n\n        # Extract modality from the DICOM header\n        modality = getattr(dcm, \"Modality\", None)\n        if not modality:\n            raise ValueError(\n                \"Could not determine modality from DICOM file. Please specify modality parameter.\"\n            )\n            # Dispatch based on modality\n\n    match modality:\n        case \"CT\" | \"MR\":\n            from imgtools.coretypes import Scan\n\n            return Scan.from_dicom(path, **kwargs)\n        case \"PT\":\n            from imgtools.coretypes import PET\n\n            return PET.from_dicom(path, **kwargs)\n        case \"RTDOSE\":\n            from imgtools.coretypes import Dose\n\n            return Dose.from_dicom(path, **kwargs)\n        case \"RTSTRUCT\":\n            from imgtools.coretypes import RTStructureSet\n\n            return RTStructureSet.from_dicom(path)\n        case \"SEG\":\n            from imgtools.coretypes import SEG\n\n            return SEG.from_dicom(path)\n        case _:\n            error_msg = f\"Unknown or unsupported modality: {modality}\"\n            raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_auto(path)","title":"<code>path</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_auto(modality)","title":"<code>modality</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_auto(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series","title":"read_dicom_series","text":"<pre><code>read_dicom_series(\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: typing.Any\n) -&gt; tuple[SimpleITK.Image, dict]\n</code></pre> <p>Read DICOM series as SimpleITK Image.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Path to directory containing the DICOM series.</p> required <code>bool</code> <p>Whether to recursively parse the input directory when searching for DICOM series,</p> <code>False</code> <code>bool</code> <p>Whether to recursively parse the input directory when searching for DICOM series,</p> <code>False</code> <code>str | None</code> <p>Specifies the DICOM series to load if multiple series are present in the directory. If None and multiple series are present, loads the first series found.</p> <code>None</code> <code>str | None</code> <p>Specifies the DICOM series to load if multiple series are present in the directory. If None and multiple series are present, loads the first series found.</p> <code>None</code> <code>list[str] | None</code> <p>If there are multiple acquisitions/\"subseries\" for an individual series, use the provided list of file_names to set the ImageSeriesReader.</p> <code>None</code> <code>list[str] | None</code> <p>If there are multiple acquisitions/\"subseries\" for an individual series, use the provided list of file_names to set the ImageSeriesReader.</p> <code>None</code> <p>Returns:</p> Type Description <code>image</code> <p>SimpleITK Image object containing the DICOM series.</p> <code>metadata</code> <p>Dictionary containing metadata extracted from one file in the series.</p> Source code in <code>src/imgtools/io/readers.py</code> <pre><code>def read_dicom_series(\n    path: str,\n    series_id: str | None = None,\n    recursive: bool = False,\n    file_names: list[str] | None = None,\n    **kwargs: Any,  # noqa\n) -&gt; tuple[sitk.Image, dict]:\n    \"\"\"Read DICOM series as SimpleITK Image.\n\n    Parameters\n    ----------\n    path\n       Path to directory containing the DICOM series.\n\n    recursive, default=False\n       Whether to recursively parse the input directory when searching for\n       DICOM series,\n\n    series_id, default=None\n       Specifies the DICOM series to load if multiple series are present in\n       the directory. If None and multiple series are present, loads the first\n       series found.\n\n    file_names, default=None\n        If there are multiple acquisitions/\"subseries\" for an individual series,\n        use the provided list of file_names to set the ImageSeriesReader.\n\n    Returns\n    -------\n    image\n        SimpleITK Image object containing the DICOM series.\n    metadata\n        Dictionary containing metadata extracted from one file in the series.\n    \"\"\"\n    reader = sitk.ImageSeriesReader()\n    sitk_file_names = reader.GetGDCMSeriesFileNames(\n        path,\n        seriesID=series_id if series_id else \"\",\n        recursive=recursive,\n    )\n    if file_names is None:\n        file_names = sitk_file_names\n    elif set(file_names) &lt;= set(\n        sitk_file_names\n    ):  # Extracts the same order provided by sitk\n        file_names = [fn for fn in sitk_file_names if fn in file_names]\n    else:\n        errmsg = (\n            \"The provided file_names are not a subset of the files in the \"\n            \"directory.\"\n        )\n        errmsg += f\"\\nProvided file_names: {file_names}\"\n        errmsg += f\"\\n\\nFiles in directory: {sitk_file_names}\"\n        raise ValueError(errmsg)\n\n    reader.SetFileNames(file_names)\n\n    metadata = kwargs.pop(\"metadata\", None)\n\n    if not metadata:\n        # Extract metadata from the first file\n        metadata = extract_metadata(file_names[0])\n    # make sure its a dictionary\n    elif not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dictionary\")\n\n    metadata = cleanse_metadata(metadata)\n    metadata = convert_dictionary_datetime_values(metadata)\n    metadata = attrify(metadata)\n    return reader.Execute(), metadata\n</code></pre>"},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(path)","title":"<code>path</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(default)","title":"<code>default</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(series_id)","title":"<code>series_id</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(default)","title":"<code>default</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(file_names)","title":"<code>file_names</code>","text":""},{"location":"reference/io/readers/#imgtools.io.readers.read_dicom_series(default)","title":"<code>default</code>","text":""},{"location":"reference/io/sample_input/","title":"Sample input","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input","title":"sample_input","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput","title":"SampleInput","text":"<p>               Bases: <code>pydantic.BaseModel</code></p> <p>Configuration model for processing medical imaging samples.</p> <p>This class provides a standardized configuration for loading and processing medical imaging data, including DICOM crawling and ROI matching settings.</p> <p>Attributes:</p> Name Type Description <code>directory</code> <code>pathlib.Path</code> <p>Directory containing the input files. Must exist and be readable.</p> <code>dataset_name</code> <code>str | None</code> <p>Optional name for the dataset. Defaults to the base name of the input directory.</p> <code>update_crawl</code> <code>bool</code> <p>Whether to force a new crawl even if one exists. Default is False.</p> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is (CPU cores - 2) or 1.</p> <code>modalities</code> <code>list[str] | None</code> <p>List of modalities to include. None means include all modalities.</p> <code>roi_matcher</code> <code>imgtools.coretypes.masktypes.ROIMatcher</code> <p>Configuration for matching regions of interest in the images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from imgtools.io.loaders.sample_input import (\n...     SampleInput,\n... )\n&gt;&gt;&gt; config = SampleInput(\n...     directory=\"data/NSCLC-Radiomics\"\n... )\n&gt;&gt;&gt; config.dataset_name\n'NSCLC-Radiomics'\n</code></pre> <pre><code>&gt;&gt;&gt; # Using the factory method with ROI matching parameters\n&gt;&gt;&gt; config = SampleInput.build(\n...     directory=\"data/NSCLC-Radiomics\",\n...     roi_match_map={\n...         \"GTV\": [\"GTV.*\"],\n...         \"PTV\": [\"PTV.*\"],\n...     },\n...     roi_ignore_case=True,\n...     roi_handling_strategy=\"merge\",\n... )\n</code></pre> <p>Methods:</p> Name Description <code>build</code> <p>Create a SampleInput with separate parameters for ROIMatcher.</p> <code>default</code> <p>Create a default SampleInput instance.</p> <code>query</code> <p>Query the interlacer for a specific modality.</p>"},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.crawler","title":"crawler  <code>property</code>","text":"<pre><code>crawler: imgtools.dicom.crawl.Crawler\n</code></pre> <p>Get the Crawler instance, initializing it if needed.</p> <p>Returns:</p> Type Description <code>imgtools.dicom.crawl.Crawler</code> <p>A DICOM crawler instance, initialized with the current configuration.</p> Notes <p>The crawler is lazily initialized on first access.</p>"},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.interlacer","title":"interlacer  <code>property</code>","text":"<pre><code>interlacer: imgtools.dicom.interlacer.Interlacer\n</code></pre> <p>Get the Interlacer instance, initializing it if needed.</p> <p>Returns:</p> Type Description <code>imgtools.dicom.interlacer.Interlacer</code> <p>An Interlacer instance tied to the current crawler.</p> Notes <p>The interlacer is lazily initialized on first access, which may trigger crawler initialization if it hasn't been accessed yet.</p>"},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build","title":"build  <code>classmethod</code>","text":"<pre><code>build(\n    directory: str | pathlib.Path,\n    dataset_name: str | None = None,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    modalities: list[str] | None = None,\n    roi_match_map: imgtools.coretypes.masktypes.Valid_Inputs = None,\n    roi_ignore_case: bool = True,\n    roi_handling_strategy: (\n        str | imgtools.coretypes.masktypes.ROIMatchStrategy\n    ) = imgtools.coretypes.masktypes.ROIMatchStrategy.MERGE,\n    roi_allow_multi_key_matches: bool = True,\n    roi_on_missing_regex: (\n        str\n        | imgtools.coretypes.masktypes.ROIMatchFailurePolicy\n    ) = imgtools.coretypes.masktypes.ROIMatchFailurePolicy.IGNORE,\n) -&gt; \"SampleInput\"\n</code></pre> <p>Create a SampleInput with separate parameters for ROIMatcher.</p> <p>This factory method allows users to specify ROIMatcher parameters directly instead of constructing a objects separately.</p> <p>Parameters:</p> Name Type Description Default <code>class</code> <p>The SampleInput class</p> required <code>str | pathlib.Path</code> <p>Directory containing the input files</p> required <code>str | None</code> <p>Name of the dataset, by default None (uses input directory name)</p> <code>None</code> <code>bool</code> <p>Whether to force recrawling, by default False</p> <code>False</code> <code>int | None</code> <p>Number of parallel jobs, by default None (uses CPU count - 2)</p> <code>None</code> <code>list[str] | None</code> <p>List of modalities to include, by default None (all)</p> <code>None</code> <code>imgtools.coretypes.masktypes.Valid_Inputs</code> <p>ROI matching patterns, by default None</p> <code>None</code> <code>bool</code> <p>Whether to ignore case in ROI matching, by default True</p> <code>True</code> <code>str | imgtools.coretypes.masktypes.ROIMatchStrategy</code> <p>Strategy for handling ROI matches, by default ROIMatchStrategy.MERGE</p> <code>imgtools.coretypes.masktypes.ROIMatchStrategy.MERGE</code> <code>bool</code> <p>Whether to allow one ROI to match multiple keys in the match_map.</p> <code>True</code> <code>str | imgtools.coretypes.masktypes.ROIMatchFailurePolicy</code> <p>How to handle when no ROI matches any pattern in match_map.</p> <code>imgtools.coretypes.masktypes.ROIMatchFailurePolicy.IGNORE</code> <p>Returns:</p> Type Description <code>imgtools.io.sample_input.SampleInput</code> <p>Configured SampleInput instance</p> Source code in <code>src/imgtools/io/sample_input.py</code> <pre><code>@classmethod\ndef build(\n    cls,\n    directory: str | Path,\n    dataset_name: str | None = None,\n    update_crawl: bool = False,\n    n_jobs: int | None = None,\n    modalities: list[str] | None = None,\n    roi_match_map: ROIMatcherInputs = None,\n    roi_ignore_case: bool = True,\n    roi_handling_strategy: str | ROIMatchStrategy = ROIMatchStrategy.MERGE,\n    roi_allow_multi_key_matches: bool = True,\n    roi_on_missing_regex: str | ROIMatchFailurePolicy = (\n        ROIMatchFailurePolicy.IGNORE\n    ),\n) -&gt; \"SampleInput\":\n    \"\"\"Create a SampleInput with separate parameters for ROIMatcher.\n\n    This factory method allows users to specify ROIMatcher parameters directly\n    instead of constructing a objects separately.\n\n    Parameters\n    ----------\n    cls : class\n        The SampleInput class\n    directory : str | Path\n        Directory containing the input files\n    dataset_name : str | None, optional\n        Name of the dataset, by default None (uses input directory name)\n    update_crawl : bool, optional\n        Whether to force recrawling, by default False\n    n_jobs : int | None, optional\n        Number of parallel jobs, by default None (uses CPU count - 2)\n    modalities : list[str] | None, optional\n        List of modalities to include, by default None (all)\n    roi_match_map : ROIMatcherInputs, optional\n        ROI matching patterns, by default None\n    roi_ignore_case : bool, optional\n        Whether to ignore case in ROI matching, by default True\n    roi_handling_strategy : str | ROIMatchStrategy, optional\n        Strategy for handling ROI matches, by default ROIMatchStrategy.MERGE\n    roi_allow_multi_key_matches : bool, default=True\n        Whether to allow one ROI to match multiple keys in the match_map.\n    roi_on_missing_regex : str | ROIMatchFailurePolicy, optional\n        How to handle when no ROI matches any pattern in match_map.\n\n    Returns\n    -------\n    SampleInput\n        Configured SampleInput instance\n    \"\"\"\n    # Convert string strategy to enum if needed\n    if isinstance(roi_handling_strategy, str):\n        roi_handling_strategy = ROIMatchStrategy(\n            roi_handling_strategy.lower()\n        )\n\n    if isinstance(roi_on_missing_regex, str):\n        roi_on_missing_regex = ROIMatchFailurePolicy(\n            roi_on_missing_regex.lower()\n        )\n\n    # Create the ROIMatcher\n    roi_matcher = create_roi_matcher(\n        roi_match_map,\n        handling_strategy=roi_handling_strategy,\n        ignore_case=roi_ignore_case,\n        allow_multi_key_matches=roi_allow_multi_key_matches,\n        on_missing_regex=roi_on_missing_regex,\n    )\n    num_jobs = n_jobs or max(1, multiprocessing.cpu_count() - 2)\n\n    # Create the SampleInput\n    return cls(\n        directory=Path(directory),\n        dataset_name=dataset_name,\n        update_crawl=update_crawl,\n        n_jobs=num_jobs,\n        modalities=modalities,\n        roi_matcher=roi_matcher,\n    )\n</code></pre>"},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(cls)","title":"<code>cls</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(directory)","title":"<code>directory</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(dataset_name)","title":"<code>dataset_name</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(update_crawl)","title":"<code>update_crawl</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(n_jobs)","title":"<code>n_jobs</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(modalities)","title":"<code>modalities</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(roi_match_map)","title":"<code>roi_match_map</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(roi_ignore_case)","title":"<code>roi_ignore_case</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(roi_handling_strategy)","title":"<code>roi_handling_strategy</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(roi_allow_multi_key_matches)","title":"<code>roi_allow_multi_key_matches</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.build(roi_on_missing_regex)","title":"<code>roi_on_missing_regex</code>","text":""},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; 'SampleInput'\n</code></pre> <p>Create a default SampleInput instance.</p> Source code in <code>src/imgtools/io/sample_input.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; \"SampleInput\":\n    \"\"\"Create a default SampleInput instance.\"\"\"\n    return cls.build(directory=\"./data\")\n</code></pre>"},{"location":"reference/io/sample_input/#imgtools.io.sample_input.SampleInput.query","title":"query","text":"<pre><code>query(\n    modalities: str | None = None,\n) -&gt; list[list[imgtools.dicom.interlacer.SeriesNode]]\n</code></pre> <p>Query the interlacer for a specific modality.</p> Source code in <code>src/imgtools/io/sample_input.py</code> <pre><code>def query(self, modalities: str | None = None) -&gt; list[list[SeriesNode]]:\n    \"\"\"Query the interlacer for a specific modality.\"\"\"\n    if modalities is None:\n        modalities = \",\".join(self.modalities) if self.modalities else \"*\"\n    return self.interlacer.query(modalities)\n</code></pre>"},{"location":"reference/io/sample_output/","title":"Sample output","text":""},{"location":"reference/io/sample_output/#imgtools.io.sample_output","title":"sample_output","text":""},{"location":"reference/io/sample_output/#imgtools.io.sample_output.AnnotatedPathSequence","title":"AnnotatedPathSequence","text":"<pre><code>AnnotatedPathSequence(\n    paths: typing.List[pathlib.Path],\n    errors: (\n        typing.List[\n            imgtools.io.sample_output.FailedToSaveSingleImageError\n        ]\n        | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>list</code></p> <p>Custom sequence of paths that behaves like a list but includes an errors attribute.</p> <p>This class is returned by SampleOutput.call to allow access to any errors that occurred during the save process while still behaving like a regular sequence of paths.</p> <p>Attributes:</p> Name Type Description <code>errors</code> <code>typing.List[imgtools.io.sample_output.FailedToSaveSingleImageError]</code> <p>List of errors that occurred during the save process.</p> <p>Parameters:</p> Name Type Description Default <code>typing.List[pathlib.Path]</code> <p>List of paths to saved files.</p> required <code>typing.List[imgtools.io.sample_output.FailedToSaveSingleImageError]</code> <p>List of errors that occurred during the save process.</p> <code>None</code> Source code in <code>src/imgtools/io/sample_output.py</code> <pre><code>def __init__(\n    self,\n    paths: List[Path],\n    errors: List[FailedToSaveSingleImageError] | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the annotated path sequence.\n\n    Parameters\n    ----------\n    paths : List[Path]\n        List of paths to saved files.\n    errors : List[FailedToSaveSingleImageError], optional\n        List of errors that occurred during the save process.\n    \"\"\"\n    super().__init__(paths)\n    self.errors = errors or []\n</code></pre>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.AnnotatedPathSequence(paths)","title":"<code>paths</code>","text":""},{"location":"reference/io/sample_output/#imgtools.io.sample_output.AnnotatedPathSequence(errors)","title":"<code>errors</code>","text":""},{"location":"reference/io/sample_output/#imgtools.io.sample_output.FailedToSaveSingleImageError","title":"FailedToSaveSingleImageError","text":"<pre><code>FailedToSaveSingleImageError(\n    message: str, image: imgtools.coretypes.MedImage\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when a single image fails to save.</p> Source code in <code>src/imgtools/io/sample_output.py</code> <pre><code>def __init__(self, message: str, image: MedImage) -&gt; None:\n    super().__init__(message)\n    self.image = image\n</code></pre>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.SampleOutput","title":"SampleOutput","text":"<p>               Bases: <code>pydantic.BaseModel</code></p> <p>Configuration model for saving medical imaging outputs.</p> <p>This class provides a standardized configuration for saving medical images, supporting various file formats and output organization strategies.</p> <p>Attributes:</p> Name Type Description <code>directory</code> <code>pathlib.Path</code> <p>Directory where output files will be saved. Must exist and be writable.</p> <code>filename_format</code> <code>str</code> <p>Format string for output filenames with placeholders for metadata values.</p> <code>existing_file_mode</code> <code>imgtools.io.writers.ExistingFileMode</code> <p>How to handle existing files (FAIL, SKIP, OVERWRITE).</p> <code>extra_context</code> <code>typing.Dict[str, typing.Any]</code> <p>Additional metadata to include when saving files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from imgtools.io import SampleOutput\n&gt;&gt;&gt; from imgtools.io.writers import ExistingFileMode\n&gt;&gt;&gt; output = SampleOutput(\n...     directory=\"results/patient_scans\",\n...     filename_format=\"{PatientID}/{Modality}/{ImageID}.nii.gz\",\n...     existing_file_mode=ExistingFileMode.SKIP,\n... )\n&gt;&gt;&gt; output(scan_list)  # Save all scans in the list\n</code></pre> <p>Methods:</p> Name Description <code>default</code> <p>Create a default instance of SampleOutput.</p> <code>model_post_init</code> <p>Initialize the writer after model initialization.</p> <code>validate_directory</code> <p>Validate that the output directory exists or can be created, and is writable.</p>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.SampleOutput.writer","title":"writer  <code>property</code>","text":"<pre><code>writer: imgtools.io.writers.AbstractBaseWriter\n</code></pre> <p>Get the writer instance.</p>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.SampleOutput.default","title":"default  <code>classmethod</code>","text":"<pre><code>default() -&gt; imgtools.io.sample_output.SampleOutput\n</code></pre> <p>Create a default instance of SampleOutput.</p> Source code in <code>src/imgtools/io/sample_output.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; SampleOutput:\n    \"\"\"Create a default instance of SampleOutput.\"\"\"\n    return cls(\n        directory=Path(\"output\"),\n        filename_format=DEFAULT_FILENAME_FORMAT,\n        existing_file_mode=ExistingFileMode.FAIL,\n        extra_context={},\n    )\n</code></pre>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.SampleOutput.model_post_init","title":"model_post_init","text":"<pre><code>model_post_init(__context) -&gt; None\n</code></pre> <p>Initialize the writer after model initialization.</p> Source code in <code>src/imgtools/io/sample_output.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:  # type: ignore # noqa: ANN001\n    \"\"\"Initialize the writer after model initialization.\"\"\"\n    self._writer = NIFTIWriter(\n        root_directory=self.directory,\n        existing_file_mode=self.existing_file_mode,\n        filename_format=self.filename_format,\n        context=self.extra_context,\n    )\n</code></pre>"},{"location":"reference/io/sample_output/#imgtools.io.sample_output.SampleOutput.validate_directory","title":"validate_directory  <code>classmethod</code>","text":"<pre><code>validate_directory(v: str | pathlib.Path) -&gt; pathlib.Path\n</code></pre> <p>Validate that the output directory exists or can be created, and is writable.</p> Source code in <code>src/imgtools/io/sample_output.py</code> <pre><code>@field_validator(\"directory\")\n@classmethod\ndef validate_directory(cls, v: str | Path) -&gt; Path:\n    \"\"\"Validate that the output directory exists or can be created, and is writable.\"\"\"\n    return validate_directory(v, create=True)\n</code></pre>"},{"location":"reference/io/validators/","title":"Validators","text":""},{"location":"reference/io/validators/#imgtools.io.validators","title":"validators","text":"<p>Validators for the SampleInput and related classes.</p> <p>Functions:</p> Name Description <code>validate_directory</code> <p>Validate that the input directory exists and is readable.</p> <code>validate_modalities</code> <p>Validate that modalities are a list of strings.</p> <code>validate_n_jobs</code> <p>Validate that n_jobs is reasonable.</p>"},{"location":"reference/io/validators/#imgtools.io.validators.validate_directory","title":"validate_directory","text":"<pre><code>validate_directory(\n    v: str | pathlib.Path, create: bool = False\n) -&gt; pathlib.Path\n</code></pre> <p>Validate that the input directory exists and is readable.</p> Source code in <code>src/imgtools/io/validators.py</code> <pre><code>def validate_directory(v: str | Path, create: bool = False) -&gt; Path:\n    \"\"\"Validate that the input directory exists and is readable.\"\"\"\n    path = Path(v) if not isinstance(v, Path) else v\n\n    if not path.exists():\n        if create:\n            try:\n                path.mkdir(parents=True, exist_ok=True)\n            except (OSError, PermissionError) as e:\n                msg = f\"Failed to create directory: {path} ({e})\"\n                raise ValueError(msg) from e\n        else:\n            msg = f\"Directory does not exist: {path}\"\n            raise ValueError(msg)\n\n    if not path.is_dir():\n        msg = f\"Path must be a directory: {path}\"\n        raise ValueError(msg)\n\n    if not os.access(path, os.R_OK):\n        msg = f\"Directory is not readable {path}\"\n        raise ValueError(msg)\n\n    return path\n</code></pre>"},{"location":"reference/io/validators/#imgtools.io.validators.validate_modalities","title":"validate_modalities","text":"<pre><code>validate_modalities(\n    v: list[str] | None,\n) -&gt; list[str] | None\n</code></pre> <p>Validate that modalities are a list of strings.</p> Source code in <code>src/imgtools/io/validators.py</code> <pre><code>def validate_modalities(v: list[str] | None) -&gt; list[str] | None:\n    \"\"\"Validate that modalities are a list of strings.\"\"\"\n    if v is None:\n        return v\n\n    if not all(isinstance(m, str) for m in v):\n        raise ValueError(\"Modalities must be a list of strings\")\n\n    return v\n</code></pre>"},{"location":"reference/io/validators/#imgtools.io.validators.validate_n_jobs","title":"validate_n_jobs","text":"<pre><code>validate_n_jobs(v: int) -&gt; int\n</code></pre> <p>Validate that n_jobs is reasonable.</p> Source code in <code>src/imgtools/io/validators.py</code> <pre><code>def validate_n_jobs(v: int) -&gt; int:\n    \"\"\"Validate that n_jobs is reasonable.\"\"\"\n    cpu_count = multiprocessing.cpu_count()\n\n    if v &lt;= 0:\n        logger.warning(\"n_jobs must be positive, using default\")\n        return max(1, cpu_count - 2)\n\n    if v &gt; cpu_count:\n        logger.warning(\n            f\"n_jobs ({v}) exceeds available CPU count ({cpu_count}), \"\n            f\"setting to {cpu_count}\"\n        )\n        return cpu_count\n\n    return v\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/","title":"Abstract base writer","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer","title":"abstract_base_writer","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter","title":"AbstractBaseWriter  <code>dataclass</code>","text":"<pre><code>AbstractBaseWriter(\n    root_directory: pathlib.Path = dataclasses.field(),\n    filename_format: str = dataclasses.field(),\n    create_dirs: bool = True,\n    existing_file_mode: imgtools.io.writers.abstract_base_writer.ExistingFileMode = imgtools.io.writers.abstract_base_writer.ExistingFileMode.FAIL,\n    sanitize_filenames: bool = True,\n    context: typing.Dict[str, typing.Any] = dict(),\n    overwrite_index: bool = False,\n    absolute_paths_in_index: bool = False,\n    index_filename: typing.Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code>, <code>typing.Generic[imgtools.io.writers.abstract_base_writer.ContentType]</code></p> <p>Abstract base class for managing file writing with customizable paths and filenames.</p> <p>This class provides a template for writing files with a flexible directory structure and consistent file naming patterns. It handles common operations such as directory creation, file path resolution, and maintaining an index of saved files.</p> <p>The class supports various file existence handling modes, filename sanitization, and easy context management for generating dynamic paths with placeholder variables.</p> <p>Attributes:</p> Name Type Description <code>root_directory</code> <code>pathlib.Path</code> <p>Root directory where files will be saved. This directory will be created if it doesn't exist and <code>create_dirs</code> is True.</p> <code>filename_format</code> <code>str</code> <p>Format string defining the directory and filename structure. Supports placeholders for context variables enclosed in curly braces. Example: '{subject_id}_{date}/{disease}.txt'</p> <code>create_dirs</code> <code>bool, default=True</code> <p>Creates necessary directories if they don't exist.</p> <code>existing_file_mode</code> <code>ExistingFileMode, default=ExistingFileMode.FAIL</code> <p>Behavior when a file already exists. Options: OVERWRITE, SKIP, FAIL</p> <code>sanitize_filenames</code> <code>bool, default=True</code> <p>Replaces illegal characters from filenames with underscores.</p> <code>context</code> <code>Dict[str, Any], default={}</code> <p>Internal context storage for pre-checking.</p> <code>index_filename</code> <code>Optional[str], default=None</code> <p>Name of the index file to track saved files. If an absolute path is provided, it will be used as is. If not provided, it will be saved in the root directory with the format of {root_directory.name}_index.csv.</p> <code>overwrite_index</code> <code>bool, default=False</code> <p>Overwrites the index file if it already exists.</p> <code>absolute_paths_in_index</code> <code>bool, default=False</code> <p>If True, saves absolute paths in the index file. If False, saves paths relative to the root directory.</p> <code>pattern_resolver</code> <code>imgtools.pattern_parser.PatternResolver</code> <p>Instance used to handle filename formatting with placeholders.</p> Properties <p>index_file : Path     Returns the path to the index CSV file.</p> Notes <p>When using this class, consider the following best practices: 1. Implement the abstract <code>save</code> method in subclasses to handle the actual file writing. 2. Use the <code>preview_path</code> method to check if a file exists before performing expensive operations. 3. Use the class as a context manager when appropriate to ensure proper resource cleanup. 4. Set appropriate file existence handling mode based on your application's needs.</p> <p>Methods:</p> Name Description <code>add_to_index</code> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <code>clear_context</code> <p>Clear the context for the writer.</p> <code>preview_path</code> <p>Pre-checking file existence and setting up the writer context.</p> <code>resolve_path</code> <p>Generate a file path based on the filename format, subject ID, and</p> <code>save</code> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <code>set_context</code> <p>Set the context for the writer.</p>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.index_file","title":"index_file  <code>property</code>","text":"<pre><code>index_file: pathlib.Path\n</code></pre> <p>Get the path to the index CSV file.</p>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index","title":"add_to_index","text":"<pre><code>add_to_index(\n    path: pathlib.Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None\n</code></pre> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <p>What It Does:</p> <ul> <li>Logs the file's path and associated context variables to a     shared CSV index file.</li> <li>Uses IndexWriter to safely handle concurrent writes and schema evolution.</li> </ul> <p>When to Use It:</p> <ul> <li>Use this method to maintain a centralized record of saved files for auditing or debugging.</li> </ul> Relevant Writer Parameters <ul> <li> <p>The <code>index_filename</code> parameter allows you to specify a custom filename for the index file. By default, it will be named after the <code>root_directory</code> with <code>_index.csv</code> appended.</p> </li> <li> <p>If the index file already exists in the root directory, it will overwrite it unless the <code>overwrite_index</code> parameter is set to <code>False</code>.</p> </li> <li> <p>The <code>absolute_paths_in_index</code> parameter controls whether the paths in the index file are absolute or relative to the <code>root_directory</code>, with <code>False</code> being the default.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path being saved.</p> required <code>bool</code> <p>If True, write existing context variables passed into writer and the additional context to the CSV. If False, determines only the context keys parsed from the <code>filename_format</code> (excludes all other context variables, and unused context keys).</p> <code>True</code> <code>str</code> <p>The name of the column to store the file path. Defaults to \"path\".</p> <code>\"path\"</code> <code>bool</code> <p>If True, checks if the file path already exists in the index and replaces it.</p> <code>False</code> <code>bool</code> <p>If True, allows schema evolution by merging new columns with existing ones. Set to False for strict schema enforcement (will raise an error if schemas don't match).</p> <code>True</code> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def add_to_index(\n    self,\n    path: Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add or update an entry in the shared CSV index file using IndexWriter.\n\n    **What It Does**:\n\n    - Logs the file's path and associated context variables to a\n        shared CSV index file.\n    - Uses IndexWriter to safely handle concurrent writes and schema evolution.\n\n    **When to Use It**:\n\n    - Use this method to maintain a centralized record of saved\n    files for auditing or debugging.\n\n    **Relevant Writer Parameters**\n    ------------------------------\n\n    - The `index_filename` parameter allows you to specify a\n    custom filename for the index file.\n    By default, it will be named after the `root_directory`\n    with `_index.csv` appended.\n\n    - If the index file already exists in the root directory,\n    it will overwrite it unless\n    the `overwrite_index` parameter is set to `False`.\n\n    - The `absolute_paths_in_index` parameter controls whether\n    the paths in the index file are absolute or relative to the\n    `root_directory`, with `False` being the default.\n\n    Parameters\n    ----------\n    path : Path\n        The file path being saved.\n    include_all_context : bool, default=True\n        If True, write existing context variables passed into writer and\n        the additional context to the CSV.\n        If False, determines only the context keys parsed from the\n        `filename_format` (excludes all other context variables, and\n        unused context keys).\n    filepath_column : str, default=\"path\"\n        The name of the column to store the file path. Defaults to \"path\".\n    replace_existing : bool, default=False\n        If True, checks if the file path already exists in the index and\n        replaces it.\n    merge_columns : bool, default=True\n        If True, allows schema evolution by merging new columns with existing ones.\n        Set to False for strict schema enforcement (will raise an error if schemas don't match).\n    \"\"\"\n    # Prepare context data\n    context = {}\n\n    # Determine which context to include\n    if include_all_context:\n        context = self.context\n    else:\n        # Only include keys from the pattern resolver\n        context = {\n            k: v\n            for k, v in self.context.items()\n            if k in self.pattern_resolver.keys\n        }\n\n    # Resolve the path according to configuration\n    resolved_path = (\n        path.resolve().absolute()\n        if self.absolute_paths_in_index\n        else path.relative_to(self.root_directory)\n    )\n\n    # Write the entry to the index file\n    try:\n        self._index_writer.write_entry(\n            path=resolved_path,\n            context=context,\n            filepath_column=filepath_column,\n            replace_existing=replace_existing,\n            merge_columns=merge_columns,\n        )\n    except (\n        IndexSchemaMismatchError,\n        IndexReadError,\n        IndexWriteError,\n        IndexWriterError,\n    ) as e:\n        logger.exception(\n            f\"Error writing to index file {self.index_file}.\", error=e\n        )\n        raise WriterIndexError(\n            f\"Error writing to index file {self.index_file}.\",\n            writer=self,\n        ) from e\n    except Exception as general_e:\n        raise general_e\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index(path)","title":"<code>path</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index(include_all_context)","title":"<code>include_all_context</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.add_to_index(merge_columns)","title":"<code>merge_columns</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(\n    data: imgtools.io.writers.abstract_base_writer.ContentType,\n    **kwargs: typing.Any\n) -&gt; pathlib.Path\n</code></pre> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <p>Can use resolve_path() to get the output path and write the data to it.</p> <p>For efficiency, use self.context to access the context variables, updating them with the kwargs passed from the save method.</p> <p>This will help simplify repeated saves with similar context variables.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>@abstractmethod\ndef save(self, data: ContentType, **kwargs: Any) -&gt; Path:\n    \"\"\"\n    Abstract method for writing data. Must be implemented by subclasses.\n\n    Can use resolve_path() to get the output path and write the data to it.\n\n    For efficiency, use self.context to access the context variables,\n    updating them with the kwargs passed from the save method.\n\n    This will help simplify repeated saves with similar context variables.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.AbstractBaseWriter.set_context","title":"set_context","text":"<pre><code>set_context(**kwargs: object) -&gt; None\n</code></pre> <p>Set the context for the writer.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def set_context(self, **kwargs: object) -&gt; None:\n    \"\"\"\n    Set the context for the writer.\n    \"\"\"\n    self.context.update(kwargs)\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter","title":"ExampleWriter  <code>dataclass</code>","text":"<pre><code>ExampleWriter(\n    root_directory: pathlib.Path = dataclasses.field(),\n    filename_format: str = dataclasses.field(),\n    create_dirs: bool = True,\n    existing_file_mode: imgtools.io.writers.abstract_base_writer.ExistingFileMode = imgtools.io.writers.abstract_base_writer.ExistingFileMode.FAIL,\n    sanitize_filenames: bool = True,\n    context: typing.Dict[str, typing.Any] = dict(),\n    overwrite_index: bool = False,\n    absolute_paths_in_index: bool = False,\n    index_filename: typing.Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.abstract_base_writer.AbstractBaseWriter[str]</code></p> <p>A concrete implementation of AbstractBaseWriter for demonstration.</p> <p>Methods:</p> Name Description <code>add_to_index</code> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <code>clear_context</code> <p>Clear the context for the writer.</p> <code>preview_path</code> <p>Pre-checking file existence and setting up the writer context.</p> <code>resolve_path</code> <p>Generate a file path based on the filename format, subject ID, and</p> <code>save</code> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <code>set_context</code> <p>Set the context for the writer.</p>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.index_file","title":"index_file  <code>property</code>","text":"<pre><code>index_file: pathlib.Path\n</code></pre> <p>Get the path to the index CSV file.</p>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index","title":"add_to_index","text":"<pre><code>add_to_index(\n    path: pathlib.Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None\n</code></pre> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <p>What It Does:</p> <ul> <li>Logs the file's path and associated context variables to a     shared CSV index file.</li> <li>Uses IndexWriter to safely handle concurrent writes and schema evolution.</li> </ul> <p>When to Use It:</p> <ul> <li>Use this method to maintain a centralized record of saved files for auditing or debugging.</li> </ul> Relevant Writer Parameters <ul> <li> <p>The <code>index_filename</code> parameter allows you to specify a custom filename for the index file. By default, it will be named after the <code>root_directory</code> with <code>_index.csv</code> appended.</p> </li> <li> <p>If the index file already exists in the root directory, it will overwrite it unless the <code>overwrite_index</code> parameter is set to <code>False</code>.</p> </li> <li> <p>The <code>absolute_paths_in_index</code> parameter controls whether the paths in the index file are absolute or relative to the <code>root_directory</code>, with <code>False</code> being the default.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path being saved.</p> required <code>bool</code> <p>If True, write existing context variables passed into writer and the additional context to the CSV. If False, determines only the context keys parsed from the <code>filename_format</code> (excludes all other context variables, and unused context keys).</p> <code>True</code> <code>str</code> <p>The name of the column to store the file path. Defaults to \"path\".</p> <code>\"path\"</code> <code>bool</code> <p>If True, checks if the file path already exists in the index and replaces it.</p> <code>False</code> <code>bool</code> <p>If True, allows schema evolution by merging new columns with existing ones. Set to False for strict schema enforcement (will raise an error if schemas don't match).</p> <code>True</code> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def add_to_index(\n    self,\n    path: Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add or update an entry in the shared CSV index file using IndexWriter.\n\n    **What It Does**:\n\n    - Logs the file's path and associated context variables to a\n        shared CSV index file.\n    - Uses IndexWriter to safely handle concurrent writes and schema evolution.\n\n    **When to Use It**:\n\n    - Use this method to maintain a centralized record of saved\n    files for auditing or debugging.\n\n    **Relevant Writer Parameters**\n    ------------------------------\n\n    - The `index_filename` parameter allows you to specify a\n    custom filename for the index file.\n    By default, it will be named after the `root_directory`\n    with `_index.csv` appended.\n\n    - If the index file already exists in the root directory,\n    it will overwrite it unless\n    the `overwrite_index` parameter is set to `False`.\n\n    - The `absolute_paths_in_index` parameter controls whether\n    the paths in the index file are absolute or relative to the\n    `root_directory`, with `False` being the default.\n\n    Parameters\n    ----------\n    path : Path\n        The file path being saved.\n    include_all_context : bool, default=True\n        If True, write existing context variables passed into writer and\n        the additional context to the CSV.\n        If False, determines only the context keys parsed from the\n        `filename_format` (excludes all other context variables, and\n        unused context keys).\n    filepath_column : str, default=\"path\"\n        The name of the column to store the file path. Defaults to \"path\".\n    replace_existing : bool, default=False\n        If True, checks if the file path already exists in the index and\n        replaces it.\n    merge_columns : bool, default=True\n        If True, allows schema evolution by merging new columns with existing ones.\n        Set to False for strict schema enforcement (will raise an error if schemas don't match).\n    \"\"\"\n    # Prepare context data\n    context = {}\n\n    # Determine which context to include\n    if include_all_context:\n        context = self.context\n    else:\n        # Only include keys from the pattern resolver\n        context = {\n            k: v\n            for k, v in self.context.items()\n            if k in self.pattern_resolver.keys\n        }\n\n    # Resolve the path according to configuration\n    resolved_path = (\n        path.resolve().absolute()\n        if self.absolute_paths_in_index\n        else path.relative_to(self.root_directory)\n    )\n\n    # Write the entry to the index file\n    try:\n        self._index_writer.write_entry(\n            path=resolved_path,\n            context=context,\n            filepath_column=filepath_column,\n            replace_existing=replace_existing,\n            merge_columns=merge_columns,\n        )\n    except (\n        IndexSchemaMismatchError,\n        IndexReadError,\n        IndexWriteError,\n        IndexWriterError,\n    ) as e:\n        logger.exception(\n            f\"Error writing to index file {self.index_file}.\", error=e\n        )\n        raise WriterIndexError(\n            f\"Error writing to index file {self.index_file}.\",\n            writer=self,\n        ) from e\n    except Exception as general_e:\n        raise general_e\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index(path)","title":"<code>path</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index(include_all_context)","title":"<code>include_all_context</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.add_to_index(merge_columns)","title":"<code>merge_columns</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.save","title":"save","text":"<pre><code>save(data: str, **kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <p>Can use resolve_path() to get the output path and write the data to it.</p> <p>For efficiency, use self.context to access the context variables, updating them with the kwargs passed from the save method.</p> <p>This will help simplify repeated saves with similar context variables.</p> <p>Save content to a file with the resolved path.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The content to write to the file.</p> required <code>typing.Any</code> <p>Additional context for filename generation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The path to the saved file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def save(self, data: str, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Save content to a file with the resolved path.\n\n    Parameters\n    ----------\n    content : str\n        The content to write to the file.\n    **kwargs : Any\n        Additional context for filename generation.\n\n    Returns\n    -------\n    Path\n        The path to the saved file.\n    \"\"\"\n    # Resolve the output file path\n    output_path = self.resolve_path(**kwargs)\n\n    # Write content to the file\n    with output_path.open(mode=\"w\", encoding=\"utf-8\") as f:\n        f.write(data)\n\n    self.add_to_index(output_path, replace_existing=output_path.exists())\n\n    return output_path\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.save(content)","title":"<code>content</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.save(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExampleWriter.set_context","title":"set_context","text":"<pre><code>set_context(**kwargs: object) -&gt; None\n</code></pre> <p>Set the context for the writer.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def set_context(self, **kwargs: object) -&gt; None:\n    \"\"\"\n    Set the context for the writer.\n    \"\"\"\n    self.context.update(kwargs)\n</code></pre>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.ExistingFileMode","title":"ExistingFileMode","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enum to specify handling behavior for existing files.</p> <p>Attributes:</p> Name Type Description <code>OVERWRITE</code> <code>str</code> <p>Overwrite the existing file. Logs as debug and continues with the operation.</p> <code>FAIL</code> <code>str</code> <p>Fail the operation if the file exists. Logs as error and raises a FileExistsError.</p> <code>SKIP</code> <code>str</code> <p>Skip the operation if the file exists. Meant to be used for previewing the path before any expensive computation. <code>preview_path()</code> will return None if the file exists. <code>resolve_path()</code> will still return the path even if the file exists. The writer's <code>save</code> method should handle the file existence if set to SKIP.</p>"},{"location":"reference/io/writers/abstract_base_writer/#imgtools.io.writers.abstract_base_writer.WriterIndexError","title":"WriterIndexError","text":"<pre><code>WriterIndexError(\n    message: str,\n    writer: imgtools.io.writers.abstract_base_writer.AbstractBaseWriter,\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when a writer encounters an error while interacting with its index.</p> <p>This exception wraps the underlying IndexWriter exceptions to provide a clearer context about the writer that encountered the error.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    writer: AbstractBaseWriter,\n) -&gt; None:\n    self.writer = writer\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/io/writers/index_writer/","title":"Index writer","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer","title":"index_writer","text":"<p>Functions:</p> Name Description <code>generate_context</code> <p>Create fake metadata for the ith file.</p> <code>write_entry</code> <p>Each parallel worker writes a unique path and context to the index.</p>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexReadError","title":"IndexReadError","text":"<pre><code>IndexReadError(\n    index_path: pathlib.Path, original_exception: Exception\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.index_writer.IndexWriterError</code></p> <p>Raised when reading the index file fails unexpectedly.</p> <p>Use this when an exception occurs while attempting to read or parse the existing CSV file.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def __init__(\n    self, index_path: Path, original_exception: Exception\n) -&gt; None:\n    self.index_path = index_path\n    self.original_exception = original_exception\n    msg = f\"Failed to read index file '{index_path}': {original_exception}\"\n    super().__init__(msg)\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexSchemaMismatchError","title":"IndexSchemaMismatchError","text":"<pre><code>IndexSchemaMismatchError(\n    missing_fields: set[str], index_path: pathlib.Path\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.index_writer.IndexWriterError</code></p> <p>Raised when the index file schema is missing required fields and merging columns is disabled.</p> <p>Use this error to notify the caller that the existing index cannot accommodate the current row\u2019s structure and merging is not allowed.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def __init__(self, missing_fields: set[str], index_path: Path) -&gt; None:\n    self.missing_fields = missing_fields\n    self.index_path = index_path\n    msg = (\n        f\"Schema mismatch in index file '{index_path}'. \"\n        f\"Missing fields: {sorted(missing_fields)}. \"\n        \"Set merge_columns=True to allow schema evolution.\"\n    )\n    super().__init__(msg)\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriteError","title":"IndexWriteError","text":"<pre><code>IndexWriteError(\n    index_path: pathlib.Path, original_exception: Exception\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.index_writer.IndexWriterError</code></p> <p>Raised when writing to the index file fails unexpectedly.</p> <p>Use this when a CSV write operation fails during append or full rewrite of the index.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def __init__(\n    self, index_path: Path, original_exception: Exception\n) -&gt; None:\n    self.index_path = index_path\n    self.original_exception = original_exception\n    msg = f\"Failed to write to index file '{index_path}': {original_exception}\"\n    super().__init__(msg)\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter","title":"IndexWriter","text":"<pre><code>IndexWriter(\n    index_path: pathlib.Path,\n    lock_path: pathlib.Path | None = None,\n)\n</code></pre> <p>Handles safe and smart updates to a shared CSV file used as an index.</p> <p>This class manages writing entries to a CSV index while avoiding problems like file corruption (from two writers editing at once), column mismatches, or missing data.</p> <p>Think of this like a notebook where many writers might want to write down their output paths and metadata. This class is the referee: it waits for its turn (locking), makes sure the notebook has the right columns, and writes everything in order.</p> <p>index_path : Path     Path to the CSV file that acts as a shared index. lock_path : Path | None, optional     Path to a <code>.lock</code> file that ensures one writer updates at a time.         If None, uses the index file path with <code>.lock</code> added.</p> <p>Methods:</p> Name Description <code>write_entry</code> <p>Write one entry to the index file. Safe in parallel with full lock.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def __init__(\n    self, index_path: Path, lock_path: Path | None = None\n) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    index_path : Path\n        Path to the CSV file that acts as a shared index.\n    lock_path : Path | None, optional\n        Path to a `.lock` file that ensures one writer updates at a time.\n            If None, uses the index file path with `.lock` added.\n    \"\"\"\n    self.index_path: Path = index_path\n    self.lock_path: Path = lock_path or index_path.with_suffix(\n        index_path.suffix + \".lock\"\n    )\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry","title":"write_entry","text":"<pre><code>write_entry(\n    path: pathlib.Path,\n    context: dict[str, typing.Any],\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None\n</code></pre> <p>Write one entry to the index file. Safe in parallel with full lock.</p> <p>You give this a path and a dictionary of info. \u2192 It checks the index file. \u2192 If the path is already in there and you want to replace it, it does. \u2192 If your new info has different keys, it adds new columns (if allowed). \u2192 Then it saves the full table back to disk, safely.</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path that you want to record in the index.</p> required <code>dict[str, typing.Any]</code> <p>Extra metadata (e.g. subject ID, date, label) to log alongside the path.</p> required <code>str</code> <p>Name of the column to store the file path. Default is \"path\".</p> <code>\"path\"</code> <code>bool</code> <p>If True, update the row if one with the same path already exists.</p> <code>False</code> <code>bool</code> <p>If True, automatically add new columns if the context has fields the     CSV didn't have yet.</p> <code>True</code> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def write_entry(\n    self,\n    path: Path,\n    context: dict[str, Any],\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None:\n    \"\"\"Write one entry to the index file. Safe in parallel with full lock.\n\n    You give this a path and a dictionary of info.\n    \u2192 It checks the index file.\n    \u2192 If the path is already in there and you want to replace it, it does.\n    \u2192 If your new info has different keys, it adds new columns (if allowed).\n    \u2192 Then it saves the full table back to disk, safely.\n\n    Parameters\n    ----------\n    path : Path\n        The file path that you want to record in the index.\n    context : dict[str, Any]\n        Extra metadata (e.g. subject ID, date, label) to log alongside the path.\n    filepath_column : str, default=\"path\"\n        Name of the column to store the file path. Default is \"path\".\n    replace_existing : bool, default=False\n        If True, update the row if one with the same path already exists.\n    merge_columns : bool, default=True\n        If True, automatically add new columns if the context has fields the\n            CSV didn't have yet.\n\n    Raises\n    ------\n    IndexSchemaMismatchError\n        If the new entry's schema doesn't match the existing one and\n        merging is not allowed.\n    IndexReadError\n        If there are issues reading the existing index file.\n    IndexWriteError\n        If there are issues writing to the index file.\n    \"\"\"\n    entry = {\n        filepath_column: str(path),\n        **{k: str(v) for k, v in context.items()},\n    }\n\n    with InterProcessLock(self.lock_path):\n        try:\n            existing_rows, existing_fieldnames = self._read_existing_rows(\n                filepath_column, replace_existing, entry\n            )\n        except OSError as e:\n            raise IndexReadError(self.index_path, e) from e\n\n        try:\n            final_fieldnames = self._validate_or_merge_schema(\n                existing_fieldnames, set(entry.keys()), merge_columns\n            )\n        except IndexSchemaMismatchError:\n            raise\n\n        all_rows = self._normalize_rows(\n            existing_rows + [entry], final_fieldnames\n        )\n\n        try:\n            self._write_rows(all_rows, final_fieldnames)\n        except Exception as e:\n            raise IndexWriteError(self.index_path, e) from e\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry(path)","title":"<code>path</code>","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry(context)","title":"<code>context</code>","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriter.write_entry(merge_columns)","title":"<code>merge_columns</code>","text":""},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.IndexWriterError","title":"IndexWriterError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all IndexWriter-related errors.</p> <p>This should be used to catch any general IndexWriter failure that does not fall under a more specific error type.</p>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.generate_context","title":"generate_context","text":"<pre><code>generate_context(i: int) -&gt; dict[str, typing.Any]\n</code></pre> <p>Create fake metadata for the ith file.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def generate_context(i: int) -&gt; dict[str, Any]:\n    \"\"\"Create fake metadata for the ith file.\"\"\"\n    return {\n        \"subject_id\": f\"subject_{i % 10}\",\n        \"modality\": random.choice([\"CT\", \"MR\", \"SEG\"]),\n        \"timestamp\": datetime.now().isoformat(),\n        \"quality_score\": round(random.uniform(0, 1), 3),\n    }\n</code></pre>"},{"location":"reference/io/writers/index_writer/#imgtools.io.writers.index_writer.write_entry","title":"write_entry","text":"<pre><code>write_entry(i: int) -&gt; None\n</code></pre> <p>Each parallel worker writes a unique path and context to the index.</p> Source code in <code>src/imgtools/io/writers/index_writer.py</code> <pre><code>def write_entry(i: int) -&gt; None:\n    \"\"\"Each parallel worker writes a unique path and context to the index.\"\"\"\n    output_path = Path(f\"output/fake_file_{i}.nii.gz\")\n    context = generate_context(i)\n\n    # Use a new IndexWriter per process to avoid shared state\n    local_writer = IndexWriter(index_path=INDEX_PATH)\n    local_writer.write_entry(path=output_path, context=context)\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/","title":"Nifti writer","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer","title":"nifti_writer","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter","title":"NIFTIWriter  <code>dataclass</code>","text":"<pre><code>NIFTIWriter(\n    root_directory: pathlib.Path = dataclasses.field(),\n    filename_format: str = dataclasses.field(),\n    create_dirs: bool = True,\n    existing_file_mode: imgtools.io.writers.abstract_base_writer.ExistingFileMode = imgtools.io.writers.abstract_base_writer.ExistingFileMode.FAIL,\n    sanitize_filenames: bool = True,\n    context: typing.Dict[str, typing.Any] = dict(),\n    overwrite_index: bool = False,\n    absolute_paths_in_index: bool = False,\n    index_filename: typing.Optional[str] = None,\n    compression_level: int = 9,\n    truncate_uids_in_filename: int = 8,\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.abstract_base_writer.AbstractBaseWriter[SimpleITK.Image | numpy.ndarray]</code></p> <p>Class for managing file writing with customizable paths and filenames for NIFTI files.</p> <p>This class extends the AbstractBaseWriter to provide specialized functionality for writing NIFTI image files. It supports both SimpleITK Image objects and numpy arrays as input data types.</p> <p>Attributes:</p> Name Type Description <code>compression_level</code> <code>int, default=9</code> <p>Compression level (0-9). Higher means better compression but slower writing. Value must be between MIN_COMPRESSION_LEVEL (0) and MAX_COMPRESSION_LEVEL (9).</p> <code>truncate_uids_in_filename</code> <code>int, default=8</code> <p>Many DICOM files have long UIDs in the filename. If used in the filename format, this will truncate the UID to the last <code>truncate_uids_in_filename</code> characters. A value of 0 means no truncation.</p> <code>VALID_EXTENSIONS</code> <code>typing.ClassVar[list[str]]</code> <p>List of valid file extensions for NIFTI files (\".nii\", \".nii.gz\").</p> <code>MAX_COMPRESSION_LEVEL</code> <code>typing.ClassVar[int]</code> <p>Maximum allowed compression level (9).</p> <code>MIN_COMPRESSION_LEVEL</code> <code>typing.ClassVar[int]</code> <p>Minimum allowed compression level (0).</p> Inherited Attributes <p>root_directory : Path     Root directory where files will be saved. This directory will be created     if it doesn't exist and <code>create_dirs</code> is True. filename_format : str     Format string defining the directory and filename structure.     Supports placeholders for context variables enclosed in curly braces.     Example: '{subject_id}_{date}/{disease}.nii.gz' create_dirs : bool, default=True     Creates necessary directories if they don't exist. existing_file_mode : ExistingFileMode, default=ExistingFileMode.FAIL     Behavior when a file already exists.     Options: OVERWRITE, SKIP, FAIL sanitize_filenames : bool, default=True     Replaces illegal characters from filenames with underscores. context : Dict[str, Any], default={}     Internal context storage for pre-checking. index_filename : Optional[str], default=None     Name of the index file to track saved files.     If an absolute path is provided, it will be used as is.     If not provided, it will be saved in the root directory with the format     of {root_directory.name}_index.csv. overwrite_index : bool, default=False     Overwrites the index file if it already exists. absolute_paths_in_index : bool, default=False     If True, saves absolute paths in the index file.     If False, saves paths relative to the root directory. pattern_resolver : PatternResolver     Instance used to handle filename formatting with placeholders.</p> Notes <p>When using this class, ensure your filename_format ends with one of the VALID_EXTENSIONS. The class validates the compression level and filename format during initialization.</p> <p>Methods:</p> Name Description <code>add_to_index</code> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <code>clear_context</code> <p>Clear the context for the writer.</p> <code>preview_path</code> <p>Pre-checking file existence and setting up the writer context.</p> <code>resolve_path</code> <p>Generate a file path based on the filename format, subject ID, and</p> <code>save</code> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <code>set_context</code> <p>Set the context for the writer.</p>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.index_file","title":"index_file  <code>property</code>","text":"<pre><code>index_file: pathlib.Path\n</code></pre> <p>Get the path to the index CSV file.</p>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index","title":"add_to_index","text":"<pre><code>add_to_index(\n    path: pathlib.Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None\n</code></pre> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <p>What It Does:</p> <ul> <li>Logs the file's path and associated context variables to a     shared CSV index file.</li> <li>Uses IndexWriter to safely handle concurrent writes and schema evolution.</li> </ul> <p>When to Use It:</p> <ul> <li>Use this method to maintain a centralized record of saved files for auditing or debugging.</li> </ul> Relevant Writer Parameters <ul> <li> <p>The <code>index_filename</code> parameter allows you to specify a custom filename for the index file. By default, it will be named after the <code>root_directory</code> with <code>_index.csv</code> appended.</p> </li> <li> <p>If the index file already exists in the root directory, it will overwrite it unless the <code>overwrite_index</code> parameter is set to <code>False</code>.</p> </li> <li> <p>The <code>absolute_paths_in_index</code> parameter controls whether the paths in the index file are absolute or relative to the <code>root_directory</code>, with <code>False</code> being the default.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path being saved.</p> required <code>bool</code> <p>If True, write existing context variables passed into writer and the additional context to the CSV. If False, determines only the context keys parsed from the <code>filename_format</code> (excludes all other context variables, and unused context keys).</p> <code>True</code> <code>str</code> <p>The name of the column to store the file path. Defaults to \"path\".</p> <code>\"path\"</code> <code>bool</code> <p>If True, checks if the file path already exists in the index and replaces it.</p> <code>False</code> <code>bool</code> <p>If True, allows schema evolution by merging new columns with existing ones. Set to False for strict schema enforcement (will raise an error if schemas don't match).</p> <code>True</code> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def add_to_index(\n    self,\n    path: Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add or update an entry in the shared CSV index file using IndexWriter.\n\n    **What It Does**:\n\n    - Logs the file's path and associated context variables to a\n        shared CSV index file.\n    - Uses IndexWriter to safely handle concurrent writes and schema evolution.\n\n    **When to Use It**:\n\n    - Use this method to maintain a centralized record of saved\n    files for auditing or debugging.\n\n    **Relevant Writer Parameters**\n    ------------------------------\n\n    - The `index_filename` parameter allows you to specify a\n    custom filename for the index file.\n    By default, it will be named after the `root_directory`\n    with `_index.csv` appended.\n\n    - If the index file already exists in the root directory,\n    it will overwrite it unless\n    the `overwrite_index` parameter is set to `False`.\n\n    - The `absolute_paths_in_index` parameter controls whether\n    the paths in the index file are absolute or relative to the\n    `root_directory`, with `False` being the default.\n\n    Parameters\n    ----------\n    path : Path\n        The file path being saved.\n    include_all_context : bool, default=True\n        If True, write existing context variables passed into writer and\n        the additional context to the CSV.\n        If False, determines only the context keys parsed from the\n        `filename_format` (excludes all other context variables, and\n        unused context keys).\n    filepath_column : str, default=\"path\"\n        The name of the column to store the file path. Defaults to \"path\".\n    replace_existing : bool, default=False\n        If True, checks if the file path already exists in the index and\n        replaces it.\n    merge_columns : bool, default=True\n        If True, allows schema evolution by merging new columns with existing ones.\n        Set to False for strict schema enforcement (will raise an error if schemas don't match).\n    \"\"\"\n    # Prepare context data\n    context = {}\n\n    # Determine which context to include\n    if include_all_context:\n        context = self.context\n    else:\n        # Only include keys from the pattern resolver\n        context = {\n            k: v\n            for k, v in self.context.items()\n            if k in self.pattern_resolver.keys\n        }\n\n    # Resolve the path according to configuration\n    resolved_path = (\n        path.resolve().absolute()\n        if self.absolute_paths_in_index\n        else path.relative_to(self.root_directory)\n    )\n\n    # Write the entry to the index file\n    try:\n        self._index_writer.write_entry(\n            path=resolved_path,\n            context=context,\n            filepath_column=filepath_column,\n            replace_existing=replace_existing,\n            merge_columns=merge_columns,\n        )\n    except (\n        IndexSchemaMismatchError,\n        IndexReadError,\n        IndexWriteError,\n        IndexWriterError,\n    ) as e:\n        logger.exception(\n            f\"Error writing to index file {self.index_file}.\", error=e\n        )\n        raise WriterIndexError(\n            f\"Error writing to index file {self.index_file}.\",\n            writer=self,\n        ) from e\n    except Exception as general_e:\n        raise general_e\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index(path)","title":"<code>path</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index(include_all_context)","title":"<code>include_all_context</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.add_to_index(merge_columns)","title":"<code>merge_columns</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.save","title":"save","text":"<pre><code>save(\n    data: SimpleITK.Image | numpy.ndarray, **kwargs: object\n) -&gt; pathlib.Path\n</code></pre> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <p>Can use resolve_path() to get the output path and write the data to it.</p> <p>For efficiency, use self.context to access the context variables, updating them with the kwargs passed from the save method.</p> <p>This will help simplify repeated saves with similar context variables.</p> <p>Write the SimpleITK image to a NIFTI file.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image | numpy.ndarray</code> <p>The SimpleITK image or numpy array to save</p> required <code>object</code> <p>Additional formatting parameters for the output path</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Path to the saved file</p> Source code in <code>src/imgtools/io/writers/nifti_writer.py</code> <pre><code>def save(self, data: sitk.Image | np.ndarray, **kwargs: object) -&gt; Path:\n    \"\"\"Write the SimpleITK image to a NIFTI file.\n\n    Parameters\n    ----------\n    data : sitk.Image | np.ndarray\n        The SimpleITK image or numpy array to save\n    **kwargs : object\n        Additional formatting parameters for the output path\n\n    Returns\n    -------\n    Path\n        Path to the saved file\n\n    Raises\n    ------\n    NiftiWriterIOError\n        If writing fails\n    NiftiWriterValidationError\n        If the input data is invalid\n    \"\"\"\n    match data:\n        case sitk.Image():\n            image = data\n        case np.ndarray():\n            image = sitk.GetImageFromArray(data)\n        case _:\n            msg = \"Input must be a SimpleITK Image or a numpy array\"\n            raise NiftiWriterValidationError(msg)\n\n    # if the object has the 'fingerprint' property, update the context\n    if hasattr(data, \"serialized_fingerprint\"):\n        self.set_context(**data.serialized_fingerprint)\n    elif isinstance(data, MedImage):\n        # if there is no fingerprint, this is unexpected\n        # this is an issue now since we auto keep context between\n        # saves, so this could lead to using the fingerprint\n        # of the previous save\n        logger.error(\n            \"No fingerprint found in the writer object. \"\n            \"This is unexpected and may indicate a bug. \"\n            \"The fingerprint fields in the index might be incorrect.\"\n        )\n        # TODO:: think of a better way to handle this\n        self.clear_context()\n\n    # TODO:: think of a better way to handle the truncate_uids_in_filename\n    if self.truncate_uids_in_filename:\n        truncated_kwargs = {\n            k: truncate_uid(str(v), self.truncate_uids_in_filename)\n            if k.lower().endswith(\"uid\")\n            else v\n            for k, v in kwargs.items()\n        }\n        out_path = self.resolve_path(**truncated_kwargs)\n        # need to update the context with the old kwargs\n        # because it will be used in the index, and we dont want\n        # to truncate the UIDs in the index\n        self.set_context(**kwargs)\n    else:\n        out_path = self.resolve_path(**kwargs)\n\n    if (\n        out_path.exists()  # check if it exists\n        # This will only be true if SKIP,\n        # OVERWRITE would have deleted the file\n        and self.existing_file_mode == ExistingFileMode.SKIP\n    ):\n        logger.debug(\"File exists, skipping.\", out_path=out_path)\n        return out_path\n\n    try:\n        sitk.WriteImage(\n            image,\n            out_path.as_posix(),\n            useCompression=True,\n            compressionLevel=self.compression_level,\n        )\n    except Exception as e:\n        msg = f\"Error writing image to file {out_path}: {e}\"\n        raise NiftiWriterIOError(msg) from e\n\n    self.add_to_index(\n        out_path,\n        filepath_column=\"filepath\",\n        replace_existing=out_path.exists(),\n    )\n\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.save(data)","title":"<code>data</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.save(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NIFTIWriter.set_context","title":"set_context","text":"<pre><code>set_context(**kwargs: object) -&gt; None\n</code></pre> <p>Set the context for the writer.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def set_context(self, **kwargs: object) -&gt; None:\n    \"\"\"\n    Set the context for the writer.\n    \"\"\"\n    self.context.update(kwargs)\n</code></pre>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NiftiWriterError","title":"NiftiWriterError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for NiftiWriter errors.</p>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NiftiWriterIOError","title":"NiftiWriterIOError","text":"<p>               Bases: <code>imgtools.io.writers.nifti_writer.NiftiWriterError</code></p> <p>Raised when I/O operations fail.</p>"},{"location":"reference/io/writers/nifti_writer/#imgtools.io.writers.nifti_writer.NiftiWriterValidationError","title":"NiftiWriterValidationError","text":"<p>               Bases: <code>imgtools.io.writers.nifti_writer.NiftiWriterError</code></p> <p>Raised when validation of writer configuration fails.</p>"},{"location":"reference/io/writers/numpy_writer/","title":"Numpy writer","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer","title":"numpy_writer","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter","title":"NumPyWriter  <code>dataclass</code>","text":"<pre><code>NumPyWriter(\n    root_directory: pathlib.Path = dataclasses.field(),\n    filename_format: str = dataclasses.field(),\n    create_dirs: bool = True,\n    existing_file_mode: imgtools.io.writers.abstract_base_writer.ExistingFileMode = imgtools.io.writers.abstract_base_writer.ExistingFileMode.FAIL,\n    sanitize_filenames: bool = True,\n    context: typing.Dict[str, typing.Any] = dict(),\n    overwrite_index: bool = False,\n    absolute_paths_in_index: bool = False,\n    index_filename: typing.Optional[str] = None,\n    compressed: bool = True,\n)\n</code></pre> <p>               Bases: <code>imgtools.io.writers.abstract_base_writer.AbstractBaseWriter[numpy.ndarray | SimpleITK.Image | dict[str, numpy.ndarray | SimpleITK.Image]]</code></p> <p>Write data to NumPy files with metadata support for SimpleITK images.</p> <p>This writer supports saving: - A single NumPy array or SimpleITK image with metadata. - Multiple arrays or images as a dictionary of key-value pairs.</p> <p>Methods:</p> Name Description <code>add_to_index</code> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <code>clear_context</code> <p>Clear the context for the writer.</p> <code>preview_path</code> <p>Pre-checking file existence and setting up the writer context.</p> <code>resolve_path</code> <p>Generate a file path based on the filename format, subject ID, and</p> <code>save</code> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <code>set_context</code> <p>Set the context for the writer.</p>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.index_file","title":"index_file  <code>property</code>","text":"<pre><code>index_file: pathlib.Path\n</code></pre> <p>Get the path to the index CSV file.</p>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index","title":"add_to_index","text":"<pre><code>add_to_index(\n    path: pathlib.Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None\n</code></pre> <p>Add or update an entry in the shared CSV index file using IndexWriter.</p> <p>What It Does:</p> <ul> <li>Logs the file's path and associated context variables to a     shared CSV index file.</li> <li>Uses IndexWriter to safely handle concurrent writes and schema evolution.</li> </ul> <p>When to Use It:</p> <ul> <li>Use this method to maintain a centralized record of saved files for auditing or debugging.</li> </ul> Relevant Writer Parameters <ul> <li> <p>The <code>index_filename</code> parameter allows you to specify a custom filename for the index file. By default, it will be named after the <code>root_directory</code> with <code>_index.csv</code> appended.</p> </li> <li> <p>If the index file already exists in the root directory, it will overwrite it unless the <code>overwrite_index</code> parameter is set to <code>False</code>.</p> </li> <li> <p>The <code>absolute_paths_in_index</code> parameter controls whether the paths in the index file are absolute or relative to the <code>root_directory</code>, with <code>False</code> being the default.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path being saved.</p> required <code>bool</code> <p>If True, write existing context variables passed into writer and the additional context to the CSV. If False, determines only the context keys parsed from the <code>filename_format</code> (excludes all other context variables, and unused context keys).</p> <code>True</code> <code>str</code> <p>The name of the column to store the file path. Defaults to \"path\".</p> <code>\"path\"</code> <code>bool</code> <p>If True, checks if the file path already exists in the index and replaces it.</p> <code>False</code> <code>bool</code> <p>If True, allows schema evolution by merging new columns with existing ones. Set to False for strict schema enforcement (will raise an error if schemas don't match).</p> <code>True</code> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def add_to_index(\n    self,\n    path: Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    merge_columns: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add or update an entry in the shared CSV index file using IndexWriter.\n\n    **What It Does**:\n\n    - Logs the file's path and associated context variables to a\n        shared CSV index file.\n    - Uses IndexWriter to safely handle concurrent writes and schema evolution.\n\n    **When to Use It**:\n\n    - Use this method to maintain a centralized record of saved\n    files for auditing or debugging.\n\n    **Relevant Writer Parameters**\n    ------------------------------\n\n    - The `index_filename` parameter allows you to specify a\n    custom filename for the index file.\n    By default, it will be named after the `root_directory`\n    with `_index.csv` appended.\n\n    - If the index file already exists in the root directory,\n    it will overwrite it unless\n    the `overwrite_index` parameter is set to `False`.\n\n    - The `absolute_paths_in_index` parameter controls whether\n    the paths in the index file are absolute or relative to the\n    `root_directory`, with `False` being the default.\n\n    Parameters\n    ----------\n    path : Path\n        The file path being saved.\n    include_all_context : bool, default=True\n        If True, write existing context variables passed into writer and\n        the additional context to the CSV.\n        If False, determines only the context keys parsed from the\n        `filename_format` (excludes all other context variables, and\n        unused context keys).\n    filepath_column : str, default=\"path\"\n        The name of the column to store the file path. Defaults to \"path\".\n    replace_existing : bool, default=False\n        If True, checks if the file path already exists in the index and\n        replaces it.\n    merge_columns : bool, default=True\n        If True, allows schema evolution by merging new columns with existing ones.\n        Set to False for strict schema enforcement (will raise an error if schemas don't match).\n    \"\"\"\n    # Prepare context data\n    context = {}\n\n    # Determine which context to include\n    if include_all_context:\n        context = self.context\n    else:\n        # Only include keys from the pattern resolver\n        context = {\n            k: v\n            for k, v in self.context.items()\n            if k in self.pattern_resolver.keys\n        }\n\n    # Resolve the path according to configuration\n    resolved_path = (\n        path.resolve().absolute()\n        if self.absolute_paths_in_index\n        else path.relative_to(self.root_directory)\n    )\n\n    # Write the entry to the index file\n    try:\n        self._index_writer.write_entry(\n            path=resolved_path,\n            context=context,\n            filepath_column=filepath_column,\n            replace_existing=replace_existing,\n            merge_columns=merge_columns,\n        )\n    except (\n        IndexSchemaMismatchError,\n        IndexReadError,\n        IndexWriteError,\n        IndexWriterError,\n    ) as e:\n        logger.exception(\n            f\"Error writing to index file {self.index_file}.\", error=e\n        )\n        raise WriterIndexError(\n            f\"Error writing to index file {self.index_file}.\",\n            writer=self,\n        ) from e\n    except Exception as general_e:\n        raise general_e\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index(path)","title":"<code>path</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index(include_all_context)","title":"<code>include_all_context</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.add_to_index(merge_columns)","title":"<code>merge_columns</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.save","title":"save","text":"<pre><code>save(\n    data: (\n        numpy.ndarray\n        | SimpleITK.Image\n        | dict[str, numpy.ndarray | SimpleITK.Image]\n    ),\n    **kwargs: object\n) -&gt; pathlib.Path\n</code></pre> <p>Abstract method for writing data. Must be implemented by subclasses.</p> <p>Can use resolve_path() to get the output path and write the data to it.</p> <p>For efficiency, use self.context to access the context variables, updating them with the kwargs passed from the save method.</p> <p>This will help simplify repeated saves with similar context variables.</p> <p>Save data to a NumPy file with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>numpy.ndarray | SimpleITK.Image | dict[str, numpy.ndarray | SimpleITK.Image]</code> <p>The data to save. Can be a single image or a dictionary of images.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>The path to the saved file.</p> Source code in <code>src/imgtools/io/writers/numpy_writer.py</code> <pre><code>def save(\n    self,\n    data: np.ndarray | sitk.Image | dict[str, np.ndarray | sitk.Image],\n    **kwargs: object,\n) -&gt; Path:\n    \"\"\"Save data to a NumPy file with optional metadata.\n\n    Parameters\n    ----------\n    data : np.ndarray | sitk.Image | dict[str, np.ndarray | sitk.Image]\n        The data to save. Can be a single image or a dictionary of images.\n\n    Returns\n    -------\n    Path\n        The path to the saved file.\n\n    Raises\n    ------\n    NumpyWriterValidationError\n        If the input data is invalid or unsupported.\n    \"\"\"\n    out_path = self.resolve_path(**kwargs)\n\n    if isinstance(data, (np.ndarray, sitk.Image)):\n        # Single image or array\n        array, metadata = self._to_numpy(data)\n        np.savez_compressed(out_path, image_array=array, **metadata)\n    elif isinstance(data, dict):\n        # Multiple images or arrays\n        arrays = {}\n        metadata = {}\n        for key, value in data.items():\n            array, meta = self._to_numpy(value)\n            arrays[key] = array\n            for meta_key, meta_value in meta.items():\n                metadata[f\"{key}_{meta_key}\"] = meta_value\n        if self.compressed:\n            np.savez_compressed(\n                out_path, allow_pickle=False, **arrays, **metadata\n            )\n        else:\n            np.savez(out_path, allow_pickle=False, **arrays, **metadata)\n    else:\n        raise NumpyWriterValidationError(\n            \"Data must be a NumPy array, SimpleITK image, or a dictionary of these types.\"\n        )\n\n    self.add_to_index(\n        out_path,\n        include_all_context=True,\n        filepath_column=\"path\",\n        replace_existing=True,\n    )\n    return out_path\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.save(data)","title":"<code>data</code>","text":""},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumPyWriter.set_context","title":"set_context","text":"<pre><code>set_context(**kwargs: object) -&gt; None\n</code></pre> <p>Set the context for the writer.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def set_context(self, **kwargs: object) -&gt; None:\n    \"\"\"\n    Set the context for the writer.\n    \"\"\"\n    self.context.update(kwargs)\n</code></pre>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumpyWriterError","title":"NumpyWriterError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for NumpyWriter errors.</p>"},{"location":"reference/io/writers/numpy_writer/#imgtools.io.writers.numpy_writer.NumpyWriterValidationError","title":"NumpyWriterValidationError","text":"<p>               Bases: <code>imgtools.io.writers.numpy_writer.NumpyWriterError</code></p> <p>Raised when validation of writer configuration fails.</p>"},{"location":"reference/loggers/json_logging/","title":"Json logging","text":""},{"location":"reference/loggers/json_logging/#imgtools.loggers.json_logging","title":"json_logging","text":""},{"location":"reference/loggers/logging_config/","title":"Logging config","text":""},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config","title":"logging_config","text":""},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config.LoggingManager","title":"LoggingManager","text":"<pre><code>LoggingManager(\n    name: str, base_dir: pathlib.Path | None = None\n)\n</code></pre> <p>Manages the configuration and initialization of a structured logger.</p> <p>This class provides flexible options for configuring log levels, formats, and output destinations.</p> <p>Examples:</p> <p>Initialize with default settings:     &gt;&gt;&gt; manager = LoggingManager(name=\"mypackage\")     &gt;&gt;&gt; logger = manager.get_logger()     &gt;&gt;&gt; logger.info(\"Info message\")</p> <p>Methods:</p> Name Description <code>configure_logging</code> <p>Dynamically adjust logging settings.</p> <code>get_logger</code> <p>Retrieve the logger instance.</p> Source code in <code>src/imgtools/loggers/logging_config.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    base_dir: Path | None = None,\n) -&gt; None:\n    self.name = name\n    self.base_dir = base_dir or Path.cwd()\n    self.level = self.env_level\n    self.enable_json_logging = (\n        os.environ.get(f\"{self.name}_enable_json_logging\".upper(), \"0\")\n        == \"1\"\n    )\n    self._initialize_logger()\n</code></pre>"},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config.LoggingManager.base_logging_config","title":"base_logging_config  <code>property</code>","text":"<pre><code>base_logging_config: typing.Dict\n</code></pre> <p>Create the basic logging configuration settings.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Base logging configuration.</p>"},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config.LoggingManager.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(\n    level: str = imgtools.loggers.logging_config.DEFAULT_LOG_LEVEL,\n) -&gt; structlog.stdlib.BoundLogger\n</code></pre> <p>Dynamically adjust logging settings.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Set the log level.</p> <code>imgtools.loggers.logging_config.DEFAULT_LOG_LEVEL</code> <p>Returns:</p> Type Description <code>structlog.stdlib.BoundLogger</code> <p>Updated logger instance.</p> Source code in <code>src/imgtools/loggers/logging_config.py</code> <pre><code>def configure_logging(\n    self, level: str = DEFAULT_LOG_LEVEL\n) -&gt; structlog.stdlib.BoundLogger:\n    \"\"\"\n    Dynamically adjust logging settings.\n\n    Parameters\n    ----------\n    level : str, optional\n        Set the log level.\n\n    Returns\n    -------\n    structlog.stdlib.BoundLogger\n        Updated logger instance.\n\n    Raises\n    ------\n    ValueError\n        If an invalid log level is specified.\n    \"\"\"\n    level_upper = level.upper()\n    if level_upper not in VALID_LOG_LEVELS:\n        msg = f\"Invalid logging level: {level}\"\n        raise ValueError(msg)\n\n    # Store the old level for logging the change\n    self.level = level_upper\n\n    self._initialize_logger()\n    logger = self.get_logger()\n\n    return logger\n</code></pre>"},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config.LoggingManager.configure_logging(level)","title":"<code>level</code>","text":""},{"location":"reference/loggers/logging_config/#imgtools.loggers.logging_config.LoggingManager.get_logger","title":"get_logger","text":"<pre><code>get_logger() -&gt; structlog.stdlib.BoundLogger\n</code></pre> <p>Retrieve the logger instance.</p> <p>Returns:</p> Type Description <code>structlog.stdlib.BoundLogger</code> <p>Configured logger instance.</p> Source code in <code>src/imgtools/loggers/logging_config.py</code> <pre><code>def get_logger(self) -&gt; structlog.stdlib.BoundLogger:\n    \"\"\"\n    Retrieve the logger instance.\n\n    Returns\n    -------\n    structlog.stdlib.BoundLogger\n        Configured logger instance.\n    \"\"\"\n    return structlog.get_logger(self.name)\n</code></pre>"},{"location":"reference/loggers/processors/","title":"Processors","text":""},{"location":"reference/loggers/processors/#imgtools.loggers.processors","title":"processors","text":""},{"location":"reference/loggers/processors/#imgtools.loggers.processors.CallPrettifier","title":"CallPrettifier","text":"<pre><code>CallPrettifier(concise: bool = True)\n</code></pre> <p>A processor to format call information in the event dictionary.</p> <p>Args:     concise (bool): Whether to use a concise format for call information. Defaults to True.</p> Source code in <code>src/imgtools/loggers/processors.py</code> <pre><code>def __init__(self, concise: bool = True) -&gt; None:\n    self.concise = concise\n</code></pre>"},{"location":"reference/loggers/processors/#imgtools.loggers.processors.ESTTimeStamper","title":"ESTTimeStamper","text":"<pre><code>ESTTimeStamper(fmt: str = '%Y-%m-%dT%H:%M:%S%z')\n</code></pre> <p>A processor to add a timestamp in Eastern Standard Time to the event dictionary.</p> Example <p>est_stamper = ESTTimeStamper() event_dict = {} est_stamper(None, None, event_dict)</p> <p>format for just time:</p> <p>est_stamper = ESTTimeStamper(fmt=\"%H:%M:%S\")</p> Source code in <code>src/imgtools/loggers/processors.py</code> <pre><code>def __init__(self, fmt: str = \"%Y-%m-%dT%H:%M:%S%z\") -&gt; None:\n    self.fmt = fmt\n    self.est = pytz.timezone(\"US/Eastern\")\n    self._last_timestamp: str | None = None\n</code></pre>"},{"location":"reference/loggers/processors/#imgtools.loggers.processors.JSONFormatter","title":"JSONFormatter","text":"<p>A processor to format the event dictionary for JSON output.</p>"},{"location":"reference/loggers/processors/#imgtools.loggers.processors.PathPrettifier","title":"PathPrettifier","text":"<pre><code>PathPrettifier(\n    base_dir: typing.Optional[pathlib.Path] = None,\n)\n</code></pre> <p>A processor to convert absolute paths to relative paths based on a base directory.</p> <p>Args:     base_dir (Optional[Path]): The base directory to which paths should be made relative. Defaults to the current working directory.</p> Source code in <code>src/imgtools/loggers/processors.py</code> <pre><code>def __init__(self, base_dir: Optional[Path] = None) -&gt; None:\n    self.base_dir = base_dir or Path.cwd()\n</code></pre>"},{"location":"reference/pattern_parser/parser/","title":"Parser","text":""},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser","title":"parser","text":"<p>Parser module for extracting and validating placeholders from target patterns.</p> Summary <p>This module provides functionality to parse and validate sorting patterns with placeholders. Users can define custom regex patterns to extract keys from their sorting patterns.</p> Extended Summary <p>The <code>PatternParser</code> class allows users to define patterns with placeholders that can be replaced with actual values. The placeholders can be defined using custom regex patterns, making the parser flexible for various use cases.</p> <p>Examples:</p> <p>Setup:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import (\n...     PatternParser,\n... )\n</code></pre> <p>Example 1: Suppose you want to parse a target pattern like <code>{Key1}-{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"John\",\n...     \"Key2\": \"Doe\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"{Key1}-{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'John-Doe'\n</code></pre> <p>Example 2: Suppose you want to parse a target pattern like <code>%&lt;Key1&gt; and {Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"Alice\",\n...     \"Key2\": \"Bob\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"%&lt;Key1&gt; and {Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(\n...     r\"%&lt;(\\w+)&gt;|\\{(\\w+)\\}\"\n... )\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s and %(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Alice and Bob'\n</code></pre> <p>Example 3: Suppose you want to parse a target pattern like <code>/path/to/{Key1}/and/{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"folder1\",\n...     \"Key2\": \"folder2\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"/path/to/{Key1}/and/{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'/path/to/%(Key1)s/and/%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'/path/to/folder1/and/folder2'\n</code></pre>"},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser.PatternParser","title":"PatternParser","text":"<pre><code>PatternParser(\n    pattern: str, pattern_matcher: typing.Pattern\n)\n</code></pre> <p>A helper class to parse, validate, and sanitize sorting patterns.</p> <p>This class handles: - Pattern parsing and validation - Key extraction from patterns</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The pattern string to parse.</p> required <code>typing.Pattern</code> <p>Custom regex pattern for parsing</p> required <p>Attributes:</p> Name Type Description <code>keys</code> <code>list of str</code> <p>Extracted keys from the pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import (\n...     PatternParser,\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"Value1\",\n...     \"Key2\": \"Value2\",\n... }\n&gt;&gt;&gt; pattern = \"{Key1}-{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Value1-Value2'\n</code></pre> <p>Methods:</p> Name Description <code>parse</code> <p>Parse and validate the pattern.</p> Source code in <code>src/imgtools/pattern_parser/parser.py</code> <pre><code>def __init__(self, pattern: str, pattern_matcher: Pattern) -&gt; None:\n    assert isinstance(pattern, str) and pattern, (\n        \"Pattern must be a non-empty string.\"\n    )\n    self._pattern = pattern\n    self._keys: List[str] = []\n    assert isinstance(pattern_matcher, Pattern), (\n        \"Pattern parser must be a regex pattern.\"\n    )\n    self._parser: Pattern = pattern_matcher\n</code></pre>"},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser.PatternParser(pattern)","title":"<code>pattern</code>","text":""},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser.PatternParser(pattern_matcher)","title":"<code>pattern_matcher</code>","text":""},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser.PatternParser.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: typing.List[str]\n</code></pre> <p>Get the list of extracted keys.</p>"},{"location":"reference/pattern_parser/parser/#imgtools.pattern_parser.parser.PatternParser.parse","title":"parse","text":"<pre><code>parse() -&gt; typing.Tuple[str, typing.List[str]]\n</code></pre> <p>Parse and validate the pattern.</p> <p>Returns:</p> Type Description <code>typing.Tuple[str, typing.List[str]]</code> <p>The formatted pattern string and a list of extracted keys.</p> Source code in <code>src/imgtools/pattern_parser/parser.py</code> <pre><code>def parse(self) -&gt; Tuple[str, List[str]]:\n    \"\"\"\n    Parse and validate the pattern.\n\n    Returns\n    -------\n    Tuple[str, List[str]]\n        The formatted pattern string and a list of extracted keys.\n\n    Raises\n    ------\n    InvalidPatternError\n        If the pattern contains no valid placeholders or is invalid.\n    \"\"\"\n\n    sanitized_pattern = self._pattern.strip()\n    if not self._parser.search(sanitized_pattern):\n        errmsg = f\"Pattern must contain placeholders matching '{self._parser.pattern}'.\"\n        raise InvalidPatternError(errmsg)\n\n    formatted_pattern = self._parser.sub(\n        self._replace_key, sanitized_pattern\n    )\n    return formatted_pattern, self._keys\n</code></pre>"},{"location":"reference/pattern_parser/pattern_resolver/","title":"Pattern resolver","text":""},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver","title":"pattern_resolver","text":""},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.MissingPlaceholderValueError","title":"MissingPlaceholderValueError","text":"<pre><code>MissingPlaceholderValueError(\n    missing_keys: set[str], class_name: str, key: str\n)\n</code></pre> <p>               Bases: <code>imgtools.pattern_parser.pattern_resolver.PatternResolverError</code></p> <p>Raised when a required placeholder value is missing in the context.</p> Source code in <code>src/imgtools/pattern_parser/pattern_resolver.py</code> <pre><code>def __init__(\n    self, missing_keys: set[str], class_name: str, key: str\n) -&gt; None:\n    self.missing_keys = missing_keys\n    self.class_name = class_name\n    self.key = key\n    super().__init__(self._build_message())\n</code></pre>"},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.PatternResolver","title":"PatternResolver  <code>dataclass</code>","text":"<pre><code>PatternResolver(filename_format: str)\n</code></pre> <p>Handles parsing and validating filename patterns.</p> <p>By default, this class uses the following pattern parser:</p> <p>DEFAULT_PATTERN: re.Pattern = re.compile( ...     r\"%(\\w+)|{(\\w+)}\" ... )</p> <p>This will match placeholders of the form <code>{key}</code> or <code>%(key)s</code>.</p> Example <p>Given a filename format like <code>\"{subject_id}_{date}/{disease}.txt\"</code>, the pattern parser will extract the following keys:</p> <p>pattern_resolver.keys</p> <p>And the following formatted pattern:</p> <p>pattern_resolver.formatted_pattern %(subject_id)s_%(date)s/%(disease)s.txt</p> <p>So you could resolve the pattern like this:</p> <p>data_dict = { ...     \"subject_id\": \"JohnDoe\", ...     \"date\": \"January-01-2025\", ...     \"disease\": \"cancer\", ... }</p> <p>pattern_resolver.formatted_pattern % data_dict 'JohnDoe_01-01-2025/cancer.txt'</p> <p>A more convenient way to resolve the pattern is to use the <code>resolve</code> method:</p> <p>pattern_resolver.resolve(data_dict)) 'JohnDoe_01-01-2025/cancer.txt'</p> <p>Methods:</p> Name Description <code>parse</code> <p>Parse and validate the pattern.</p> <code>resolve</code> <p>Resolve the pattern using the provided context dictionary.</p> Source code in <code>src/imgtools/pattern_parser/pattern_resolver.py</code> <pre><code>def __init__(self, filename_format: str) -&gt; None:\n    self.filename_format = filename_format\n\n    try:\n        self.pattern_parser = PatternParser(\n            self.filename_format, pattern_matcher=self.DEFAULT_PATTERN\n        )\n        self.formatted_pattern, self.keys = (\n            self.parse()\n        )  # Validate the pattern by parsing it\n    except InvalidPatternError as e:\n        msg = f\"Invalid filename format: {e}\"\n        raise PatternResolverError(msg) from e\n    else:\n        logger.debug(\n            \"Pattern validation successful.\",\n            keys=self.keys,\n            formatted_pattern=self.formatted_pattern,\n        )\n</code></pre>"},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.PatternResolver.parse","title":"parse","text":"<pre><code>parse() -&gt; typing.Tuple[str, list[str]]\n</code></pre> <p>Parse and validate the pattern.</p> <p>Returns:</p> Type Description <code>typing.Tuple[str, List[str]]</code> <p>The formatted pattern string and a list of extracted keys.</p> Source code in <code>src/imgtools/pattern_parser/pattern_resolver.py</code> <pre><code>def parse(self) -&gt; Tuple[str, list[str]]:\n    \"\"\"\n    Parse and validate the pattern.\n\n    Returns\n    -------\n    Tuple[str, List[str]]\n        The formatted pattern string and a list of extracted keys.\n\n    Raises\n    ------\n    InvalidPatternError\n        If the pattern contains no valid placeholders or is invalid.\n    \"\"\"\n    if hasattr(self, \"formatted_pattern\") and hasattr(self, \"keys\"):\n        return self.formatted_pattern, self.keys\n\n    self.formatted_pattern, self.keys = self.pattern_parser.parse()\n    return self.formatted_pattern, self.keys\n</code></pre>"},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.PatternResolver.resolve","title":"resolve","text":"<pre><code>resolve(context: typing.Dict[str, typing.Any]) -&gt; str\n</code></pre> <p>Resolve the pattern using the provided context dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Dict[str, typing.Any]</code> <p>Dictionary containing key-value pairs to substitute in the pattern.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The resolved pattern string with placeholders replaced by values.</p> Source code in <code>src/imgtools/pattern_parser/pattern_resolver.py</code> <pre><code>def resolve(self, context: Dict[str, Any]) -&gt; str:\n    \"\"\"Resolve the pattern using the provided context dictionary.\n\n    Parameters\n    ----------\n    context : Dict[str, Any]\n        Dictionary containing key-value pairs to substitute in the pattern.\n\n    Returns\n    -------\n    str\n        The resolved pattern string with placeholders replaced by values.\n\n    Raises\n    ------\n    PatternResolverError\n        If a required key is missing from the context dictionary.\n    \"\"\"\n\n    # simultaneously check for None values and validate the pattern\n    if len(none_keys := [k for k, v in context.items() if v is None]) &gt; 0:\n        msg = \"None is not a valid value for a placeholder in the pattern.\"\n        msg += f\" None keys: {none_keys}\"\n        raise PatternResolverError(msg)\n\n    try:\n        return self.formatted_pattern % context\n    except KeyError as e:\n        # Determine the missing key and construct the error dynamically\n        missing_keys = set(self.keys) - set(context.keys())\n        raise MissingPlaceholderValueError(\n            missing_keys=missing_keys,\n            class_name=self.__class__.__name__,\n            key=e.args[0],\n        ) from e\n</code></pre>"},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.PatternResolver.resolve(context)","title":"<code>context</code>","text":""},{"location":"reference/pattern_parser/pattern_resolver/#imgtools.pattern_parser.pattern_resolver.PatternResolverError","title":"PatternResolverError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for errors in pattern resolution.</p>"},{"location":"reference/transforms/base_transform/","title":"Base transform","text":""},{"location":"reference/transforms/base_transform/#imgtools.transforms.base_transform","title":"base_transform","text":""},{"location":"reference/transforms/base_transform/#imgtools.transforms.base_transform.BaseTransform","title":"BaseTransform","text":"<p>               Bases: <code>typing.Generic[imgtools.transforms.base_transform.T_Image]</code>, <code>abc.ABC</code></p> <p>Abstract base class for image transforms.</p> <p>Classes inheriting from this must implement the <code>__call__</code> method that applies a transformation to an image and returns the result.</p> <p>This class provides a common interface for all transforms in the package, allowing them to be used interchangeably and composed together.</p>"},{"location":"reference/transforms/base_transform/#imgtools.transforms.base_transform.BaseTransform.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/functional/","title":"Functional","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional","title":"functional","text":"<p>Functions:</p> Name Description <code>clip_intensity</code> <p>Clip image intensities to a specified range.</p> <code>crop</code> <p>Crop an image to a specified window size about a given center.</p> <code>resample</code> <p>Resample an image to a new spacing with optional transform.</p> <code>resize</code> <p>Resize an image to a specified size by resampling its coordinates.</p> <code>rotate</code> <p>Rotate an image around a specified center.</p> <code>window_intensity</code> <p>Restrict image grey level intensities to a given window and level.</p> <code>zoom</code> <p>Rescale image, preserving its spatial extent.</p>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.clip_intensity","title":"clip_intensity","text":"<pre><code>clip_intensity(\n    image: SimpleITK.Image, lower: float, upper: float\n) -&gt; SimpleITK.Image\n</code></pre> <p>Clip image intensities to a specified range.</p> <p>Adjusts the input image so that all voxel intensity values lie within the [lower, upper] range. Values below the lower bound are set to lower, while those above the upper bound are set to upper.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The input intensity image.</p> required <code>float</code> <p>The minimum allowable intensity value.</p> required <code>float</code> <p>The maximum allowable intensity value.</p> required <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The resulting image with intensity values clipped between lower and upper.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def clip_intensity(\n    image: sitk.Image, lower: float, upper: float\n) -&gt; sitk.Image:\n    \"\"\"Clip image intensities to a specified range.\n\n    Adjusts the input image so that all voxel intensity values lie within the\n    [lower, upper] range. Values below the lower bound are set to lower, while\n    those above the upper bound are set to upper.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The input intensity image.\n    lower : float\n        The minimum allowable intensity value.\n    upper : float\n        The maximum allowable intensity value.\n\n    Returns\n    -------\n    sitk.Image\n        The resulting image with intensity values clipped between lower and upper.\n    \"\"\"\n    return sitk.Clamp(image, image.GetPixelID(), lower, upper)\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.clip_intensity(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.clip_intensity(lower)","title":"<code>lower</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.clip_intensity(upper)","title":"<code>upper</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.crop","title":"crop","text":"<pre><code>crop(\n    image: SimpleITK.Image,\n    crop_centre: list[float] | numpy.ndarray,\n    size: int | list[int] | numpy.ndarray,\n) -&gt; SimpleITK.Image\n</code></pre> <p>Crop an image to a specified window size about a given center.</p> <p>This function extracts a sub-region from the input image centered at the provided coordinates. If the cropping window extends beyond the image boundaries, the resulting image will be clipped accordingly. A cropping size of 0 for any dimension retains the original image extent along that axis.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The SimpleITK image to crop.</p> required <code>list[float] | numpy.ndarray</code> <p>The center of the cropping window in image coordinates.</p> required <code>int | list[int] | numpy.ndarray</code> <p>The size of the cropping window in pixels. If an int is provided, the same size is applied to all dimensions; a sequence specifies the size along each axis. Use 0 to preserve the original size along a particular dimension.</p> required <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The cropped image.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def crop(\n    image: sitk.Image,\n    crop_centre: list[float] | np.ndarray,\n    size: int | list[int] | np.ndarray,\n) -&gt; sitk.Image:\n    \"\"\"Crop an image to a specified window size about a given center.\n\n    This function extracts a sub-region from the input image centered at the\n    provided coordinates. If the cropping window extends beyond the image\n    boundaries, the resulting image will be clipped accordingly. A cropping size\n    of 0 for any dimension retains the original image extent along that axis.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The SimpleITK image to crop.\n    crop_centre : list[float] | np.ndarray\n        The center of the cropping window in image coordinates.\n    size : int | list[int] | np.ndarray\n        The size of the cropping window in pixels. If an int is provided,\n        the same size is applied to all dimensions; a sequence specifies the\n        size along each axis. Use 0 to preserve the original size along a\n        particular dimension.\n\n    Returns\n    -------\n    sitk.Image\n        The cropped image.\n\n    Raises\n    ------\n    ValueError\n        If the cropping center is outside the image boundaries.\n    \"\"\"\n    crop_centre = np.asarray(crop_centre, dtype=np.float64)\n    original_size = np.asarray(image.GetSize())\n\n    size = (\n        np.array([size for _ in image.GetSize()])\n        if isinstance(size, int)\n        else np.asarray(size)\n    )\n\n    if (crop_centre &lt; 0).any() or (crop_centre &gt; original_size).any():\n        msg = f\"Crop centre outside image boundaries. Image size = {original_size}, crop centre = {crop_centre}\"\n        raise ValueError(msg)\n\n    min_coords = np.clip(\n        np.floor(crop_centre - size / 2).astype(np.int64), 0, original_size\n    )\n    min_coords = np.where(size == 0, 0, min_coords)\n\n    max_coords = np.clip(\n        np.floor(crop_centre + size / 2).astype(np.int64), 0, original_size\n    )\n    max_coords = np.where(size == 0, original_size, max_coords)\n\n    min_x, min_y, min_z = min_coords\n    max_x, max_y, max_z = max_coords\n\n    return image[min_x:max_x, min_y:max_y, min_z:max_z]\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.crop(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.crop(crop_centre)","title":"<code>crop_centre</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.crop(size)","title":"<code>size</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample","title":"resample","text":"<pre><code>resample(\n    image: SimpleITK.Image,\n    spacing: float | list[float] | numpy.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | list[float] | None = None,\n    transform: SimpleITK.Transform | None = None,\n    output_size: list[float] | None = None,\n) -&gt; SimpleITK.Image\n</code></pre> <p>Resample an image to a new spacing with optional transform.</p> <p>Resamples the input image using the specified spacing, computing a new image size to maintain the original spatial extent unless explicitly set via output_size. A transformation can be applied during resampling, and Gaussian smoothing is used for anti-aliasing when downsampling.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The SimpleITK image to be resampled.</p> required <code>float | list[float] | numpy.ndarray</code> <p>The desired spacing for each axis. A single float applies to all dimensions, while a sequence specifies spacing per axis. Use 0 for any axis to retain its original spacing.</p> required <code>str</code> <p>The interpolation method to use. Accepted values are \"linear\", \"nearest\", and \"bspline\". Defaults to \"linear\".</p> <code>'linear'</code> <code>bool</code> <p>If True, applies Gaussian smoothing before resampling when downsampling to reduce aliasing artifacts. Defaults to True.</p> <code>True</code> <code>float | list[float] | None</code> <p>The standard deviation for the Gaussian smoothing kernel. If not provided, it is automatically computed.</p> <code>None</code> <code>SimpleITK.Transform | None</code> <p>A transformation to apply to the image coordinates during resampling. Defaults to the identity transformation if not specified.</p> <code>None</code> <code>list[float] | None</code> <p>The desired size of the output image. If omitted, the size is calculated to preserve the entire extent of the input image.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The resampled image.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def resample(\n    image: sitk.Image,\n    spacing: float | list[float] | np.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | list[float] | None = None,\n    transform: sitk.Transform | None = None,\n    output_size: list[float] | None = None,\n) -&gt; sitk.Image:\n    \"\"\"Resample an image to a new spacing with optional transform.\n\n    Resamples the input image using the specified spacing, computing a new\n    image size to maintain the original spatial extent unless explicitly set\n    via output_size. A transformation can be applied during resampling, and\n    Gaussian smoothing is used for anti-aliasing when downsampling.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The SimpleITK image to be resampled.\n    spacing : float | list[float] | np.ndarray\n        The desired spacing for each axis. A single float applies to all\n        dimensions, while a sequence specifies spacing per axis. Use 0 for any\n        axis to retain its original spacing.\n    interpolation : str, optional\n        The interpolation method to use. Accepted values are \"linear\",\n        \"nearest\", and \"bspline\". Defaults to \"linear\".\n    anti_alias : bool, optional\n        If True, applies Gaussian smoothing before resampling when downsampling\n        to reduce aliasing artifacts. Defaults to True.\n    anti_alias_sigma : float | list[float] | None, optional\n        The standard deviation for the Gaussian smoothing kernel. If not\n        provided, it is automatically computed.\n    transform : sitk.Transform | None, optional\n        A transformation to apply to the image coordinates during resampling.\n        Defaults to the identity transformation if not specified.\n    output_size : list[float] | None, optional\n        The desired size of the output image. If omitted, the size is\n        calculated to preserve the entire extent of the input image.\n\n    Returns\n    -------\n    sitk.Image\n        The resampled image.\n\n    Raises\n    ------\n    ValueError\n        If the specified interpolation method is not supported.\n    \"\"\"\n\n    try:\n        interpolator = INTERPOLATORS[interpolation]\n    except KeyError as ke:\n        msg = f\"interpolator must be one of {list(INTERPOLATORS.keys())}, got {interpolation}.\"\n        raise ValueError(msg) from ke\n\n    original_spacing = np.array(image.GetSpacing())\n    original_size = np.array(image.GetSize())\n\n    if isinstance(spacing, (float, int)):\n        new_spacing = np.repeat(spacing, len(original_spacing)).astype(\n            np.float64\n        )\n    else:\n        spacing = np.asarray(spacing)\n        new_spacing = np.where(spacing == 0, original_spacing, spacing)\n\n    if output_size is None:\n        new_size = np.round(\n            original_size * original_spacing / new_spacing, decimals=0\n        ).astype(int)\n    else:\n        new_size = np.asarray(output_size).astype(int)\n\n    rif = sitk.ResampleImageFilter()\n    rif.SetOutputOrigin(image.GetOrigin())\n    rif.SetOutputSpacing(new_spacing)\n    rif.SetOutputDirection(image.GetDirection())\n    rif.SetSize(new_size.tolist())\n\n    if transform is not None:\n        rif.SetTransform(transform)\n\n    downsample = new_spacing &gt; original_spacing\n    if downsample.any() and anti_alias:\n        if not anti_alias_sigma:\n            # sigma computation adapted from scikit-image\n            # https://github.com/scikit-image/scikit-image/blob/master/skimage/transform/_warps.py\n            anti_alias_sigma = list(\n                np.maximum(1e-11, (original_spacing / new_spacing - 1) / 2)\n            )\n        sigma = np.where(downsample, anti_alias_sigma, 1e-11)\n        image = sitk.SmoothingRecursiveGaussian(image, sigma)\n\n    rif.SetInterpolator(interpolator)\n    resampled_image = rif.Execute(image)\n\n    return resampled_image\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(spacing)","title":"<code>spacing</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(transform)","title":"<code>transform</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resample(output_size)","title":"<code>output_size</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize","title":"resize","text":"<pre><code>resize(\n    image: SimpleITK.Image,\n    size: int | list[int] | numpy.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n) -&gt; SimpleITK.Image\n</code></pre> <p>Resize an image to a specified size by resampling its coordinates.</p> <p>The function calculates new spacing based on the target size and the original image's dimensions. A value of 0 in the size parameter for any axis preserves the original size for that dimension.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The image to be resized.</p> required <code>int | list[int] | numpy.ndarray</code> <p>The target image size. If an integer is provided, the same size is applied to all axes. For sequences, a value of 0 for an axis retains the original size in that dimension.</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi-/trilinear interpolation (default) - \"nearest\" for nearest-neighbor interpolation - \"bspline\" for order-3 b-spline interpolation.</p> <code>'linear'</code> <code>bool</code> <p>If True, apply Gaussian smoothing before resampling when downsampling to reduce aliasing artifacts.</p> <code>True</code> <code>float | None</code> <p>The standard deviation of the Gaussian kernel used for anti-aliasing.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The resized image.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def resize(\n    image: sitk.Image,\n    size: int | list[int] | np.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n) -&gt; sitk.Image:\n    \"\"\"Resize an image to a specified size by resampling its coordinates.\n\n    The function calculates new spacing based on the target size and the\n    original image's dimensions. A value of 0 in the size parameter for any\n    axis preserves the original size for that dimension.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The image to be resized.\n    size : int | list[int] | np.ndarray\n        The target image size. If an integer is provided, the same size is\n        applied to all axes. For sequences, a value of 0 for an axis retains\n        the original size in that dimension.\n    interpolation : str, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi-/trilinear interpolation (default)\n        - \"nearest\" for nearest-neighbor interpolation\n        - \"bspline\" for order-3 b-spline interpolation.\n    anti_alias : bool, optional\n        If True, apply Gaussian smoothing before resampling when downsampling\n        to reduce aliasing artifacts.\n    anti_alias_sigma : float | None, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    Returns\n    -------\n    sitk.Image\n        The resized image.\n    \"\"\"\n\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n\n    if isinstance(size, (float, int)):\n        new_size = np.repeat(size, len(original_size)).astype(np.float64)\n    else:\n        size = np.asarray(size)\n        new_size = np.where(size == 0, original_size, size)\n\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(\n        image,\n        new_spacing,\n        anti_alias=anti_alias,\n        anti_alias_sigma=anti_alias_sigma,\n        interpolation=interpolation,\n        output_size=list(new_size),\n    )\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize(size)","title":"<code>size</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.resize(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.rotate","title":"rotate","text":"<pre><code>rotate(\n    image: SimpleITK.Image,\n    rotation_centre: list[int],\n    angles: list[float],\n    interpolation: str = \"linear\",\n) -&gt; SimpleITK.Image\n</code></pre> <p>Rotate an image around a specified center.</p> <p>This function applies an Euler rotation to the input image. For 2D images, only the first angle in the provided list is used. For 3D images, all three angles (for the x, y, and z axes, respectively) are applied.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The image to rotate.</p> required <code>list[int]</code> <p>The center of rotation in image coordinates. If provided as a NumPy array, it is converted to a list.</p> required <code>list[float]</code> <p>A list of rotation angles in radians. For 2D images, only the first value is used. For 3D images, the angles correspond to rotations around the x, y, and z axes.</p> required <code>str</code> <p>The interpolation method for resampling (e.g., \"linear\", \"nearest\").</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The rotated image.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def rotate(\n    image: sitk.Image,\n    rotation_centre: list[int],\n    angles: list[float],\n    interpolation: str = \"linear\",\n) -&gt; sitk.Image:\n    \"\"\"Rotate an image around a specified center.\n\n    This function applies an Euler rotation to the input image. For 2D images,\n    only the first angle in the provided list is used. For 3D images, all three\n    angles (for the x, y, and z axes, respectively) are applied.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The image to rotate.\n    rotation_centre : list[int]\n        The center of rotation in image coordinates. If provided as a NumPy\n        array, it is converted to a list.\n    angles : list[float]\n        A list of rotation angles in radians. For 2D images, only the first\n        value is used. For 3D images, the angles correspond to rotations around\n        the x, y, and z axes.\n    interpolation : str, optional\n        The interpolation method for resampling (e.g., \"linear\", \"nearest\").\n\n    Returns\n    -------\n    sitk.Image\n        The rotated image.\n    \"\"\"\n    if isinstance(rotation_centre, np.ndarray):\n        rotation_centre = rotation_centre.tolist()\n\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n\n    rotation: sitk.Euler2DTransform | sitk.Euler3DTransform\n    if image.GetDimension() == 2:\n        rotation = sitk.Euler2DTransform(\n            rotation_centre,\n            angles,\n            (0.0, 0.0),  # no translation\n        )\n    elif image.GetDimension() == 3:\n        x_angle, y_angle, z_angle = angles\n\n        rotation = sitk.Euler3DTransform(\n            rotation_centre,\n            x_angle,  # the angle of rotation around the x-axis, in radians -&gt; coronal rotation\n            y_angle,  # the angle of rotation around the y-axis, in radians -&gt; saggittal rotation\n            z_angle,  # the angle of rotation around the z-axis, in radians -&gt; axial rotation\n            (0.0, 0.0, 0.0),  # no translation\n        )\n    return resample(\n        image,\n        spacing=image.GetSpacing(),\n        interpolation=interpolation,\n        transform=rotation,\n    )\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.rotate(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.rotate(rotation_centre)","title":"<code>rotation_centre</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.rotate(angles)","title":"<code>angles</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.rotate(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.window_intensity","title":"window_intensity","text":"<pre><code>window_intensity(\n    image: SimpleITK.Image, window: float, level: float\n) -&gt; SimpleITK.Image\n</code></pre> <p>Restrict image grey level intensities to a given window and level.</p> <p>The grey level intensities in the resulting image will fall in the range [level - window / 2, level + window / 2].</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The intensity image to window.</p> required <code>float</code> <p>The width of the intensity window.</p> required <code>float</code> <p>The mid-point of the intensity window.</p> required <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The windowed intensity image</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def window_intensity(\n    image: sitk.Image, window: float, level: float\n) -&gt; sitk.Image:\n    \"\"\"Restrict image grey level intensities to a given window and level.\n\n    The grey level intensities in the resulting image will fall in the range\n    [level - window / 2, level + window / 2].\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The intensity image to window.\n    window : float\n        The width of the intensity window.\n    level : float\n        The mid-point of the intensity window.\n\n    Returns\n    -------\n    sitk.Image\n        The windowed intensity image\n    \"\"\"\n    lower = level - window / 2\n    upper = level + window / 2\n    return clip_intensity(image, lower, upper)\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.window_intensity(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.window_intensity(window)","title":"<code>window</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.window_intensity(level)","title":"<code>level</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom","title":"zoom","text":"<pre><code>zoom(\n    image: SimpleITK.Image,\n    scale_factor: float | list[float],\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n) -&gt; SimpleITK.Image\n</code></pre> <p>Rescale image, preserving its spatial extent.</p> <p>The image is rescaled using the provided scale factor for each dimension while maintaining its original spatial extent. A single float applies uniformly across all dimensions, whereas a sequence specifies a separate factor for each axis.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The image to rescale.</p> required <code>float | list[float]</code> <p>The scaling factor(s) to apply. A float applies to all dimensions; a sequence specifies a factor for each corresponding dimension.</p> required <code>str</code> <p>The interpolation method to use. Options include \"linear\" (default), \"nearest\", and \"bspline\".</p> <code>'linear'</code> <code>bool</code> <p>Whether to smooth the image using a Gaussian kernel before resampling, which helps reduce aliasing artifacts during downsampling.</p> <code>True</code> <code>float | None</code> <p>The standard deviation for the Gaussian kernel used in anti-aliasing.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The rescaled image with the same spatial extent as the original.</p> Source code in <code>src/imgtools/transforms/functional.py</code> <pre><code>def zoom(\n    image: sitk.Image,\n    scale_factor: float | list[float],\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n) -&gt; sitk.Image:\n    \"\"\"Rescale image, preserving its spatial extent.\n\n    The image is rescaled using the provided scale factor for each dimension\n    while maintaining its original spatial extent. A single float applies\n    uniformly across all dimensions, whereas a sequence specifies a separate\n    factor for each axis.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The image to rescale.\n    scale_factor : float | list[float]\n        The scaling factor(s) to apply. A float applies to all dimensions;\n        a sequence specifies a factor for each corresponding dimension.\n    interpolation : str, optional\n        The interpolation method to use. Options include \"linear\" (default),\n        \"nearest\", and \"bspline\".\n    anti_alias : bool, optional\n        Whether to smooth the image using a Gaussian kernel before resampling,\n        which helps reduce aliasing artifacts during downsampling.\n    anti_alias_sigma : float | None, optional\n        The standard deviation for the Gaussian kernel used in anti-aliasing.\n\n    Returns\n    -------\n    sitk.Image\n        The rescaled image with the same spatial extent as the original.\n    \"\"\"\n    dimension = image.GetDimension()\n\n    if isinstance(scale_factor, float):\n        scale_factor = (scale_factor,) * dimension\n\n    centre_idx = np.array(image.GetSize()) / 2\n    centre = image.TransformContinuousIndexToPhysicalPoint(centre_idx)\n\n    transform = sitk.ScaleTransform(dimension, scale_factor)\n    transform.SetCenter(centre)\n\n    return resample(\n        image,\n        spacing=image.GetSpacing(),\n        interpolation=interpolation,\n        anti_alias=anti_alias,\n        anti_alias_sigma=anti_alias_sigma,\n        transform=transform,\n        output_size=image.GetSize(),\n    )\n</code></pre>"},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom(image)","title":"<code>image</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom(scale_factor)","title":"<code>scale_factor</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/functional/#imgtools.transforms.functional.zoom(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/intensity_transforms/","title":"Intensity transforms","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms","title":"intensity_transforms","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.ClipIntensity","title":"ClipIntensity  <code>dataclass</code>","text":"<pre><code>ClipIntensity(lower: float, upper: float)\n</code></pre> <p>               Bases: <code>imgtools.transforms.intensity_transforms.IntensityTransform</code></p> <p>ClipIntensity operation class.</p> <p>A callable class that clips image grey level intensities to specified range.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>The lower bound on grey level intensity. Voxels with lower intensity will be set to this value.</p> required <code>float</code> <p>The upper bound on grey level intensity. Voxels with higer intensity will be set to this value.</p> required <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.ClipIntensity(lower)","title":"<code>lower</code>","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.ClipIntensity(upper)","title":"<code>upper</code>","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.ClipIntensity.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.ClipIntensity.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always False for intensity transforms.</p> Source code in <code>src/imgtools/transforms/intensity_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        Always False for intensity transforms.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.IntensityTransform","title":"IntensityTransform","text":"<p>               Bases: <code>imgtools.transforms.base_transform.BaseTransform</code></p> <p>Base class for intensity transforms.</p> <p>Intensity transforms modify the pixel/voxel intensity values of an image without changing its spatial properties.</p> <p>All intensity transforms operate on individual pixel/voxel values independently, leaving the image dimensions, spacing, and orientation unchanged.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is an abstract base class and cannot be instantiated directly.\n&gt;&gt;&gt; # Use one of its subclasses like ClipIntensity or WindowIntensity.\n</code></pre> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.IntensityTransform.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.IntensityTransform.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always False for intensity transforms.</p> Source code in <code>src/imgtools/transforms/intensity_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        Always False for intensity transforms.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.WindowIntensity","title":"WindowIntensity  <code>dataclass</code>","text":"<pre><code>WindowIntensity(window: float, level: float)\n</code></pre> <p>               Bases: <code>imgtools.transforms.intensity_transforms.IntensityTransform</code></p> <p>WindowIntensity operation class.</p> <p>A callable class that restricts image grey level intensities to a given window and level. This is commonly used in medical imaging to enhance visibility of specific tissue types (e.g., bone window, lung window in CT).</p> <p>The window/level parameters define a range centered at the 'level' value with a width of 'window'. All intensities outside this range are clipped.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>The width of the intensity window. Must be positive.</p> required <code>float</code> <p>The mid-point of the intensity window.</p> required <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.WindowIntensity(window)","title":"<code>window</code>","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.WindowIntensity(level)","title":"<code>level</code>","text":""},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.WindowIntensity.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/intensity_transforms/#imgtools.transforms.intensity_transforms.WindowIntensity.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Always False for intensity transforms.</p> Source code in <code>src/imgtools/transforms/intensity_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        Always False for intensity transforms.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/lambda_transforms/","title":"Lambda transforms","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms","title":"lambda_transforms","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction","title":"ImageFunction","text":"<pre><code>ImageFunction(\n    function: typing.Callable[..., SimpleITK.Image],\n    copy_geometry: bool = True,\n    **kwargs: typing.Any\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.base_transform.BaseTransform</code></p> <p>Apply a custom function to process an image.</p> <p>This class wraps a user-defined function and allows it to be used like any other transform in the library. The function must accept a SimpleITK image as its first parameter and return a SimpleITK image.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Callable[..., SimpleITK.Image]</code> <p>A callable that processes a SimpleITK image and returns a processed image.</p> required <code>bool</code> <p>If True, copies the input image's geometry to the result.</p> <code>True</code> <code>typing.Any</code> <p>Optional keyword arguments to be passed to the processing function.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import SimpleITK as sitk\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from imgtools.datasets.sample_images import (\n...     create_sphere_image,\n... )\n&gt;&gt;&gt; from imgtools.transforms import ImageFunction\n&gt;&gt;&gt; # Create a sample sphere image\n&gt;&gt;&gt; image = create_sphere_image()\n&gt;&gt;&gt; # Define a custom function to invert intensities\n&gt;&gt;&gt; def invert_intensity(image, max_value=1.0):\n...     return sitk.Subtract(max_value, image)\n&gt;&gt;&gt; # Create a transform using this function\n&gt;&gt;&gt; inverter = ImageFunction(\n...     invert_intensity, max_value=1.0\n... )\n&gt;&gt;&gt; inverted = inverter(image)\n&gt;&gt;&gt; # Define a function that operates on the NumPy array\n&gt;&gt;&gt; def add_random_noise(image, noise_level=0.1):\n...     array = sitk.GetArrayFromImage(image)\n...     noise = np.random.normal(\n...         0, noise_level, array.shape\n...     )\n...     array = array + noise\n...     result = sitk.GetImageFromArray(array)\n...     result.CopyInformation(image)\n...     return result\n&gt;&gt;&gt; # Create and apply a noise transform\n&gt;&gt;&gt; noiser = ImageFunction(\n...     add_random_noise, noise_level=0.2\n... )\n&gt;&gt;&gt; noisy = noiser(image)\n&gt;&gt;&gt; print(\"Applied custom image transforms\")\nApplied custom image transforms\n</code></pre> <p>Registers a user-defined function for image processing along with its settings. The supplied function must accept a SimpleITK image and return a processed image. If copy_geometry is True, the geometry of the input image will be copied to the output.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Callable[..., SimpleITK.Image]</code> <p>A callable that processes a SimpleITK image and returns a processed image.</p> required <code>bool</code> <p>If True, copies the input image's geometry to the result.</p> <code>True</code> <code>typing.Any</code> <p>Optional keyword arguments to be passed to the processing function.</p> <code>{}</code> Source code in <code>src/imgtools/transforms/lambda_transforms.py</code> <pre><code>def __init__(\n    self,\n    function: Callable[..., sitk.Image],\n    copy_geometry: bool = True,\n    **kwargs: Any,  # noqa: ANN401\n) -&gt; None:\n    \"\"\"Initialize an ImageFunction transform.\n\n    Registers a user-defined function for image processing along with its\n    settings. The supplied function must accept a SimpleITK image and\n    return a processed image. If copy_geometry is True, the geometry of\n    the input image will be copied to the output.\n\n    Parameters\n    ----------\n    function : Callable[..., sitk.Image]\n        A callable that processes a SimpleITK image and returns a\n        processed image.\n    copy_geometry : bool, optional\n        If True, copies the input image's geometry to the result.\n    **kwargs : Any, optional\n        Optional keyword arguments to be passed to the processing function.\n    \"\"\"\n    self.function = function\n    self.copy_geometry = copy_geometry\n    self.kwargs = kwargs\n</code></pre>"},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(function)","title":"<code>function</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(copy_geometry)","title":"<code>copy_geometry</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(function)","title":"<code>function</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(copy_geometry)","title":"<code>copy_geometry</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.ImageFunction.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter","title":"SimpleITKFilter","text":"<pre><code>SimpleITKFilter(\n    sitk_filter: SimpleITK.ImageFilter,\n    *execute_args: typing.Any\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.base_transform.BaseTransform</code></p> <p>Apply a SimpleITK image filter to an image.</p> <p>This class wraps a SimpleITK image filter and allows it to be used like any other transform in the library. It provides a consistent interface for applying SimpleITK filters within the transform framework.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.ImageFilter</code> <p>A SimpleITK image filter instance to process images.</p> required <code>typing.Any</code> <p>Optional positional arguments to pass to the filter's Execute method. These arguments come after the input image in the Execute call.</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import SimpleITK as sitk\n&gt;&gt;&gt; from imgtools.datasets.sample_images import (\n...     create_noisy_sphere_image,\n... )\n&gt;&gt;&gt; from imgtools.transforms import SimpleITKFilter\n&gt;&gt;&gt; # Create a sample image with noise\n&gt;&gt;&gt; image = create_noisy_sphere_image(\n...     noise_level=0.3\n... )\n&gt;&gt;&gt; # Create a median filter to remove noise\n&gt;&gt;&gt; median_filter = sitk.MedianImageFilter()\n&gt;&gt;&gt; median_filter.SetRadius(2)\n&gt;&gt;&gt; # Use SimpleITKFilter to apply the filter\n&gt;&gt;&gt; median_transform = SimpleITKFilter(median_filter)\n&gt;&gt;&gt; filtered_image = median_transform(image)\n&gt;&gt;&gt; # Use SimpleITKFilter with a different filter that takes parameters\n&gt;&gt;&gt; gradient_magnitude = sitk.GradientMagnitudeRecursiveGaussianImageFilter()\n&gt;&gt;&gt; gradient_transform = SimpleITKFilter(\n...     gradient_magnitude, 1.5\n... )  # sigma=1.5\n&gt;&gt;&gt; gradient_image = gradient_transform(image)\n&gt;&gt;&gt; print(\n...     \"Applied median filter and gradient magnitude filter\"\n... )\nApplied median filter and gradient magnitude filter\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.ImageFilter</code> <p>A SimpleITK image filter instance to process images.</p> required <code>typing.Any</code> <p>Optional positional arguments to pass to the filter's Execute method.</p> <code>()</code> Source code in <code>src/imgtools/transforms/lambda_transforms.py</code> <pre><code>def __init__(\n    self,\n    sitk_filter: sitk.ImageFilter,\n    *execute_args: Any,  # noqa: ANN401\n) -&gt; None:\n    \"\"\"Initialize a SimpleITKFilter with a filter and execution args.\n\n    Parameters\n    ----------\n    sitk_filter : sitk.ImageFilter\n        A SimpleITK image filter instance to process images.\n    *execute_args : Any, optional\n        Optional positional arguments to pass to the filter's Execute\n        method.\n    \"\"\"\n    self.sitk_filter = sitk_filter\n    self.execute_args = execute_args\n</code></pre>"},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter(sitk_filter)","title":"<code>sitk_filter</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter(*execute_args)","title":"<code>*execute_args</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter(sitk_filter)","title":"<code>sitk_filter</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter(*execute_args)","title":"<code>*execute_args</code>","text":""},{"location":"reference/transforms/lambda_transforms/#imgtools.transforms.lambda_transforms.SimpleITKFilter.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/","title":"Spatial transforms","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms","title":"spatial_transforms","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.InPlaneRotate","title":"InPlaneRotate  <code>dataclass</code>","text":"<pre><code>InPlaneRotate(angle: float, interpolation: str = 'linear')\n</code></pre> <p>               Bases: <code>imgtools.transforms.spatial_transforms.SpatialTransform</code></p> <p>InPlaneRotate operation class.</p> <p>A callable class that rotates an image on a plane.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>The angle of rotation.</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi/trilinear interpolation (default) - \"nearest\" for nearest neighbour interpolation - \"bspline\" for order-3 b-spline interpolation</p> <code>'linear'</code> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.InPlaneRotate(angle)","title":"<code>angle</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.InPlaneRotate(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.InPlaneRotate.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.InPlaneRotate.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        False by default. Subclasses should override this method\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample","title":"Resample  <code>dataclass</code>","text":"<pre><code>Resample(\n    spacing: float | typing.Sequence[float] | numpy.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n    transform: SimpleITK.Transform | None = None,\n    output_size: list[float] | None = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.spatial_transforms.SpatialTransform</code></p> <p>Resample operation class.</p> <p>A callable class that resamples image to a given spacing, optionally applying a transformation.</p> <p>Parameters:</p> Name Type Description Default <code>float | typing.Sequence[float] | numpy.ndarray</code> <p>The new image spacing. If float, assumes the same spacing in all directions. Alternatively, a sequence of floats can be passed to specify spacing along each dimension. Passing 0 at any position will keep the original spacing along that dimension (useful for in-plane resampling).</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi/trilinear interpolation (default) - \"nearest\" for nearest neighbour interpolation - \"bspline\" for order-3 b-spline interpolation</p> <code>'linear'</code> <code>bool</code> <p>Whether to smooth the image with a Gaussian kernel before resampling. Only used when downsampling, i.e. when <code>spacing &lt; image.GetSpacing()</code>. This should be used to avoid aliasing artifacts.</p> <code>True</code> <code>float | None</code> <p>The standard deviation of the Gaussian kernel used for anti-aliasing.</p> <code>None</code> <code>SimpleITK.Transform | None</code> <p>Transform to apply to input coordinates when resampling. If None, defaults to identity transformation.</p> <code>None</code> <code>list[float] | None</code> <p>Size of the output image. If None, it is computed to preserve the whole extent of the input image.</p> <code>None</code> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(spacing)","title":"<code>spacing</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(transform)","title":"<code>transform</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample(output_size)","title":"<code>output_size</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resample.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> <code>Return whether this transform supports reference images.</code> <p>Returns:</p> Type Description <code>bool</code> <p>Always True for spatial transforms.</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        Always True for spatial transforms.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize","title":"Resize  <code>dataclass</code>","text":"<pre><code>Resize(\n    size: int | list[int] | numpy.ndarray,\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.spatial_transforms.SpatialTransform</code></p> <p>Resize operation class.</p> <p>A callable class that resizes image to a given size by resampling coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>int | list[int] | numpy.ndarray</code> <p>The new image size. If float, assumes the same size in all directions. Alternatively, a sequence of floats can be passed to specify size along each dimension. Passing 0 at any position will keep the original size along that dimension.</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi/trilinear interpolation (default) - \"nearest\" for nearest neighbour interpolation - \"bspline\" for order-3 b-spline interpolation</p> <code>'linear'</code> <code>bool</code> <p>Whether to smooth the image with a Gaussian kernel before resampling. Only used when downsampling, i.e. when <code>size &lt; image.GetSize()</code>. This should be used to avoid aliasing artifacts.</p> <code>True</code> <code>float | None</code> <p>The standard deviation of the Gaussian kernel used for anti-aliasing.</p> <code>None</code> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize(size)","title":"<code>size</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Resize.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        False by default. Subclasses should override this method\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate","title":"Rotate  <code>dataclass</code>","text":"<pre><code>Rotate(\n    rotation_centre: list[int],\n    angles: float | list[float],\n    interpolation: str = \"linear\",\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.spatial_transforms.SpatialTransform</code></p> <p>Rotate operation class.</p> <p>A callable class that rotates an image around a given centre.</p> <p>Parameters:</p> Name Type Description Default <code>list[int]</code> <p>The centre of rotation in image coordinates.</p> required <code>float | list[float]</code> <p>The angles of rotation around x, y and z axes.</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi/trilinear interpolation (default) - \"nearest\" for nearest neighbour interpolation - \"bspline\" for order-3 b-spline interpolation</p> <code>'linear'</code> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate(rotation_centre)","title":"<code>rotation_centre</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate(angles)","title":"<code>angles</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Rotate.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        False by default. Subclasses should override this method\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.SpatialTransform","title":"SpatialTransform","text":"<p>               Bases: <code>imgtools.transforms.base_transform.BaseTransform</code></p> <p>Base class for spatial transforms.</p> <p>Spatial transforms modify the spatial properties of an image, such as spacing, size, or orientation.</p> <p>All spatial transforms support an optional reference image parameter in their call method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # This is an abstract base class and cannot be instantiated directly.\n&gt;&gt;&gt; # Use one of its subclasses like Resample, Resize, etc.\n</code></pre> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.SpatialTransform.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.SpatialTransform.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        False by default. Subclasses should override this method\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom","title":"Zoom  <code>dataclass</code>","text":"<pre><code>Zoom(\n    scale_factor: float | list[float],\n    interpolation: str = \"linear\",\n    anti_alias: bool = True,\n    anti_alias_sigma: float | None = None,\n)\n</code></pre> <p>               Bases: <code>imgtools.transforms.spatial_transforms.SpatialTransform</code></p> <p>Zoom operation class.</p> <p>A callable class that rescales image, preserving its spatial extent.</p> <p>The rescaled image will have the same spatial extent (size) but will be rescaled by <code>scale_factor</code> in each dimension. Alternatively, a separate scale factor for each dimension can be specified by passing a sequence of floats.</p> <p>Parameters:</p> Name Type Description Default <code>float | list[float]</code> <p>If float, each dimension will be scaled by that factor. If tuple, each dimension will be scaled by the corresponding element.</p> required <code>str</code> <p>The interpolation method to use. Valid options are: - \"linear\" for bi/trilinear interpolation (default) - \"nearest\" for nearest neighbour interpolation - \"bspline\" for order-3 b-spline interpolation</p> <code>'linear'</code> <code>bool</code> <p>Whether to smooth the image with a Gaussian kernel before resampling. Only used when downsampling, i.e. when <code>size &lt; image.GetSize()</code>. This should be used to avoid aliasing artifacts.</p> <code>True</code> <code>float | None</code> <p>The standard deviation of the Gaussian kernel used for anti-aliasing.</p> <code>None</code> <p>Methods:</p> Name Description <code>supports_reference</code> <p>Return whether this transform supports reference images.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom(scale_factor)","title":"<code>scale_factor</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom(interpolation)","title":"<code>interpolation</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom(anti_alias)","title":"<code>anti_alias</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom(anti_alias_sigma)","title":"<code>anti_alias_sigma</code>","text":""},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the transform class for logging and debugging.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the transform class.</p>"},{"location":"reference/transforms/spatial_transforms/#imgtools.transforms.spatial_transforms.Zoom.supports_reference","title":"supports_reference","text":"<pre><code>supports_reference() -&gt; bool\n</code></pre> <p>Return whether this transform supports reference images.</p> <p>Returns:</p> Type Description <code>bool</code> <p>False by default. Subclasses should override this method</p> Source code in <code>src/imgtools/transforms/spatial_transforms.py</code> <pre><code>def supports_reference(self) -&gt; bool:\n    \"\"\"Return whether this transform supports reference images.\n\n    Returns\n    -------\n    bool\n        False by default. Subclasses should override this method\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/transforms/transformer/","title":"Transformer","text":""},{"location":"reference/transforms/transformer/#imgtools.transforms.transformer","title":"transformer","text":""},{"location":"reference/transforms/transformer/#imgtools.transforms.transformer.Transformer","title":"Transformer  <code>dataclass</code>","text":"<pre><code>Transformer(\n    transforms: typing.Sequence[\n        imgtools.transforms.BaseTransform\n    ],\n)\n</code></pre> <p>               Bases: <code>typing.Generic[imgtools.transforms.transformer.T_MedImage]</code></p>"},{"location":"reference/utils/date_time/","title":"Date time","text":""},{"location":"reference/utils/date_time/#imgtools.utils.date_time","title":"date_time","text":"<p>Functions:</p> Name Description <code>convert_dictionary_datetime_values</code> <p>Convert date/time/duration strings in a DICOM metadata dictionary.</p> <code>datetime_to_iso_string</code> <p>Convert datetime/date/time to an ISO 8601 string with second precision.</p> <code>parse_datetime</code> <p>Parse date/time or duration values from keyword/value pairs.</p>"},{"location":"reference/utils/date_time/#imgtools.utils.date_time.convert_dictionary_datetime_values","title":"convert_dictionary_datetime_values","text":"<pre><code>convert_dictionary_datetime_values(\n    dicom_dict: dict[str, str],\n) -&gt; dict[str, datetime.date | datetime.time | float | str]\n</code></pre> <p>Convert date/time/duration strings in a DICOM metadata dictionary.</p> <p>This function iterates over all key-value pairs in a dictionary and attempts to convert strings that appear to represent date, time, or duration fields into corresponding Python types (<code>date</code>, <code>time</code>, or <code>float</code>). Keys that do not match temporal patterns are returned unchanged with their original string values.</p> <p>Parameters:</p> Name Type Description Default <code>dict[str, str]</code> <pre><code>A dictionary containing DICOM metadata with string values.\n</code></pre> required <p>Returns:</p> Type Description <code>dict[str, datetime.date | datetime.time | float | str]</code> <p>A new dictionary where values for date/time/duration fields are parsed into native Python types when possible. All keys are retained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dicom_dict = {\n...     \"AcquisitionDate\": \"20240101\",\n...     \"EchoTime\": \"45.0\",\n...     \"InstanceNumber\": \"5\",\n... }\n&gt;&gt;&gt; convert_dictionary_datetime_values(dicom_dict)\n{\n    'AcquisitionDate': datetime.date(2024, 1, 1),\n    'EchoTime': 45.0,\n    'InstanceNumber': '5'\n}\n</code></pre> Source code in <code>src/imgtools/utils/date_time.py</code> <pre><code>def convert_dictionary_datetime_values(\n    dicom_dict: dict[str, str],\n) -&gt; dict[str, date | time | float | str]:\n    \"\"\"\n    Convert date/time/duration strings in a DICOM metadata dictionary.\n\n    This function iterates over all key-value pairs in a dictionary and attempts\n    to convert strings that appear to represent date, time, or duration fields into\n    corresponding Python types (`date`, `time`, or `float`). Keys that do not match\n    temporal patterns are returned unchanged with their original string values.\n\n    Parameters\n    ----------\n    dicom_dict : dict[str, str]\n            A dictionary containing DICOM metadata with string values.\n\n    Returns\n    -------\n    dict[str, date | time | float | str]\n            A new dictionary where values for date/time/duration fields are parsed\n            into native Python types when possible. All keys are retained.\n\n    Examples\n    --------\n    &gt;&gt;&gt; dicom_dict = {\n    ...     \"AcquisitionDate\": \"20240101\",\n    ...     \"EchoTime\": \"45.0\",\n    ...     \"InstanceNumber\": \"5\",\n    ... }\n    &gt;&gt;&gt; convert_dictionary_datetime_values(dicom_dict)\n    {\n        'AcquisitionDate': datetime.date(2024, 1, 1),\n        'EchoTime': 45.0,\n        'InstanceNumber': '5'\n    }\n    \"\"\"\n    result: dict[str, date | time | float | str] = {}\n\n    for key, value in dicom_dict.items():\n        if not isinstance(value, str):\n            # Skip non-string values\n            result[key] = value\n            continue\n        if value and value.lower() != \"none\":\n            key_lower = key.lower()\n            if (\n                \"date\" in key_lower\n                or \"time\" in key_lower\n                or \"duration\" in key_lower\n            ):\n                try:\n                    result[key] = parse_datetime(key, value)\n                    continue\n                except Exception:\n                    pass  # fall through to keep raw value\n\n        result[key] = value\n\n    return result\n</code></pre>"},{"location":"reference/utils/date_time/#imgtools.utils.date_time.convert_dictionary_datetime_values(dicom_dict)","title":"<code>dicom_dict</code>","text":""},{"location":"reference/utils/date_time/#imgtools.utils.date_time.datetime_to_iso_string","title":"datetime_to_iso_string","text":"<pre><code>datetime_to_iso_string(\n    datetime_obj: (\n        datetime.datetime | datetime.date | datetime.time\n    ),\n) -&gt; str\n</code></pre> <p>Convert datetime/date/time to an ISO 8601 string with second precision.</p> <p>Parameters:</p> Name Type Description Default <code>datetime.datetime.datetime or datetime.datetime.date or datetime.datetime.time</code> <p>The datetime, date, or time object to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted ISO 8601 string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; datetime_to_iso_string(\n...     datetime.datetime(2024, 1, 1, 12, 0, 0)\n... )\n'2024-01-01T12:00:00'\n&gt;&gt;&gt; datetime_to_iso_string(datetime.date(2024, 1, 1))\n'2024-01-01'\n&gt;&gt;&gt; datetime_to_iso_string(datetime.time(12, 0, 0))\n'12:00:00'\n</code></pre> Source code in <code>src/imgtools/utils/date_time.py</code> <pre><code>def datetime_to_iso_string(\n    datetime_obj: datetime | date | time,\n) -&gt; str:\n    \"\"\"Convert datetime/date/time to an ISO 8601 string with second precision.\n\n    Parameters\n    ----------\n    datetime_obj : datetime.datetime or datetime.date or datetime.time\n        The datetime, date, or time object to convert.\n\n    Returns\n    -------\n    str\n        Formatted ISO 8601 string.\n\n    Raises\n    ------\n    TypeError\n        If 'datetime_obj' is not a datetime, date, or time object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; datetime_to_iso_string(\n    ...     datetime.datetime(2024, 1, 1, 12, 0, 0)\n    ... )\n    '2024-01-01T12:00:00'\n    &gt;&gt;&gt; datetime_to_iso_string(datetime.date(2024, 1, 1))\n    '2024-01-01'\n    &gt;&gt;&gt; datetime_to_iso_string(datetime.time(12, 0, 0))\n    '12:00:00'\n    \"\"\"\n    if isinstance(datetime_obj, datetime):\n        return datetime_obj.isoformat(timespec=\"seconds\")\n    if isinstance(datetime_obj, date):\n        return datetime_obj.isoformat()\n    if isinstance(datetime_obj, time):\n        return datetime_obj.isoformat(timespec=\"seconds\")\n    raise TypeError(\"Expected a datetime, date, or time object.\")\n</code></pre>"},{"location":"reference/utils/date_time/#imgtools.utils.date_time.datetime_to_iso_string(datetime_obj)","title":"<code>datetime_obj</code>","text":""},{"location":"reference/utils/date_time/#imgtools.utils.date_time.parse_datetime","title":"parse_datetime","text":"<pre><code>parse_datetime(\n    key: str, value: str\n) -&gt; datetime.date | datetime.time | float\n</code></pre> <p>Parse date/time or duration values from keyword/value pairs.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The name of the DICOM field (e.g., \"AcquisitionTime\", \"EchoTime\").</p> required <code>str</code> <p>The associated value to be parsed.</p> required <p>Returns:</p> Type Description <code>datetime.date or datetime.time or float</code> <p>Depending on the field type, returns either a Python date, time, or a float representing a duration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_datetime(\"EchoTime\", \"45.0\")\n45.0\n&gt;&gt;&gt; parse_datetime(\"AcquisitionDate\", \"20240101\")\ndatetime.date(2024, 1, 1)\n&gt;&gt;&gt; parse_datetime(\"AcquisitionTime\", \"230000\")\ndatetime.time(23, 0)\n</code></pre> Source code in <code>src/imgtools/utils/date_time.py</code> <pre><code>def parse_datetime(key: str, value: str) -&gt; date | time | float:\n    \"\"\"\n    Parse date/time or duration values from keyword/value pairs.\n\n    Parameters\n    ----------\n    key : str\n        The name of the DICOM field (e.g., \"AcquisitionTime\", \"EchoTime\").\n    value : str\n        The associated value to be parsed.\n\n    Returns\n    -------\n    date or time or float\n        Depending on the field type, returns either a Python date, time,\n        or a float representing a duration.\n\n    Raises\n    ------\n    ValueError\n        If the value cannot be parsed correctly for the given field type.\n    TypeError\n        If the parsed result is not a date or time object, when such an\n        object is expected.\n\n    Examples\n    --------\n    &gt;&gt;&gt; parse_datetime(\"EchoTime\", \"45.0\")\n    45.0\n    &gt;&gt;&gt; parse_datetime(\"AcquisitionDate\", \"20240101\")\n    datetime.date(2024, 1, 1)\n    &gt;&gt;&gt; parse_datetime(\"AcquisitionTime\", \"230000\")\n    datetime.time(23, 0)\n    \"\"\"\n    # duration fields \u2192 parse as float\n    DURATION_FIELDS = {  # noqa: N806\n        \"EchoTime\",\n        \"RepetitionTime\",\n        \"InversionTime\",\n        \"FrameReferenceTime\",\n        \"FrameTime\",\n        \"AcquisitionDuration\",\n        \"ExposureTime\",\n    }\n\n    # clock-style time fields \u2192 parse as HHMMSS\n    CLOCK_FIELDS = {  # noqa: N806\n        \"AcquisitionTime\",\n        \"ContentTime\",\n        \"InstanceCreationTime\",\n        \"SeriesTime\",\n        \"StudyTime\",\n        \"StructureSetTime\",\n    }\n\n    if not value or value.lower() == \"none\":\n        # msg = f\"Empty or 'None' value for key: {key}\"\n        # return 0 as default for empty values\n        return 0\n    key_base = str(key)\n\n    # Check if this is a duration field first (highest priority)\n    if key_base in DURATION_FIELDS:\n        try:\n            # should return datetime.time as a duration\n            return float(value)\n        except ValueError as e:\n            msg = f\"Failed to parse duration field {key}='{value}' as float\"\n            raise ValueError(msg) from e\n\n    # Next, check if this is a clock-style time field\n    if key_base in CLOCK_FIELDS:\n        ok, result = parse_dicom_time(value)\n    elif \"Date\" in key_base:\n        ok, result = parse_dicom_date(value)\n    elif \"Time\" in key_base:\n        ok, result = parse_dicom_time(value)\n    else:\n        # For unknown keys, try datetime parsing as fallback\n        try:\n            assert value.isdigit(), (\n                f\"Non-digit characters in date/time: {value}\"\n            )\n            ok, result = parse_dicom_date(value)\n        except (ValueError, AssertionError) as e:\n            msg = f\"Unknown DICOM key: '{key}'\"\n            raise ValueError(msg) from e\n\n    if not ok:\n        msg = f\"Failed to parse {key}='{value}': {result}\"\n        raise ValueError(msg)\n\n    if not isinstance(result, (date, time)):\n        msg = f\"Expected date or time object, got {type(result)}\"\n        raise TypeError(msg)\n\n    return result\n</code></pre>"},{"location":"reference/utils/date_time/#imgtools.utils.date_time.parse_datetime(key)","title":"<code>key</code>","text":""},{"location":"reference/utils/date_time/#imgtools.utils.date_time.parse_datetime(value)","title":"<code>value</code>","text":""},{"location":"reference/utils/dictionaries/","title":"Dictionaries","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries","title":"dictionaries","text":"<p>Dict utility functions and classes supporting attribute access and dot-notation.</p> <p>This module provides tools for working with dictionaries that allow:     - Attribute-style access to dictionary keys     - Conversion between nested and flattened (dot-notated) dictionaries     - Safe access to nested dictionary or object fields     - Recursive data cleaning of metadata dictionaries</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = AttrDict({\"a\": {\"b\": 1}})\n&gt;&gt;&gt; d.a.b\n1\n</code></pre> <pre><code>&gt;&gt;&gt; flatten_dictionary({\"a\": {\"b\": 1}})\n{'a.b': 1}\n</code></pre> <pre><code>&gt;&gt;&gt; expand_dictionary({\"a.b\": 1})\n{'a': {'b': 1}}\n</code></pre> <p>Functions:</p> Name Description <code>cleanse_metadata</code> <p>Recursively cleanse metadata dictionaries for serialization.</p> <code>expand_dictionary</code> <p>Expand dot-notated keys into a nested dictionary.</p> <code>flatten_dictionary</code> <p>Flatten a nested dictionary using dot-notation keys.</p> <code>retrieve_nested_value</code> <p>Retrieve a value or attribute using a dot-notation path.</p>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.AttrDict","title":"AttrDict","text":"<pre><code>AttrDict(*args: typing.Any, **kwargs: typing.Any)\n</code></pre> <p>               Bases: <code>dict</code></p> <p>A dictionary that supports dot-style attribute access and nested utilities.</p> <p>This class extends the built-in <code>dict</code> to enable accessing keys as attributes and includes helpers to flatten and expand nested structures.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = AttrDict({\"x\": {\"y\": 5}})\n&gt;&gt;&gt; d.x.y\n5\n</code></pre> <pre><code>&gt;&gt;&gt; flat = d.to_flat_dict()\n&gt;&gt;&gt; flat\n{'x.y': 5}\n</code></pre> <pre><code>&gt;&gt;&gt; AttrDict.from_flat_dict({\"x.y\": 5})\n{'x': {'y': 5}}\n</code></pre> <p>Methods:</p> Name Description <code>from_flat_dict</code> <p>Inflate a flat dict into a nested AttrDict.</p> <code>to_flat_dict</code> <p>Return a flattened dictionary using dot-notation keys.</p> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    # if empty, initialize with an empty dict\n    if not args and not kwargs:\n        super().__init__()\n        return\n\n    super().__init__(\n        {k: attrify(v) for k, v in dict(*args, **kwargs).items()}\n    )\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.AttrDict.from_flat_dict","title":"from_flat_dict  <code>classmethod</code>","text":"<pre><code>from_flat_dict(\n    *args: typing.Any, **kwargs: typing.Any\n) -&gt; imgtools.utils.dictionaries.AttrDict\n</code></pre> <p>Inflate a flat dict into a nested AttrDict.</p> Example <p>AttrDict.from_flat_dict({\"a.b\": 1}) {'a': {'b': 1}}</p> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>@classmethod\ndef from_flat_dict(cls, *args: Any, **kwargs: Any) -&gt; AttrDict:\n    \"\"\"Inflate a flat dict into a nested AttrDict.\n\n    Example\n    -------\n    &gt;&gt;&gt; AttrDict.from_flat_dict({\"a.b\": 1})\n    {'a': {'b': 1}}\n    \"\"\"\n    return cls(expand_dictionary(dict(*args, **kwargs)))\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.AttrDict.to_flat_dict","title":"to_flat_dict","text":"<pre><code>to_flat_dict() -&gt; dict[str, typing.Any]\n</code></pre> <p>Return a flattened dictionary using dot-notation keys.</p> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def to_flat_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Return a flattened dictionary using dot-notation keys.\"\"\"\n    return flatten_dictionary(self)\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.attrify","title":"attrify","text":"<pre><code>attrify(data: typing.Any) -&gt; typing.Any\n</code></pre> <p>Recursively convert dicts to AttrDict and handle lists of dicts as well.</p> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def attrify(data: Any) -&gt; Any:\n    \"\"\"Recursively convert dicts to AttrDict and handle lists of dicts as well.\"\"\"\n    if isinstance(data, dict):\n        return AttrDict(data)\n    if isinstance(data, list):\n        return [attrify(elem) for elem in data]\n    return data\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.cleanse_metadata","title":"cleanse_metadata","text":"<pre><code>cleanse_metadata(metadata: typing.Any) -&gt; typing.Any\n</code></pre> <p>Recursively cleanse metadata dictionaries for serialization.</p> <p>Fixes applied:     1. Converts NaN values to None.     2. Cleans nested dictionaries and iterables.     3. Converts datetime.{datetime,date,time} to ISO format strings.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>The input dictionary, list, or primitive.</p> required <p>Returns:</p> Type Description <code>typing.Any</code> <p>The cleansed version of the input.</p> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def cleanse_metadata(metadata: Any) -&gt; Any:\n    \"\"\"Recursively cleanse metadata dictionaries for serialization.\n\n    Fixes applied:\n        1. Converts NaN values to None.\n        2. Cleans nested dictionaries and iterables.\n        3. Converts datetime.{datetime,date,time} to ISO format strings.\n\n    Parameters\n    ----------\n    metadata : Any\n        The input dictionary, list, or primitive.\n\n    Returns\n    -------\n    Any\n        The cleansed version of the input.\n    \"\"\"\n\n    match metadata:\n        case dict():\n            return {k: cleanse_metadata(v) for k, v in metadata.items()}\n        case collections.abc.Iterable() if not isinstance(\n            metadata, (str, bytes)\n        ):\n            return [cleanse_metadata(v) for v in metadata]\n        case float() if math.isnan(metadata):\n            return None\n        case datetime.datetime() | datetime.date() | datetime.time():\n            return datetime_to_iso_string(metadata)\n        case _:\n            return metadata\n\n    return metadata\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.cleanse_metadata(metadata)","title":"<code>metadata</code>","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.expand_dictionary","title":"expand_dictionary","text":"<pre><code>expand_dictionary(\n    flat_dict: dict[str, typing.Any],\n) -&gt; dict[str, typing.Any]\n</code></pre> <p>Expand dot-notated keys into a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>Dictionary with keys in dot-notation.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Nested dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expand_dictionary({\"a.b\": 1})\n{'a': {'b': 1}}\n</code></pre> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def expand_dictionary(flat_dict: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Expand dot-notated keys into a nested dictionary.\n\n    Parameters\n    ----------\n    flat_dict : dict\n        Dictionary with keys in dot-notation.\n\n    Returns\n    -------\n    dict\n        Nested dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; expand_dictionary({\"a.b\": 1})\n    {'a': {'b': 1}}\n    \"\"\"\n    nested_dict: dict[str, Any] = {}\n    for key, value in flat_dict.items():\n        parts = key.split(\".\")\n        node = nested_dict\n        for part in parts[:-1]:\n            node = node.setdefault(part, {})\n        node[parts[-1]] = value\n    return nested_dict\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.expand_dictionary(flat_dict)","title":"<code>flat_dict</code>","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.flatten_dictionary","title":"flatten_dictionary","text":"<pre><code>flatten_dictionary(\n    nested_dict: dict[str, typing.Any],\n    parent_key_prefix: str = \"\",\n) -&gt; dict[str, typing.Any]\n</code></pre> <p>Flatten a nested dictionary using dot-notation keys.</p> <p>Parameters:</p> Name Type Description Default <code>dict</code> <p>The nested dictionary to flatten.</p> required <code>str</code> <p>Prefix for internal recursive use.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>Flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; flatten_dictionary({\"a\": {\"b\": 1}})\n{'a.b': 1}\n</code></pre> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def flatten_dictionary(\n    nested_dict: dict[str, Any], parent_key_prefix: str = \"\"\n) -&gt; dict[str, Any]:\n    \"\"\"Flatten a nested dictionary using dot-notation keys.\n\n    Parameters\n    ----------\n    nested_dict : dict\n        The nested dictionary to flatten.\n    parent_key_prefix : str, optional\n        Prefix for internal recursive use.\n\n    Returns\n    -------\n    dict\n        Flattened dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; flatten_dictionary({\"a\": {\"b\": 1}})\n    {'a.b': 1}\n    \"\"\"\n    flat_dict: dict[str, Any] = {}\n    for key, value in nested_dict.items():\n        full_key = f\"{parent_key_prefix}.{key}\" if parent_key_prefix else key\n        if isinstance(value, dict):\n            flat_dict.update(\n                flatten_dictionary(value, parent_key_prefix=full_key)\n            )\n        else:\n            flat_dict[full_key] = value\n    return flat_dict\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.flatten_dictionary(nested_dict)","title":"<code>nested_dict</code>","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.flatten_dictionary(parent_key_prefix)","title":"<code>parent_key_prefix</code>","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.retrieve_nested_value","title":"retrieve_nested_value","text":"<pre><code>retrieve_nested_value(\n    container: typing.Any, field_path: str\n) -&gt; typing.Any | None\n</code></pre> <p>Retrieve a value or attribute using a dot-notation path.</p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>The object or dictionary to access.</p> required <code>str</code> <p>Dot-notation string path.</p> required <p>Returns:</p> Type Description <code>typing.Any or None</code> <p>The resolved value or None if not found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; retrieve_nested_value({\"a\": {\"b\": 1}}, \"a.b\")\n1\n</code></pre> Source code in <code>src/imgtools/utils/dictionaries.py</code> <pre><code>def retrieve_nested_value(container: Any, field_path: str) -&gt; Any | None:\n    \"\"\"Retrieve a value or attribute using a dot-notation path.\n\n    Parameters\n    ----------\n    container : Any\n        The object or dictionary to access.\n    field_path : str\n        Dot-notation string path.\n\n    Returns\n    -------\n    Any or None\n        The resolved value or None if not found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; retrieve_nested_value({\"a\": {\"b\": 1}}, \"a.b\")\n    1\n    \"\"\"\n    try:\n        return container[field_path]\n    except (TypeError, KeyError):\n        pass\n    try:\n        return getattr(container, field_path)\n    except AttributeError as exc:\n        if f\"object has no attribute {field_path!r}\" not in str(exc):\n            raise\n\n    node = container\n    for part in field_path.split(\".\"):\n        try:\n            node = node[part]\n            continue\n        except (TypeError, KeyError):\n            pass\n        try:\n            node = getattr(node, part)\n            continue\n        except AttributeError as exc:\n            if f\"object has no attribute {part!r}\" not in str(exc):\n                raise\n            return None\n    return node\n</code></pre>"},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.retrieve_nested_value(container)","title":"<code>container</code>","text":""},{"location":"reference/utils/dictionaries/#imgtools.utils.dictionaries.retrieve_nested_value(field_path)","title":"<code>field_path</code>","text":""},{"location":"reference/utils/imageutils/","title":"Imageutils","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils","title":"imageutils","text":"<p>Functions:</p> Name Description <code>array_to_image</code> <p>Convert a numpy array to a SimpleITK image with optional metadata.</p> <code>idxs_to_physical_points</code> <p>Converts image indices to physical points based on the reference image's</p> <code>image_to_array</code> <p>Convert a SimpleITK image to a numpy array along with its metadata.</p> <code>physical_points_to_idxs</code> <p>Convert physical points to image indices based on the reference image's</p>"},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image","title":"array_to_image","text":"<pre><code>array_to_image(\n    array: numpy.ndarray,\n    origin: imgtools.utils.imageutils.Array3D = (\n        0.0,\n        0.0,\n        0.0,\n    ),\n    direction: typing.Tuple[float, ...] = (\n        1.0,\n        0.0,\n        0.0,\n        0.0,\n        1.0,\n        0.0,\n        0.0,\n        0.0,\n        1.0,\n    ),\n    spacing: imgtools.utils.imageutils.Array3D = (\n        1.0,\n        1.0,\n        1.0,\n    ),\n    reference_image: SimpleITK.Image | None = None,\n) -&gt; SimpleITK.Image\n</code></pre> <p>Convert a numpy array to a SimpleITK image with optional metadata.</p> <p>Parameters:</p> Name Type Description Default <code>numpy.ndarray</code> <p>The numpy array to convert.</p> required <code>imgtools.utils.imageutils.Array3D</code> <p>The origin of the image (default is (0.0, 0.0, 0.0)).</p> <code>(0.0, 0.0, 0.0)</code> <code>typing.Tuple[float, ...]</code> <p>The direction cosines of the image (default is identity matrix).</p> <code>(1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)</code> <code>imgtools.utils.imageutils.Array3D</code> <p>The pixel spacing of the image (default is (1.0, 1.0, 1.0)).</p> <code>(1.0, 1.0, 1.0)</code> <code>SimpleITK.Image</code> <p>A reference SimpleITK image to copy metadata from (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>SimpleITK.Image</code> <p>The resulting SimpleITK image.</p> Source code in <code>src/imgtools/utils/imageutils.py</code> <pre><code>def array_to_image(\n    array: np.ndarray,\n    origin: Array3D = (0.0, 0.0, 0.0),\n    direction: Tuple[float, ...] = (\n        1.0,\n        0.0,\n        0.0,\n        0.0,\n        1.0,\n        0.0,\n        0.0,\n        0.0,\n        1.0,\n    ),\n    spacing: Array3D = (1.0, 1.0, 1.0),\n    reference_image: sitk.Image | None = None,\n) -&gt; sitk.Image:\n    \"\"\"Convert a numpy array to a SimpleITK image with optional metadata.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        The numpy array to convert.\n    origin : Array3D, optional\n        The origin of the image (default is (0.0, 0.0, 0.0)).\n    direction : Tuple[float, ...], optional\n        The direction cosines of the image (default is identity matrix).\n    spacing : Array3D, optional\n        The pixel spacing of the image (default is (1.0, 1.0, 1.0)).\n    reference_image : sitk.Image, optional\n        A reference SimpleITK image to copy metadata from (default is None).\n\n    Returns\n    -------\n    sitk.Image\n        The resulting SimpleITK image.\n    \"\"\"\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        image.CopyInformation(reference_image)\n    elif all(x is not None for x in (origin, direction, spacing)):\n        image.SetOrigin(origin)\n        image.SetDirection(direction)\n        image.SetSpacing(spacing)\n    else:\n        errmsg = (\n            \"Either a reference image or all of the origin, direction, and spacing \"\n            \"must be provided to create a new image.\"\n        )\n        raise ValueError(errmsg)\n\n    return image\n</code></pre>"},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image(array)","title":"<code>array</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image(origin)","title":"<code>origin</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image(direction)","title":"<code>direction</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image(spacing)","title":"<code>spacing</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.array_to_image(reference_image)","title":"<code>reference_image</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.idxs_to_physical_points","title":"idxs_to_physical_points","text":"<pre><code>idxs_to_physical_points(\n    image: SimpleITK.Image, idxs: numpy.ndarray\n) -&gt; numpy.ndarray\n</code></pre> <p>Converts image indices to physical points based on the reference image's geometry.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The reference SimpleITK image.</p> required <code>numpy.ndarray</code> <p>Array of 3D indices (continuous or discrete).</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Physical coordinates corresponding to the given indices.</p> Source code in <code>src/imgtools/utils/imageutils.py</code> <pre><code>def idxs_to_physical_points(image: sitk.Image, idxs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts image indices to physical points based on the reference image's\n    geometry.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The reference SimpleITK image.\n    idxs : np.ndarray\n        Array of 3D indices (continuous or discrete).\n\n    Returns\n    -------\n    np.ndarray\n        Physical coordinates corresponding to the given indices.\n    \"\"\"\n    continuous = np.issubdtype(idxs.dtype, np.floating)\n\n    # TransformIndexToPhysicalPoint expects a list not a numpy array\n\n    transform = (\n        image.TransformContinuousIndexToPhysicalPoint\n        if continuous\n        else image.TransformIndexToPhysicalPoint\n    )\n    # vectorized_transform = np.vectorize(\n    #     lambda x: np.array(transform(x)), signature=\"(3)-&gt;(3)\"\n    # )\n    # return vectorized_transform(idxs)\n\n    # Convert indices to lists of lists and apply the transformation\n    idxs_list = idxs.tolist()\n    return np.array([transform(idx) for idx in idxs_list])\n</code></pre>"},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.idxs_to_physical_points(image)","title":"<code>image</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.idxs_to_physical_points(idxs)","title":"<code>idxs</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.image_to_array","title":"image_to_array","text":"<pre><code>image_to_array(\n    image: SimpleITK.Image,\n) -&gt; imgtools.utils.imageutils.ImageArrayMetadata\n</code></pre> <p>Convert a SimpleITK image to a numpy array along with its metadata.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The SimpleITK image to convert.</p> required <p>Returns:</p> Name Type Description <code>ImageArrayMetadata</code> <code>typing.Tuple[numpy.ndarray, imgtools.utils.imageutils.Array3D, imgtools.utils.imageutils.Array3D, imgtools.utils.imageutils.Array3D]</code> <p>A tuple containing: - The image as a numpy array. - The origin of the image (tuple of floats). - The direction cosines of the image (tuple of floats). - The pixel spacing of the image (tuple of floats).</p> Source code in <code>src/imgtools/utils/imageutils.py</code> <pre><code>def image_to_array(image: sitk.Image) -&gt; ImageArrayMetadata:\n    \"\"\"Convert a SimpleITK image to a numpy array along with its metadata.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The SimpleITK image to convert.\n\n    Returns\n    -------\n    ImageArrayMetadata : Tuple[np.ndarray, Array3D, Array3D, Array3D]\n        A tuple containing:\n        - The image as a numpy array.\n        - The origin of the image (tuple of floats).\n        - The direction cosines of the image (tuple of floats).\n        - The pixel spacing of the image (tuple of floats).\n    \"\"\"\n    array: np.ndarray = sitk.GetArrayFromImage(image)\n    return array, image[\"origin\"], image[\"direction\"], image[\"spacing\"]\n</code></pre>"},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.image_to_array(image)","title":"<code>image</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.physical_points_to_idxs","title":"physical_points_to_idxs","text":"<pre><code>physical_points_to_idxs(\n    image: SimpleITK.Image,\n    points: typing.List[numpy.ndarray],\n    continuous: bool = False,\n) -&gt; typing.List[numpy.ndarray]\n</code></pre> <p>Convert physical points to image indices based on the reference image's geometry.</p> <p>This function uses the geometry of a SimpleITK image (origin, spacing, direction) to convert real-world physical coordinates into indices in the image grid. It optionally supports continuous indices for sub-pixel precision.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>The reference SimpleITK image.</p> required <code>typing.List[numpy.ndarray]</code> <p>List of 3D physical points to transform.</p> required <code>bool</code> <p>If True, returns continuous indices; otherwise, returns integer indices. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>typing.List[numpy.ndarray]</code> <p>A list of transformed points in image index space, reversed to match library conventions.</p> Notes <p>The following steps occur within the function: 1. A <code>numpy.vectorize</code> function is defined to apply the transformation    method (physical to index) to each 3D point in the input array. 2. The transformation is applied to each set of points in the list,    reversing the coordinate order to match the library's indexing    convention.</p> Source code in <code>src/imgtools/utils/imageutils.py</code> <pre><code>def physical_points_to_idxs(\n    image: sitk.Image,\n    points: List[np.ndarray],\n    continuous: bool = False,\n) -&gt; List[np.ndarray]:\n    \"\"\"Convert physical points to image indices based on the reference image's\n    geometry.\n\n    This function uses the geometry of a SimpleITK image (origin, spacing,\n    direction) to convert real-world physical coordinates into indices in the\n    image grid. It optionally supports continuous indices for sub-pixel\n    precision.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        The reference SimpleITK image.\n    points : List[np.ndarray]\n        List of 3D physical points to transform.\n    continuous : bool, optional\n        If True, returns continuous indices; otherwise, returns integer indices.\n        Default is False.\n\n    Returns\n    -------\n    List[np.ndarray]\n        A list of transformed points in image index space, reversed to match\n        library conventions.\n\n    Notes\n    -----\n    The following steps occur within the function:\n    1. A `numpy.vectorize` function is defined to apply the transformation\n       method (physical to index) to each 3D point in the input array.\n    2. The transformation is applied to each set of points in the list,\n       reversing the coordinate order to match the library's indexing\n       convention.\n    \"\"\"\n    # Select the appropriate transformation function based on the `continuous` parameter.\n    transform = (\n        image.TransformPhysicalPointToContinuousIndex\n        if continuous\n        else image.TransformPhysicalPointToIndex\n    )\n\n    # Step 1: Define a vectorized transformation function\n    # The lambda function takes a single 3D point `x` and:\n    # - Applies the selected transformation (`transform(x)`) to convert it from physical space to index space.\n    # - Wraps the result into a numpy array for further processing.\n    # `np.vectorize` creates a vectorized function that can process arrays of points in one call.\n    # The `signature=\"(3)-&gt;(3)\"` ensures the transformation operates on 3D points, returning 3D results.\n    vectorized_transform = np.vectorize(\n        lambda x: np.array(transform(x)), signature=\"(3)-&gt;(3)\"\n    )\n\n    # Step 2: Apply the vectorized transformation to all slices of points.\n    # For each 2D array `slc` in the `points` list:\n    # - `vectorized_transform(slc)` applies the transformation to all points in `slc`.\n    # - `[:, ::-1]` reverses the coordinate order (from (x, y, z) to (z, y, x)) to match the library's convention.\n    # The result is stored as a list of numpy arrays (`t_points`), each corresponding to a transformed slice.\n    t_points: List[np.ndarray] = [\n        vectorized_transform(slc)[:, ::-1] for slc in points\n    ]\n\n    # Return the list of transformed points.\n    return t_points\n</code></pre>"},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.physical_points_to_idxs(image)","title":"<code>image</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.physical_points_to_idxs(points)","title":"<code>points</code>","text":""},{"location":"reference/utils/imageutils/#imgtools.utils.imageutils.physical_points_to_idxs(continuous)","title":"<code>continuous</code>","text":""},{"location":"reference/utils/interlacer_utils/","title":"Interlacer utils","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils","title":"interlacer_utils","text":"<p>Interlacer Utils Module</p> <p>This module implements functions utilized by the Interlacer module as well as the SeriesNode dataclass.</p> <p>Functions:</p> Name Description <code>visualize_forest</code> <p>Visualize the forest as an interactive network graph.</p>"},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.ModalityHighlighter","title":"ModalityHighlighter","text":"<p>               Bases: <code>rich.highlighter.RegexHighlighter</code></p> <p>Highlights DICOM modality tags using custom styles.</p>"},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode","title":"SeriesNode  <code>dataclass</code>","text":"<pre><code>SeriesNode(\n    SeriesInstanceUID: str,\n    Modality: str,\n    PatientID: str,\n    StudyInstanceUID: str,\n    folder: str,\n    ReferencedSeriesUID: str | None = None,\n    children: list[\n        imgtools.utils.interlacer_utils.SeriesNode\n    ] = list(),\n)\n</code></pre> <p>A node in the series tree representing a DICOM series.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The SeriesInstanceUID of this node</p> required <code>str</code> <p>The DICOM modality type</p> required <code>str</code> <p>The patient identifier</p> required <code>str</code> <p>The study instance identifier</p> required <code>str</code> <p>Path to the folder containing the DICOM files</p> required <code>str | None</code> <p>Series that this one references, if any</p> <code>None</code> <code>list[imgtools.utils.interlacer_utils.SeriesNode]</code> <p>Child nodes representing referenced series</p> <code>list()</code> <p>Methods:</p> Name Description <code>add_child</code> <p>Add SeriesNode to children</p>"},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(SeriesInstanceUID)","title":"<code>SeriesInstanceUID</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(Modality)","title":"<code>Modality</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(PatientID)","title":"<code>PatientID</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(StudyInstanceUID)","title":"<code>StudyInstanceUID</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(folder)","title":"<code>folder</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(ReferencedSeriesUID)","title":"<code>ReferencedSeriesUID</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode(children)","title":"<code>children</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.SeriesNode.add_child","title":"add_child","text":"<pre><code>add_child(\n    child_node: imgtools.utils.interlacer_utils.SeriesNode,\n) -&gt; None\n</code></pre> <p>Add SeriesNode to children</p> Source code in <code>src/imgtools/utils/interlacer_utils.py</code> <pre><code>def add_child(self, child_node: SeriesNode) -&gt; None:\n    \"\"\"Add SeriesNode to children\"\"\"\n    self.children.append(child_node)\n</code></pre>"},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.visualize_forest","title":"visualize_forest","text":"<pre><code>visualize_forest(\n    root_nodes: list[\n        imgtools.utils.interlacer_utils.SeriesNode\n    ],\n    save_path: str | pathlib.Path,\n) -&gt; pathlib.Path\n</code></pre> <p>Visualize the forest as an interactive network graph.</p> <p>Creates an HTML visualization showing nodes for each SeriesNode and edges for parent-child relationships.</p> <p>Parameters:</p> Name Type Description Default <code>list[imgtools.utils.interlacer_utils.SeriesNode]</code> <p>the root nodes of the tree.</p> required <code>str | pathlib.Path</code> <p>Path to save the HTML visualization.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Path to the saved HTML visualization</p> Source code in <code>src/imgtools/utils/interlacer_utils.py</code> <pre><code>def visualize_forest(\n    root_nodes: list[SeriesNode], save_path: str | Path\n) -&gt; Path:\n    \"\"\"\n    Visualize the forest as an interactive network graph.\n\n    Creates an HTML visualization showing nodes for each SeriesNode and\n    edges for parent-child relationships.\n\n    Parameters\n    ----------\n\n    root_nodes: list[SeriesNode]\n        the root nodes of the tree.\n\n    save_path : str | Path\n        Path to save the HTML visualization.\n\n    Returns\n    -------\n    Path\n        Path to the saved HTML visualization\n\n    Raises\n    ------\n    OptionalImportError\n        If pyvis package is not installed\n    \"\"\"\n    if not _pyvis_available:\n        raise OptionalImportError(\"pyvis\")\n\n    save_path = Path(save_path)\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    net = pyvis.network.Network(\n        height=\"800px\", width=\"100%\", notebook=False, directed=True\n    )\n\n    modality_colors = {\n        \"CT\": \"#1f77b4\",  # Blue\n        \"MR\": \"#ff7f0e\",  # Orange\n        \"PT\": \"#2ca02c\",  # Green\n        \"SEG\": \"#d62728\",  # Red\n        \"RTSTRUCT\": \"#9467bd\",  # Purple\n        \"RTPLAN\": \"#8c564b\",  # Brown\n        \"RTDOSE\": \"#e377c2\",  # Pink\n    }\n\n    patient_trees = {}  # Store patient-to-root mappings\n\n    def add_node_and_edges(\n        node: SeriesNode, parent: SeriesNode | None = None\n    ) -&gt; None:\n        color = modality_colors.get(\n            node.Modality, \"#7f7f7f\"\n        )  # Default gray if unknown\n        title = (\n            f\"PatientID: {node.PatientID}\\nSeries: {node.SeriesInstanceUID}\"\n        )\n        net.add_node(\n            node.SeriesInstanceUID,\n            label=node.Modality,\n            title=title,\n            color=color,\n        )\n        if parent:\n            net.add_edge(node.SeriesInstanceUID, parent.SeriesInstanceUID)\n\n        for child in node.children:\n            add_node_and_edges(child, node)\n\n    # Add root nodes (each representing a patient)\n    for root in root_nodes:\n        add_node_and_edges(root)\n        patient_trees[root.PatientID] = (\n            root.SeriesInstanceUID\n        )  # Store the root Series as entry point for the patient\n\n    net.force_atlas_2based()\n\n    # Generate the sidebar HTML with clickable patient IDs\n    sidebar_html = \"\"\"\n    &lt;div id=\"sidebar\"&gt;\n        &lt;h2&gt;Patient List&lt;/h2&gt;\n        &lt;ul&gt;\n    \"\"\"\n    for patient_id, root_series in patient_trees.items():\n        sidebar_html += f'&lt;li&gt;&lt;a href=\"#\" onclick=\"focusNode(\\'{root_series}\\')\"&gt;{patient_id}&lt;/a&gt;&lt;/li&gt;'\n\n    sidebar_html += \"\"\"\n        &lt;/ul&gt;\n    &lt;/div&gt;\n\n    &lt;style&gt;\n        body {\n            margin: 0;\n            padding: 0;\n        }\n\n        #sidebar {\n            position: fixed;\n            left: 0;\n            top: 0;\n            width: 250px;\n            height: 100%;\n            background: #f4f4f4;\n            padding: 20px;\n            overflow-y: auto;\n            box-shadow: 2px 0 5px rgba(0,0,0,0.3);\n            z-index: 1000;\n        }\n\n        #sidebar h2 {\n            text-align: center;\n            font-family: Arial, sans-serif;\n        }\n\n        #sidebar ul {\n            list-style: none;\n            padding: 0;\n        }\n\n        #sidebar li {\n            margin: 10px 0;\n        }\n\n        #sidebar a {\n            text-decoration: none;\n            color: #007bff;\n            font-weight: bold;\n            font-family: Arial, sans-serif;\n        }\n\n        #sidebar a:hover {\n            text-decoration: underline;\n        }\n\n        #mynetwork {\n            margin-left: 270px; /* Room for sidebar */\n            height: 100vh;\n        }\n    &lt;/style&gt;\n\n    &lt;script type=\"text/javascript\"&gt;\n        function focusNode(nodeId) {\n            if (typeof network !== 'undefined') {\n                network.selectNodes([nodeId]);\n                network.focus(nodeId, {\n                    scale: 3.5,\n                    animation: {\n                        duration: 500,\n                        easingFunction: \"easeInOutQuad\"\n                    }\n                });\n            } else {\n                alert(\"Network graph not loaded yet.\");\n            }\n        }\n    &lt;/script&gt;\n    \"\"\"\n\n    # Generate the full HTML file\n    logger.info(\"Saving forest visualization...\", path=save_path)\n    net_html = net.generate_html()\n    full_html = net_html.replace(\n        \"&lt;body&gt;\", f\"&lt;body&gt;{sidebar_html}\"\n    )  # Insert sidebar into HTML\n\n    # Write the final HTML file\n    save_path.write_text(full_html, encoding=\"utf-8\")\n\n    return save_path\n</code></pre>"},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.visualize_forest(root_nodes)","title":"<code>root_nodes</code>","text":""},{"location":"reference/utils/interlacer_utils/#imgtools.utils.interlacer_utils.visualize_forest(save_path)","title":"<code>save_path</code>","text":""},{"location":"reference/utils/nnunet/","title":"Nnunet","text":""},{"location":"reference/utils/nnunet/#imgtools.utils.nnunet","title":"nnunet","text":"<p>Functions:</p> Name Description <code>generate_dataset_json</code> <p>Generates a dataset.json file in the output folder</p> <code>generate_nnunet_scripts</code> <p>Creates two bash scripts:</p>"},{"location":"reference/utils/nnunet/#imgtools.utils.nnunet.generate_dataset_json","title":"generate_dataset_json","text":"<pre><code>generate_dataset_json(\n    output_folder: pathlib.Path | str,\n    channel_names: typing.Dict[str, str],\n    labels: typing.Dict[str, int | list[int]],\n    num_training_cases: int,\n    file_ending: str,\n    regions_class_order: (\n        typing.Tuple[int, ...] | None\n    ) = None,\n    dataset_name: str | None = None,\n    reference: str | None = None,\n    release: str | None = None,\n    usage_license: str = \"hands off!\",\n    description: str | None = None,\n    overwrite_image_reader_writer: str | None = None,\n    **kwargs: object\n) -&gt; None\n</code></pre> <p>Generates a dataset.json file in the output folder</p> <p>Code from: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/dataset_conversion/generate_dataset_json.py</p> <p>channel_names:     Channel names must map the index to the name of the channel, example:     {         0: 'T1',         1: 'CT'     }     Note that the channel names may influence the normalization scheme!! Learn more in the documentation.</p> <p>labels:     This will tell nnU-Net what labels to expect. Important: This will also determine whether you use region-based training or not.     Example regular labels:     {         'background': 0,         'left atrium': 1,         'some other label': 2     }     Example region-based training:     {         'background': 0,         'whole tumor': (1, 2, 3),         'tumor core': (2, 3),         'enhancing tumor': 3     }</p> <pre><code>Remember that nnU-Net expects consecutive values for labels! nnU-Net also expects 0 to be background!\n</code></pre> <p>num_training_cases: is used to double check all cases are there!</p> <p>file_ending: needed for finding the files correctly. IMPORTANT! File endings must match between images and segmentations!</p> <p>dataset_name, reference, release, license, description: self-explanatory and not used by nnU-Net. Just for completeness and as a reminder that these would be great!</p> <p>overwrite_image_reader_writer: If you need a special IO class for your dataset you can derive it from BaseReaderWriter, place it into nnunet.imageio and reference it here by name</p> <p>kwargs: whatever you put here will be placed in the dataset.json as well</p> Source code in <code>src/imgtools/utils/nnunet.py</code> <pre><code>def generate_dataset_json(\n    output_folder: pathlib.Path | str,\n    channel_names: Dict[str, str],\n    labels: Dict[str, int | list[int]],\n    num_training_cases: int,\n    file_ending: str,\n    regions_class_order: Tuple[int, ...] | None = None,\n    dataset_name: str | None = None,\n    reference: str | None = None,\n    release: str | None = None,\n    usage_license: str = \"hands off!\",\n    description: str | None = None,\n    overwrite_image_reader_writer: str | None = None,\n    **kwargs: object,\n) -&gt; None:\n    \"\"\"\n    Generates a dataset.json file in the output folder\n\n    Code from: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/dataset_conversion/generate_dataset_json.py\n\n    channel_names:\n        Channel names must map the index to the name of the channel, example:\n        {\n            0: 'T1',\n            1: 'CT'\n        }\n        Note that the channel names may influence the normalization scheme!! Learn more in the documentation.\n\n    labels:\n        This will tell nnU-Net what labels to expect. Important: This will also determine whether you use region-based training or not.\n        Example regular labels:\n        {\n            'background': 0,\n            'left atrium': 1,\n            'some other label': 2\n        }\n        Example region-based training:\n        {\n            'background': 0,\n            'whole tumor': (1, 2, 3),\n            'tumor core': (2, 3),\n            'enhancing tumor': 3\n        }\n\n        Remember that nnU-Net expects consecutive values for labels! nnU-Net also expects 0 to be background!\n\n    num_training_cases: is used to double check all cases are there!\n\n    file_ending: needed for finding the files correctly. IMPORTANT! File endings must match between images and\n    segmentations!\n\n    dataset_name, reference, release, license, description: self-explanatory and not used by nnU-Net. Just for\n    completeness and as a reminder that these would be great!\n\n    overwrite_image_reader_writer: If you need a special IO class for your dataset you can derive it from\n    BaseReaderWriter, place it into nnunet.imageio and reference it here by name\n\n    kwargs: whatever you put here will be placed in the dataset.json as well\n\n    \"\"\"\n\n    has_regions: bool = any(\n        [isinstance(i, (tuple, list)) and len(i) &gt; 1 for i in labels.values()]\n    )\n    if has_regions:\n        assert regions_class_order is not None, (\n            \"You have defined regions but regions_class_order is not set. \"\n            \"You need that.\"\n        )\n\n    # Construct the dataset JSON structure\n    dataset_json = {\n        \"channel_names\": channel_names,\n        \"labels\": labels,\n        \"numTraining\": num_training_cases,\n        \"file_ending\": file_ending,\n        \"name\": dataset_name,\n        \"reference\": reference,\n        \"release\": release,\n        \"licence\": usage_license,\n        \"description\": description,\n        \"overwrite_image_reader_writer\": overwrite_image_reader_writer,\n        \"regions_class_order\": regions_class_order,\n    }\n\n    dataset_json = {k: v for k, v in dataset_json.items() if v is not None}\n\n    dataset_json.update(kwargs)\n\n    output_path = pathlib.Path(output_folder) / \"dataset.json\"\n    with output_path.open(\"w\") as f:\n        json.dump(dataset_json, f, indent=4, sort_keys=False)\n</code></pre>"},{"location":"reference/utils/nnunet/#imgtools.utils.nnunet.generate_nnunet_scripts","title":"generate_nnunet_scripts","text":"<pre><code>generate_nnunet_scripts(\n    output_directory: str | pathlib.Path, dataset_id: int\n) -&gt; None\n</code></pre> <p>Creates two bash scripts: 1. <code>nnunet_preprocess.sh</code> for running nnUNet preprocessing. 2. <code>nnunet_train.sh</code> for running nnUNet training.</p> <p>Parameters: - output_directory (str): The directory where the output and subdirectories are located. - dataset_id (int): The ID of the dataset to be processed.</p> Source code in <code>src/imgtools/utils/nnunet.py</code> <pre><code>def generate_nnunet_scripts(\n    output_directory: str | pathlib.Path, dataset_id: int\n) -&gt; None:\n    \"\"\"\n    Creates two bash scripts:\n    1. `nnunet_preprocess.sh` for running nnUNet preprocessing.\n    2. `nnunet_train.sh` for running nnUNet training.\n\n    Parameters:\n    - output_directory (str): The directory where the output and subdirectories are located.\n    - dataset_id (int): The ID of the dataset to be processed.\n    \"\"\"\n    # Define paths using pathlib\n    output_directory = pathlib.Path(output_directory).resolve()\n\n    # Paths for the scripts\n    preprocess_shell_path = output_directory / \"nnunet_preprocess.sh\"\n    train_shell_path = output_directory / \"nnunet_train.sh\"\n    base_dir = output_directory.parent.parent\n\n    # Remove any existing script files before creating new ones\n    if preprocess_shell_path.exists():\n        preprocess_shell_path.unlink()\n    if train_shell_path.exists():\n        train_shell_path.unlink()\n\n    # Preprocessing script content\n    preprocess_script_content = f\"\"\"#!/bin/bash\nset -e\n\nexport nnUNet_raw=\"{base_dir}/nnUNet_raw\"\nexport nnUNet_preprocessed=\"{base_dir}/nnUNet_preprocessed\"\nexport nnUNet_results=\"{base_dir}/nnUNet_results\"\n\n# Preprocessing command for dataset {dataset_id}\nnnUNetv2_plan_and_preprocess -d {dataset_id} --verify_dataset_integrity -c 3d_fullres\n\"\"\"\n\n    # Write the preprocessing script\n    with preprocess_shell_path.open(\"w\", newline=\"\\n\") as f:\n        f.write(preprocess_script_content)\n\n    # Make script executable\n    preprocess_shell_path.chmod(preprocess_shell_path.stat().st_mode | 0o111)\n\n    # Training script content\n    train_script_content = f\"\"\"#!/bin/bash\nset -e\n\nexport nnUNet_raw=\"{base_dir}/nnUNet_raw\"\nexport nnUNet_preprocessed=\"{base_dir}/nnUNet_preprocessed\"\nexport nnUNet_results=\"{base_dir}/nnUNet_results\"\n\n# Training loop\nfor (( i=0; i&lt;5; i++ ))\ndo\n    nnUNetv2_train {dataset_id} 3d_fullres $i\ndone\n\"\"\"\n\n    # Write the training script\n    with train_shell_path.open(\"w\", newline=\"\\n\") as f:\n        f.write(train_script_content)\n\n    # Make script executable\n    train_shell_path.chmod(train_shell_path.stat().st_mode | 0o111)\n</code></pre>"},{"location":"reference/utils/optional_import/","title":"Optional import","text":""},{"location":"reference/utils/optional_import/#imgtools.utils.optional_import","title":"optional_import","text":"<p>Functions:</p> Name Description <code>optional_import</code> <p>Attempt to import an optional module and handle its absence gracefully.</p>"},{"location":"reference/utils/optional_import/#imgtools.utils.optional_import.optional_import","title":"optional_import","text":"<pre><code>optional_import(\n    module_name: str, raise_error: bool = False\n) -&gt; typing.Tuple[typing.Any, bool]\n</code></pre> <p>Attempt to import an optional module and handle its absence gracefully.</p> <p>This function is useful when you want to provide optional features that depend on modules that may not be installed.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>Name of the module to import (e.g., 'numpy', 'torch').</p> required <code>bool</code> <p>If True, raise an OptionalImportError when the module is not found. If False, return (None, False) when import fails.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing (module, success_flag), where: - module: The imported module if successful, None if failed - success_flag: True if import succeeded, False otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage - silent failure\n&gt;&gt;&gt; numpy, has_numpy = optional_import(\"numpy\")\n&gt;&gt;&gt; if not has_numpy:\n...     raise OptionalImportError(\"numpy\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Usage with error raising\n&gt;&gt;&gt; torch, _ = optional_import(\n...     \"torch\", raise_error=True\n... )\n&gt;&gt;&gt; # Will raise OptionalImportError if torch is not installed\n</code></pre> Source code in <code>src/imgtools/utils/optional_import.py</code> <pre><code>def optional_import(\n    module_name: str,\n    raise_error: bool = False,\n) -&gt; Tuple[Any, bool]:\n    \"\"\"\n    Attempt to import an optional module and handle its absence gracefully.\n\n    This function is useful when you want to provide optional features that depend\n    on modules that may not be installed.\n\n    Parameters\n    ----------\n    module_name : str\n        Name of the module to import (e.g., 'numpy', 'torch').\n    raise_error : bool, optional\n        If True, raise an OptionalImportError when the module is not found.\n        If False, return (None, False) when import fails.\n\n    Returns\n    -------\n    tuple\n        A tuple containing (module, success_flag), where:\n        - module: The imported module if successful, None if failed\n        - success_flag: True if import succeeded, False otherwise\n    Examples\n    --------\n    &gt;&gt;&gt; # Basic usage - silent failure\n    &gt;&gt;&gt; numpy, has_numpy = optional_import(\"numpy\")\n    &gt;&gt;&gt; if not has_numpy:\n    ...     raise OptionalImportError(\"numpy\")\n\n    &gt;&gt;&gt; # Usage with error raising\n    &gt;&gt;&gt; torch, _ = optional_import(\n    ...     \"torch\", raise_error=True\n    ... )\n    &gt;&gt;&gt; # Will raise OptionalImportError if torch is not installed\n    \"\"\"\n    try:\n        module = importlib.import_module(module_name)\n        return module, True\n    except ImportError as ie:\n        if raise_error:\n            raise OptionalImportError(module_name) from ie\n        return None, False\n</code></pre>"},{"location":"reference/utils/optional_import/#imgtools.utils.optional_import.optional_import(module_name)","title":"<code>module_name</code>","text":""},{"location":"reference/utils/optional_import/#imgtools.utils.optional_import.optional_import(raise_error)","title":"<code>raise_error</code>","text":""},{"location":"reference/utils/path_limits/","title":"Path limits","text":""},{"location":"reference/utils/path_limits/#imgtools.utils.path_limits","title":"path_limits","text":"<p>An attempt at a cross-platform way to get the maximum path length for a given OS.</p> <p>This is useful for ensuring file operations stay within safe bounds for the operating system.</p> <p>Mostly to deal with the fact that Windows has a default path length of 260 characters, and so a dicomsorter path should be validated to be less than that.</p> <p>Functions:</p> Name Description <code>os_max_filename_length</code> <p>Returns the maximum filename length supported by the current operating system.</p> <code>os_max_path_length</code> <p>Returns the maximum file path length supported by the current operating system.</p>"},{"location":"reference/utils/path_limits/#imgtools.utils.path_limits.os_max_filename_length","title":"os_max_filename_length","text":"<pre><code>os_max_filename_length() -&gt; int\n</code></pre> <p>Returns the maximum filename length supported by the current operating system.</p> <p>On Windows, the default maximum filename length is 255 characters. On Linux and macOS (Darwin), it retrieves the limit using os.pathconf or falls back to a safe default value.</p> <p>Returns:</p> Type Description <code>int</code> <p>The maximum filename length in characters for the current operating system.</p> Source code in <code>src/imgtools/utils/path_limits.py</code> <pre><code>def os_max_filename_length() -&gt; int:\n    \"\"\"\n    Returns the maximum filename length supported by the current operating system.\n\n    On Windows, the default maximum filename length is 255 characters.\n    On Linux and macOS (Darwin), it retrieves the limit using os.pathconf or\n    falls back to a safe default value.\n\n    Returns\n    -------\n    int\n        The maximum filename length in characters for the current operating system.\n\n    Raises\n    ------\n    ValueError\n        If the operating system is unknown and the filename length cannot be determined.\n    \"\"\"\n    system = platform.system()\n    if system == \"Windows\":\n        return 255  # Windows NTFS and FAT32 typically support 255 characters\n    elif system in (\"Linux\", \"Darwin\"):\n        try:\n            return os.pathconf(\"/\", \"PC_NAME_MAX\")\n        except (AttributeError, ValueError, OSError):\n            return 255  # Common fallback for most filesystems\n    else:\n        errmsg = \"Unknown operating system. Unable to determine max filename length.\"\n        raise ValueError(errmsg)\n</code></pre>"},{"location":"reference/utils/path_limits/#imgtools.utils.path_limits.os_max_path_length","title":"os_max_path_length","text":"<pre><code>os_max_path_length() -&gt; int\n</code></pre> <p>Returns the maximum file path length supported by the current operating system.</p> <p>On Windows, this function accounts for long path support if enabled (32767 characters). On Linux and macOS (Darwin), it retrieves the limit using os.pathconf or falls back to reasonable default values.</p> <p>Returns:</p> Type Description <code>int</code> <p>The maximum file path length in characters for the current operating system.</p> Source code in <code>src/imgtools/utils/path_limits.py</code> <pre><code>def os_max_path_length() -&gt; int:\n    \"\"\"\n    Returns the maximum file path length supported by the current operating system.\n\n    On Windows, this function accounts for long path support if enabled (32767 characters).\n    On Linux and macOS (Darwin), it retrieves the limit using os.pathconf or falls back\n    to reasonable default values.\n\n    Returns\n    -------\n    int\n        The maximum file path length in characters for the current operating system.\n\n    Raises\n    ------\n    ValueError\n        If the operating system is unknown and the path length cannot be determined.\n    \"\"\"\n    system = platform.system()\n    if system == \"Windows\":\n        # Check for long path support on Windows\n        # MAX_PATH is 260, but long paths allow up to 32767 characters\n        return 32767 if os.environ.get(\"LongPathsEnabled\", \"0\") == \"1\" else 260  # noqa: SIM112\n    elif system == \"Linux\":\n        try:\n            return os.pathconf(\"/\", \"PC_PATH_MAX\")\n        except (AttributeError, ValueError, OSError):\n            return 4096  # Common default for Linux filesystems\n    elif system == \"Darwin\":\n        try:\n            return os.pathconf(\"/\", \"PC_PATH_MAX\")\n        except (AttributeError, ValueError, OSError):\n            return 1024  # Safe fallback for macOS\n    else:\n        errmsg = (\n            \"Unknown operating system. Unable to determine max path length.\"\n        )\n        raise ValueError(errmsg)\n</code></pre>"},{"location":"reference/utils/sanitize_file_name/","title":"Sanitize file name","text":""},{"location":"reference/utils/sanitize_file_name/#imgtools.utils.sanitize_file_name","title":"sanitize_file_name","text":"<p>Functions:</p> Name Description <code>sanitize_file_name</code> <p>Sanitize filenames by replacing potentially dangerous characters.</p> <p>Examples:</p> <p>Sanitize a filename:     &gt;&gt;&gt; sanitize_file_name(\"test&lt;&gt;file:/name.dcm\")     'test_file_name.dcm'</p>"},{"location":"reference/utils/sanitize_file_name/#imgtools.utils.sanitize_file_name.sanitize_file_name","title":"sanitize_file_name","text":"<pre><code>sanitize_file_name(filename: str) -&gt; str\n</code></pre> <p>Sanitize the file name by removing or replacing disallowed characters, while preserving forward slashes.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The input file name to sanitize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The sanitized file name.</p> Source code in <code>src/imgtools/utils/sanitize_file_name.py</code> <pre><code>def sanitize_file_name(filename: str) -&gt; str:\n    \"\"\"\n    Sanitize the file name by removing or replacing disallowed characters,\n    while preserving forward slashes.\n\n    Parameters\n    ----------\n    filename : str\n        The input file name to sanitize.\n\n    Returns\n    -------\n    str\n        The sanitized file name.\n    \"\"\"\n    assert filename and isinstance(filename, str)\n\n    # Remove disallowed characters at the start and end\n    filename = DISALLOWED_CHARS_STRIP_REGEX.sub(\"\", filename).strip()\n\n    filename = filename.replace(\" - \", \"-\")\n\n    # Replace multiple spaces with a single space\n    filename = re.sub(r\"\\s+\", \"_\", filename)\n\n    # Replace multiple consecutive disallowed characters with a single underscore\n    filename = re.sub(f\"[{DISALLOWED_CHARS}]+\", \"_\", filename)\n\n    # Replace remaining disallowed characters with underscores\n    sanitized = DISALLOWED_CHARS_PATTERN.sub(\"_\", filename)\n\n    return sanitized\n</code></pre>"},{"location":"reference/utils/sanitize_file_name/#imgtools.utils.sanitize_file_name.sanitize_file_name(filename)","title":"<code>filename</code>","text":""},{"location":"reference/utils/timer_utils/","title":"Timer utils","text":""},{"location":"reference/utils/timer_utils/#imgtools.utils.timer_utils","title":"timer_utils","text":"<p>Functions:</p> Name Description <code>timed_context</code> <p>Context manager to measure the execution time of a block of code and log it with a custom name.</p> <code>timer</code> <p>Decorator to measure the execution time of a function and log it with a custom name.</p>"},{"location":"reference/utils/timer_utils/#imgtools.utils.timer_utils.timed_context","title":"timed_context","text":"<pre><code>timed_context(\n    name: str,\n) -&gt; imgtools.utils.timer_utils.TimerContext\n</code></pre> <p>Context manager to measure the execution time of a block of code and log it with a custom name.</p> <p>Returns:</p> Type Description <code>    TimerContext:</code> <p>A context manager that measures the execution time of a block of code.</p> Example <pre><code>with timed_context(\"My Block\"):\n    # do something\n\n# Output: `My Block took 3.1244 seconds`\n</code></pre> Source code in <code>src/imgtools/utils/timer_utils.py</code> <pre><code>def timed_context(name: str) -&gt; TimerContext:\n    \"\"\"\n    Context manager to measure the execution time of a block of code and log it with a custom name.\n\n    Parameters\n    ----------\n        name (str): The custom name to use in the log message.\n\n    Returns\n    -------\n        TimerContext:\n        A context manager that measures the execution time of a block of code.\n\n    Example\n    -------\n        with timed_context(\"My Block\"):\n            # do something\n\n        # Output: `My Block took 3.1244 seconds`\n    \"\"\"\n    return TimerContext(name)\n</code></pre>"},{"location":"reference/utils/timer_utils/#imgtools.utils.timer_utils.timer","title":"timer","text":"<pre><code>timer(\n    name: str,\n) -&gt; typing.Callable[\n    [typing.Callable[..., typing.Any]],\n    typing.Callable[..., typing.Any],\n]\n</code></pre> <p>Decorator to measure the execution time of a function and log it with a custom name.</p> <p>Returns:</p> Type Description <code>    Callable[[Callable[..., Any]], Callable[..., Any]]:</code> <p>A decorator that wraps the function to measure its execution time.</p> Example <pre><code>@timer(\"My Function\")\ndef my_function():\n    # do something\n\nmy_function()\n# Output: `My Function took 3.1244 seconds`\n</code></pre> Source code in <code>src/imgtools/utils/timer_utils.py</code> <pre><code>def timer(name: str) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"\n    Decorator to measure the execution time of a function and log it with a custom name.\n\n    Parameters\n    ----------\n        name (str): The custom name to use in the log message.\n\n    Returns\n    -------\n        Callable[[Callable[..., Any]], Callable[..., Any]]:\n        A decorator that wraps the function to measure its execution time.\n\n    Example\n    -------\n        @timer(\"My Function\")\n        def my_function():\n            # do something\n\n        my_function()\n        # Output: `My Function took 3.1244 seconds`\n    \"\"\"\n\n    def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:  # noqa\n            with TimerContext(name):\n                result = func(*args, **kwargs)\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/utils/truncate_uid/","title":"Truncate uid","text":""},{"location":"reference/utils/truncate_uid/#imgtools.utils.truncate_uid","title":"truncate_uid","text":"<p>Functions:</p> Name Description <code>truncate_uid</code> <p>Truncate the UID to the last n characters (including periods and underscores).</p>"},{"location":"reference/utils/truncate_uid/#imgtools.utils.truncate_uid.truncate_uid","title":"truncate_uid","text":"<pre><code>truncate_uid(uid: str, last_digits: int = 5) -&gt; str\n</code></pre> <p>Truncate the UID to the last n characters (including periods and underscores).</p> <p>If the UID is shorter than <code>last_digits</code>, the entire UID is returned.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The UID string to truncate.</p> required <code>int</code> <p>The number of characters to keep at the end of the UID (default is 5).</p> <code>5</code> <p>Returns:</p> Type Description <code>str</code> <p>The truncated UID string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; truncate_uid(\n...     \"1.2.840.10008.1.2.1\",\n...     last_digits=5,\n... )\n'.1.2.1'\n&gt;&gt;&gt; truncate_uid(\n...     \"12345\",\n...     last_digits=10,\n... )\n'12345'\n</code></pre> Source code in <code>src/imgtools/utils/truncate_uid.py</code> <pre><code>def truncate_uid(uid: str, last_digits: int = 5) -&gt; str:\n    \"\"\"\n    Truncate the UID to the last n characters (including periods and underscores).\n\n    If the UID is shorter than `last_digits`, the entire UID is returned.\n\n    Parameters\n    ----------\n    uid : str\n        The UID string to truncate.\n    last_digits : int, optional\n        The number of characters to keep at the end of the UID (default is 5).\n\n    Returns\n    -------\n    str\n        The truncated UID string.\n\n    Examples\n    --------\n    &gt;&gt;&gt; truncate_uid(\n    ...     \"1.2.840.10008.1.2.1\",\n    ...     last_digits=5,\n    ... )\n    '.1.2.1'\n    &gt;&gt;&gt; truncate_uid(\n    ...     \"12345\",\n    ...     last_digits=10,\n    ... )\n    '12345'\n    \"\"\"\n    assert uid is not None\n    assert isinstance(uid, str)\n    assert isinstance(last_digits, int)\n    if last_digits &gt;= len(uid) or last_digits &lt;= 0:\n        return uid\n\n    return uid[-last_digits:]\n</code></pre>"},{"location":"reference/utils/truncate_uid/#imgtools.utils.truncate_uid.truncate_uid(uid)","title":"<code>uid</code>","text":""},{"location":"reference/utils/truncate_uid/#imgtools.utils.truncate_uid.truncate_uid(last_digits)","title":"<code>last_digits</code>","text":""},{"location":"reference/vizualize/utils/","title":"Utils","text":""},{"location":"reference/vizualize/utils/#imgtools.vizualize.utils","title":"utils","text":""},{"location":"reference/vizualize/utils/#imgtools.vizualize.utils.EnvironmentType","title":"EnvironmentType","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enumeration of different Python execution environments.</p> <p>Attributes:</p> Name Type Description <code>JUPYTER_NOTEBOOK</code> <code>str</code> <p>Jupyter Notebook or Jupyter Lab.</p> <code>JUPYTER_QTCONSOLE</code> <code>str</code> <p>Jupyter QtConsole.</p> <code>GOOGLE_COLAB</code> <code>str</code> <p>Google Colaboratory (Cloud-based Jupyter).</p> <code>VSCODE_NOTEBOOK</code> <code>str</code> <p>Jupyter Kernel running inside VSCode.</p> <code>IPYTHON_TERMINAL</code> <code>str</code> <p>IPython interactive shell in a terminal.</p> <code>STANDARD_PYTHON</code> <code>str</code> <p>Standard Python interpreter (script or REPL).</p> <p>Methods:</p> Name Description <code>detect</code> <p>Detect the current Python execution environment.</p>"},{"location":"reference/vizualize/utils/#imgtools.vizualize.utils.EnvironmentType.detect","title":"detect  <code>classmethod</code>","text":"<pre><code>detect() -&gt; imgtools.vizualize.utils.EnvironmentType\n</code></pre> <p>Detect the current Python execution environment.</p> <p>Returns:</p> Type Description <code>imgtools.vizualize.utils.EnvironmentType</code> <p>The detected environment type.</p> Source code in <code>src/imgtools/vizualize/utils.py</code> <pre><code>@classmethod\ndef detect(cls) -&gt; \"EnvironmentType\":\n    \"\"\"\n    Detect the current Python execution environment.\n\n    Returns\n    -------\n    EnvironmentType\n        The detected environment type.\n    \"\"\"\n    try:\n        from IPython import get_ipython\n\n        shell = get_ipython().__class__.__name__\n\n        if \"COLAB_GPU\" in os.environ:\n            return cls.GOOGLE_COLAB\n        if \"VSCODE_PID\" in os.environ:\n            return cls.VSCODE_NOTEBOOK\n        if shell == \"ZMQInteractiveShell\":\n            return cls.JUPYTER_NOTEBOOK  # QtConsole also uses this\n        if shell == \"TerminalInteractiveShell\":\n            return cls.IPYTHON_TERMINAL\n    except (ImportError, AttributeError):\n        pass\n\n    return cls.STANDARD_PYTHON  # Default to standard Python\n</code></pre>"},{"location":"reference/vizualize/visualizer/","title":"Visualizer","text":""},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer","title":"visualizer","text":"<p>3DSliceImages: Efficiently generate and store 2D slices from a 3D SimpleITK image.</p> <p>Functions:</p> Name Description <code>view_multiple_SliceImage3DObjects</code> <p>This is a grand view to help when you want to view multiple</p>"},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer.SliceImage3D","title":"SliceImage3D  <code>dataclass</code>","text":"<pre><code>SliceImage3D(\n    image: SimpleITK.Image,\n    mask: SimpleITK.Image | None = None,\n    alpha: float = 0.2,\n    disable_progress: bool = False,\n)\n</code></pre> <p>Generates 2D slices from a 3D SimpleITK image and optionally overlays a mask.</p> <p>Methods:</p> Name Description <code>generate_dim_slices</code> <p>Generate 2D slices along a given dimension, optionally overlaying the mask.</p>"},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer.SliceImage3D.generate_dim_slices","title":"generate_dim_slices","text":"<pre><code>generate_dim_slices(\n    dim: int, every_n: int = 5\n) -&gt; imgtools.vizualize.visualizer.SliceImage3D\n</code></pre> <p>Generate 2D slices along a given dimension, optionally overlaying the mask.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The dimension along which to generate slices (0, 1, or 2).</p> required <code>int</code> <p>Generate a slice every <code>every_n</code> pixels, by default 5.</p> <code>5</code> Source code in <code>src/imgtools/vizualize/visualizer.py</code> <pre><code>def generate_dim_slices(\n    self,\n    dim: int,\n    every_n: int = 5,\n) -&gt; SliceImage3D:\n    \"\"\"Generate 2D slices along a given dimension, optionally overlaying the mask.\n\n    Parameters\n    ----------\n    dim : int\n        The dimension along which to generate slices (0, 1, or 2).\n    every_n : int, optional\n        Generate a slice every `every_n` pixels, by default 5.\n    \"\"\"\n    slices = self.image_slices(dim, every_n)\n    mask_slices: list[np.ndarray] | None = self.mask_slices(dim, every_n)\n\n    if mask_slices is None:\n        mask_slices = [\n            np.zeros_like(slices[0]) for _ in range(len(slices))\n        ]\n    logger.warning(\"Generating slices...\")\n    tasks = [\n        (\n            i,\n            img,\n            mask,\n            self.alpha,\n            self.vmax,\n            self.vmin,\n        )\n        for i, (img, mask) in enumerate(\n            zip(slices, mask_slices, strict=False)\n        )\n    ]\n    start = time.time()\n    results = [\n        SliceImage3D._generate_slice(*task)\n        for task in tqdm(tasks, disable=self.disable_progress)\n    ]\n    logger.info(f\"Time taken: {time.time() - start:.2f}s\")\n\n    # Store slices in a dictionary\n    images = [image for _, image in results]\n    self.slices = ImageSlices(images)\n\n    return self\n</code></pre>"},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer.SliceImage3D.generate_dim_slices(dim)","title":"<code>dim</code>","text":""},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer.SliceImage3D.generate_dim_slices(every_n)","title":"<code>every_n</code>","text":""},{"location":"reference/vizualize/visualizer/#imgtools.vizualize.visualizer.view_multiple_SliceImage3DObjects","title":"view_multiple_SliceImage3DObjects","text":"<pre><code>view_multiple_SliceImage3DObjects(\n    *args: tuple[\n        str, imgtools.vizualize.visualizer.SliceImage3D\n    ]\n) -&gt; None\n</code></pre> <p>This is a grand view to help when you want to view multiple</p> <p>pass in all of them, with generate_dim_slices already called ALL of them MUST have the same number of slices this will then use a SINGLE slider, to view and iterate over all of them</p> Source code in <code>src/imgtools/vizualize/visualizer.py</code> <pre><code>def view_multiple_SliceImage3DObjects(*args: tuple[str, SliceImage3D]) -&gt; None:  # noqa\n    \"\"\"This is a grand view to help when you want to view multiple\n\n    pass in all of them, with generate_dim_slices already called\n    ALL of them MUST have the same number of slices\n    this will then use a SINGLE slider, to view and iterate over all of them\n\n    \"\"\"\n\n    # calculate some metadata for all the slices\n    metadata_dict = {\n        name: f\"Shape : x={slice_image.image_array.shape[2]}, y={slice_image.image_array.shape[1]}\"\n        for name, slice_image in args\n    }\n\n    def display_slices(index: int) -&gt; None:\n        # Create a vertical box layout for each slice with a label\n        vboxes = []\n        for name, slice_image in args:\n            # Convert the Pillow image to Base64\n            buffer = BytesIO()\n            slice_image.slices[index].save(buffer, format=\"PNG\")\n            base64_image = base64.b64encode(buffer.getvalue()).decode()\n\n            metadata_str = metadata_dict[name]\n            # Generate HTML content with Base64-encoded image\n            html_content = f\"\"\"\n            &lt;div style=\"text-align: center;\"&gt;\n                &lt;h3&gt;{name}&lt;/h3&gt;\n                &lt;p&gt;{metadata_str}&lt;/p&gt;\n                &lt;img src=\"data:image/png;base64,{base64_image}\" \n                    style=\"width: 500px; height: auto;\"&gt;\n            &lt;/div&gt;\n            \"\"\"\n            vbox = widgets.VBox([widgets.HTML(html_content)])\n            vboxes.append(vbox)\n\n        # Create a horizontal box layout for all vboxes\n        hbox = widgets.HBox(vboxes)\n        display(hbox)\n\n    if not args:\n        raise ValueError(\"At least one SliceImage3D object must be provided\")\n\n    num_slices = len(args[0][1].slices)\n    if not all(\n        len(slice_image.slices) == num_slices for _, slice_image in args\n    ):\n        errmsg = (\n            \"All SliceImage3D objects must have the same number of slices!!\"\n            f\" (expected {num_slices}) but got {[len(s.slices) for _, s in args]}\"\n        )\n        raise ValueError(errmsg)\n\n    max_slider = num_slices - 1\n    widgets.interact(\n        display_slices,\n        index=widgets.IntSlider(\n            min=0, max=max_slider, step=1, value=max_slider // 2\n        ),\n    )\n</code></pre>"},{"location":"reference/vizualize/visualizer2/","title":"Visualizer2","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2","title":"visualizer2","text":"<p>Functions:</p> Name Description <code>display_slices</code> <p>Display orthogonal slices of a 3D array.</p>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer","title":"ImageVisualizer  <code>dataclass</code>","text":"<pre><code>ImageVisualizer(main_image: SimpleITK.Image)\n</code></pre> <p>Simple visualizer for sitk based images.</p> <p>Methods:</p> Name Description <code>from_image</code> <p>Create an ImageVisualizer from a SimpleITK image.</p> <code>from_image_and_mask</code> <p>Create an ImageVisualizer with a mask overlay on the image.</p> <code>view_grid</code> <p>Visualize slices in a grid.</p>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image","title":"from_image  <code>classmethod</code>","text":"<pre><code>from_image(\n    image: SimpleITK.Image,\n) -&gt; imgtools.vizualize.visualizer2.ImageVisualizer\n</code></pre> <p>Create an ImageVisualizer from a SimpleITK image.</p> Source code in <code>src/imgtools/vizualize/visualizer2.py</code> <pre><code>@classmethod\ndef from_image(\n    cls,\n    image: sitk.Image,\n) -&gt; ImageVisualizer:\n    \"\"\"Create an ImageVisualizer from a SimpleITK image.\"\"\"\n\n    return cls(main_image=image)\n</code></pre>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask","title":"from_image_and_mask  <code>classmethod</code>","text":"<pre><code>from_image_and_mask(\n    image: SimpleITK.Image,\n    mask: SimpleITK.Image,\n    label: int = 1,\n    as_contour: bool = False,\n    mask_color: imgtools.vizualize.visualizer2.MaskColor = imgtools.vizualize.visualizer2.MaskColor.GREEN,\n    opacity: float = 0.5,\n    background_label: int = 0,\n    crop_to_bbox: bool = True,\n    croppad: int = 2,\n) -&gt; imgtools.vizualize.visualizer2.ImageVisualizer\n</code></pre> <p>Create an ImageVisualizer with a mask overlay on the image.</p> <p>Parameters:</p> Name Type Description Default <code>SimpleITK.Image</code> <p>Base image for visualization</p> required <code>SimpleITK.Image</code> <p>Mask image to overlay on the base image</p> required <code>int</code> <p>Label value in the mask to use for overlay</p> <code>1</code> <code>bool</code> <p>If True, convert the mask to a contour before overlay</p> <code>False</code> <code>imgtools.vizualize.visualizer2.MaskColor</code> <p>Color to use for the mask overlay</p> <code>MaskColor.GREEN</code> <code>float</code> <p>Opacity of the mask overlay (0.0-1.0)</p> <code>0.5</code> <code>int</code> <p>Label value in the mask to treat as background</p> <code>0</code> <code>bool</code> <p>If True, crop the image to the bounding box of the mask</p> <code>True</code> <code>int</code> <p>Padding to add around the crop region</p> <code>2</code> <p>Returns:</p> Type Description <code>imgtools.vizualize.visualizer2.ImageVisualizer</code> <p>Instance with the mask overlaid on the image</p> Source code in <code>src/imgtools/vizualize/visualizer2.py</code> <pre><code>@classmethod\ndef from_image_and_mask(\n    cls,\n    image: sitk.Image,\n    mask: sitk.Image,\n    label: int = 1,\n    as_contour: bool = False,\n    # overlay settings\n    mask_color: MaskColor = MaskColor.GREEN,\n    opacity: float = 0.5,\n    background_label: int = 0,\n    # preprocessing settings\n    crop_to_bbox: bool = True,\n    croppad: int = 2,\n) -&gt; ImageVisualizer:\n    \"\"\"Create an ImageVisualizer with a mask overlay on the image.\n\n    Parameters\n    ----------\n    image : sitk.Image\n        Base image for visualization\n    mask : sitk.Image\n        Mask image to overlay on the base image\n    label : int, default=1\n        Label value in the mask to use for overlay\n    as_contour : bool, default=False\n        If True, convert the mask to a contour before overlay\n    mask_color : MaskColor, default=MaskColor.GREEN\n        Color to use for the mask overlay\n    opacity : float, default=0.5\n        Opacity of the mask overlay (0.0-1.0)\n    background_label : int, default=0\n        Label value in the mask to treat as background\n    crop_to_bbox : bool, default=True\n        If True, crop the image to the bounding box of the mask\n    croppad : int, default=2\n        Padding to add around the crop region\n\n    Returns\n    -------\n    ImageVisualizer\n        Instance with the mask overlaid on the image\n    \"\"\"\n\n    region = (\n        RegionBox.from_mask_bbox(mask, label=label)\n        .expand_to_cube()  # cube expanding will help for grid\n        .pad(croppad)\n    )\n\n    if as_contour:\n        f_mask = sitk.BinaryContour(mask, fullyConnected=True)\n    else:\n        f_mask = mask\n\n    combined_image = sitk.LabelOverlay(\n        sitk.Cast(image, pixelID=sitk.sitkUInt8),\n        f_mask,\n        opacity=opacity,\n        backgroundValue=background_label,\n        colormap=mask_color.value,\n    )\n\n    if crop_to_bbox:\n        combined_image = region.crop_image(combined_image)\n\n    return cls(main_image=combined_image)\n</code></pre>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(image)","title":"<code>image</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(mask)","title":"<code>mask</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(label)","title":"<code>label</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(as_contour)","title":"<code>as_contour</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(mask_color)","title":"<code>mask_color</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(opacity)","title":"<code>opacity</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(background_label)","title":"<code>background_label</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(crop_to_bbox)","title":"<code>crop_to_bbox</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.from_image_and_mask(croppad)","title":"<code>croppad</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.view_grid","title":"view_grid","text":"<pre><code>view_grid(\n    every: int = 1,\n    fig: matplotlib.pyplot.Figure | None = None,\n) -&gt; matplotlib.pyplot.Figure\n</code></pre> <p>Visualize slices in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Step size for slice selection</p> <code>1</code> <code>matplotlib.pyplot.Figure</code> <p>Existing figure to use for visualization If None, a new figure will be created</p> <code>None</code> <p>Returns:</p> Type Description <code>matplotlib.pyplot.Figure</code> <p>Figure containing the grid of slices</p> Source code in <code>src/imgtools/vizualize/visualizer2.py</code> <pre><code>def view_grid(\n    self,\n    every: int = 1,\n    fig: plt.Figure | None = None,\n) -&gt; plt.Figure:\n    \"\"\"Visualize slices in a grid.\n\n    Parameters\n    ----------\n    every : int\n        Step size for slice selection\n    fig : plt.Figure, optional\n        Existing figure to use for visualization\n        If None, a new figure will be created\n\n    Returns\n    -------\n    plt.Figure\n        Figure containing the grid of slices\n    \"\"\"\n    dimension = 2\n    plot_size = 3\n\n    image = self.main_image\n    size = image.GetSize()\n\n    slice_indices = list(range(0, size[dimension], every))\n\n    # should be a square number of slices\n    cols = math.ceil(np.sqrt(len(slice_indices)))\n    rows = math.ceil(len(slice_indices) / cols)\n    figsize = (plot_size * cols, plot_size * rows)\n\n    if fig is None:\n        fig, axes = plt.subplots(\n            rows, cols, figsize=figsize, constrained_layout=True\n        )\n    else:\n        axes = fig.subplots(rows, cols)\n\n    axes = axes.ravel()\n\n    for idx, slice_index in enumerate(slice_indices):\n        slice_image = image[:, :, slice_index]\n        array = sitk.GetArrayViewFromImage(slice_image)\n\n        ax = axes[idx]\n        ax.imshow(array, cmap=\"gray\")\n        ax.set_title(f\"Slice {slice_index + 1}/{size[2]}\", fontsize=8)\n        ax.axis(\"off\")\n\n    # Hide any extra axes if the grid is larger than the number of slices\n    for idx in range(len(slice_indices), rows * cols):\n        axes[idx].axis(\"off\")\n\n    return fig\n</code></pre>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.view_grid(every)","title":"<code>every</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.ImageVisualizer.view_grid(fig)","title":"<code>fig</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.MaskColor","title":"MaskColor","text":"<p>               Bases: <code>enum.Enum</code></p> <p>Predefined color values for mask overlays.</p> <p>Each color is represented as an RGB list with values from 0-255. The RGB value specifically contains concatenated R, G, B channels for sitk colormap.</p>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.display_slices","title":"display_slices","text":"<pre><code>display_slices(\n    array: numpy.ndarray,\n    x_index: int,\n    y_index: int,\n    z_index: int,\n) -&gt; None\n</code></pre> <p>Display orthogonal slices of a 3D array.</p> <p>Parameters:</p> Name Type Description Default <code>numpy.ndarray</code> <p>3D numpy array representing the image volume</p> required <code>int</code> <p>Index for slice in the x dimension</p> required <code>int</code> <p>Index for slice in the y dimension</p> required <code>int</code> <p>Index for slice in the z dimension</p> required Source code in <code>src/imgtools/vizualize/visualizer2.py</code> <pre><code>def display_slices(\n    array: np.ndarray, x_index: int, y_index: int, z_index: int\n) -&gt; None:\n    \"\"\"Display orthogonal slices of a 3D array.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        3D numpy array representing the image volume\n    x_index : int\n        Index for slice in the x dimension\n    y_index : int\n        Index for slice in the y dimension\n    z_index : int\n        Index for slice in the z dimension\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(array[x_index, :, :], aspect=\"auto\")\n    axes[1].imshow(array[:, y_index, :], aspect=\"auto\")\n    axes[2].imshow(array[:, :, z_index], aspect=\"auto\")\n    # turn off axes\n    for ax in axes:\n        ax.axis(\"off\")\n\n    plt.show()\n    for ax in axes:\n        ax.axis(\"off\")\n\n    plt.show()\n</code></pre>"},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.display_slices(array)","title":"<code>array</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.display_slices(x_index)","title":"<code>x_index</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.display_slices(y_index)","title":"<code>y_index</code>","text":""},{"location":"reference/vizualize/visualizer2/#imgtools.vizualize.visualizer2.display_slices(z_index)","title":"<code>z_index</code>","text":""},{"location":"usage/autopipeline/","title":"Taming the Chaos: AutoPipeline for Medical Image Processing","text":""},{"location":"usage/autopipeline/#tldr-from-raw-data-to-research-ready-in-minutes","title":"TLDR; From Raw Data to Research-Ready in Minutes","text":"<p>AutoPipeline bridges the gap between raw clinical data and research analysis,  making it easier for imaging researchers of all experience levels to work with  complex medical imaging data.</p> <p>By standardizing the processing pipeline and capturing provenance information,  it also improves reproducibility - a crucial aspect of scientific research.</p>"},{"location":"usage/autopipeline/#the-medical-imaging-data-problem","title":"The Medical Imaging Data Problem","text":"<p>Ever spent hours organizing messy DICOM files from different scanners and  institutions? Medical imaging datasets are notoriously complex - with multiple scan types, segmentation masks, and treatment plans all interconnected but scattered across folder structures that seem designed to confuse.</p> <p>As an imaging researcher or student, you've probably faced this frustration:</p> <ul> <li>Folder chaos: Hundreds of nested directories with cryptic names</li> <li>Format soup: DICOM files that need conversion to research-friendly formats</li> <li>Relationship puzzles: CT scans that reference segmentation masks that    reference treatment plans</li> <li>Name confusion: Different institutions using different terms for the same    anatomical structures</li> </ul>"},{"location":"usage/autopipeline/#autopipeline-streamlined-medical-imaging-workflow","title":"AutoPipeline: Streamlined Medical Imaging Workflow","text":"<p>AutoPipeline is the core automation feature of med-imagetools designed to rescue you from these headaches. It provides a streamlined, reproducible workflow that:</p> <ol> <li>Crawls through your messy DICOM directories to discover all files</li> <li>Indexes the metadata and builds relationships between different series</li> <li>Processes linked series together, applying transformations as needed</li> <li>Organizes the output in a clean, standardized structure</li> <li>Tracks everything it does for complete reproducibility</li> </ol> <p>Think of it as your personal research assistant that handles all the tedious data preparation work, letting you focus on the exciting science!</p>"},{"location":"usage/autopipeline/#how-autopipeline-works","title":"How AutoPipeline Works","text":"<p>Behind the scenes, AutoPipeline brings together several powerful components:</p> <pre><code>flowchart TD\n    subgraph Ingest[\"Ingest \ud83d\udcc2 + \ud83d\udcc7\"]\n        A[Input DICOM Directory]\n        B[Crawler]\n        C[Index File]\n        A --&gt; B --&gt; C\n    end\n\n    subgraph Relationship_Builder[\"Relationship Builder \ud83d\udd78\ufe0f \"]\n        D[Interlacer]\n        E[Query Results]\n        D --&gt; E\n    end\n\n    subgraph Processing[\"Processing \ud83d\uddbc\ufe0f + \ud83d\udd27\"]\n        F[Sample Input]\n        G[\"MedImage Objects: Scan/VectorMask\"]\n        H[\"Transforms (optional): Resample, Windowing\"]\n        I[Processed Images]\n        F --&gt; G --&gt; H --&gt; I\n    end\n\n    subgraph Output[\"Output \ud83d\udcc1 + \ud83d\udcdd\"]\n        J[Sample Output]\n        K[NIfTI Files]\n        J --&gt; K\n    end\n\n    Ingest --&gt; Relationship_Builder\n    Relationship_Builder --&gt; Processing\n    Processing --&gt; Output\n</code></pre>"},{"location":"usage/autopipeline/#crawler-the-data-explorer","title":"Crawler: The Data Explorer","text":"<p>First, AutoPipeline dispatches the Crawler to search recursively through  your input directories. The Crawler identifies all DICOM files and extracts key  metadata like:</p> <ul> <li>Patient identifiers  </li> <li>Study and series UIDs</li> <li>Modality information (CT, MR, RTSTRUCT, etc.)</li> <li>Spatial characteristics</li> <li>References between series</li> </ul> <p>This information is compiled into an index that serves as a map of your entire  dataset.</p>"},{"location":"usage/autopipeline/#interlacer-the-relationship-builder","title":"Interlacer: The Relationship Builder","text":"<p>Next, the Interlacer takes this index and constructs a hierarchical forest  that represents the relationships between different series. It understands the  DICOM standards and knows, for example, that:</p> <ul> <li>An <code>RTSTRUCT</code> (radiation therapy structure) references a specific CT series</li> <li>A <code>PET</code> scan might be registered to a corresponding CT</li> <li>A treatment plan (<code>RTPLAN</code>) might hold the information about the referenced     <code>RTSTRUCT</code> used in a <code>RTDOSE</code> file</li> </ul> <p>The Interlacer can visualize these relationships and query them based on  modality combinations you're interested in.</p>"},{"location":"usage/autopipeline/#sample-processing-the-conversion-engine","title":"Sample Processing: The Conversion Engine","text":"<p>When you specify which modalities you want (like <code>\"CT,RTSTRUCT\"</code>), AutoPipeline:</p> <ol> <li>Queries the Interlacer for samples matching your criteria</li> <li>Loads the DICOM data into memory as MedImage objects</li> <li>Applies transformations like resampling or intensity windowing</li> <li>Saves the results in standard research formats (<code>NIfTI</code>)</li> </ol> <p>For segmentation data (<code>RTSTRUCT</code> or <code>SEG</code>), AutoPipeline can match region names  (like \"parotid\" or \"GTV\") to standardized keys using regular expressions,  solving the problem of inconsistent naming across institutions.</p>"},{"location":"usage/autopipeline/#using-autopipeline-a-quick-start","title":"Using AutoPipeline: A Quick Start","text":"<p>Using AutoPipeline is simple! </p> <p>The command line interface provides an intuitive way to process your data:</p> Full CLI Interface <p>API Reference</p> <p></p> Basic UsageWith TransformationsForce Update the Index <pre><code># Basic usage with CT and RT structure sets\nimgtools autopipeline \\\n    --modalities CT,RTSTRUCT \\\n    /path/to/messy/dicoms/ \\\n    /path/to/output/\n</code></pre> <pre><code># Adding transformations \nimgtools autopipeline\\\n    --modalities CT,RTSTRUCT \\\n    --spacing 1.0,1.0,1.0 \\\n    --window-width 400 --window-level 40 \\\n    /path/to/dicoms/ \\\n    /path/to/output/\n</code></pre> <pre><code># Force update the index if you have new files\nimgtools autopipeline \\\n    --modalities CT,RTSTRUCT \\\n    --update-crawl \\\n    /path/to/dicoms/ \\\n    /path/to/output/\n</code></pre>"},{"location":"usage/autopipeline/#standardizing-region-names-with-roi-matching","title":"Standardizing Region Names with ROI Matching","text":"<p>A common challenge in medical imaging is inconsistent naming of regions of  interest (ROIs). AutoPipeline solves this with powerful pattern matching.</p> <p>Med-ImageTools implements the <code>ROIMatcher</code> class, which allows you to define pattern matching rules for your ROIs. AutoPipeline will match the ROI names from your RTSTRUCT or SEG files against  these patterns and standardize them in the output files.</p> <p>We flexibly support describing these in the CLI and/or in a YAML file.</p> With CLIWith YAML File <pre><code>imgtools autopipeline\\\n    --modalities CT,RTSTRUCT \\\n    -rmap \"GTV:GTV,gtv,Gross.*Volume\" \\\n    -rmap \"Parotid_L:LeftParotid,PAROTID_L,L_Parotid\" \\\n    -rmap \"Parotid_R:RightParotid,PAROTID_R,R_Parotid\" \\\n    -rmap \"Cord:SpinalCord,Cord,Spinal_Cord\" \\\n    -rmap \"Mandible:mandible.*\" \\\n    /path/to/dicoms/ \\\n    /path/to/output/\n</code></pre> <p>Create a YAML file like this:</p> <pre><code># roi_patterns.yaml\nGTV: [\"GTV\", \"gtv\", \"Gross.*Volume\"]\nParotid_L: [\"LeftParotid\", \"PAROTID_L\", \"L_Parotid\"]\nParotid_R: [\"RightParotid\", \"PAROTID_R\", \"R_Parotid\"]\nCord: [\"SpinalCord\", \"Cord\", \"Spinal_Cord\"]\nMandible: [\"mandible.*\"]\n</code></pre> <p>Then run AutoPipeline with the <code>--roi-match-yaml</code> option:</p> <pre><code>imgtools autopipeline\\\n    --modalities CT,RTSTRUCT \\\n    --roi-match-yaml roi_patterns.yaml\n</code></pre>"},{"location":"usage/autopipeline/#advanced-features","title":"Advanced Features","text":""},{"location":"usage/autopipeline/#parallel-processing","title":"Parallel Processing","text":"<p>Processing large datasets? AutoPipeline can parallelize operations:</p> <pre><code>imgtools autopipeline /path/to/dicoms/ /path/to/output/ \\\n    --modalities CT,RTSTRUCT \\\n    --jobs 8  # Use 8 parallel processes\n</code></pre>"},{"location":"usage/autopipeline/#custom-output-formatting","title":"Custom Output Formatting","text":"<p>Control your output file structure with formatting patterns:</p> <pre><code>imgtools autopipeline /path/to/dicoms/ /path/to/output/ \\\n    --modalities CT,RTSTRUCT \\\n    --filename-format \"{PatientID}/{Modality}/{ImageID}.nii.gz\"\n</code></pre>"},{"location":"usage/autopipeline/#handling-existing-files","title":"Handling Existing Files","text":"<p>Choose how to handle existing files in your output directory:</p> <pre><code>imgtools autopipeline /path/to/dicoms/ /path/to/output/ \\\n    --modalities CT,RTSTRUCT \\\n    --existing-file-mode skip  # Options: skip, overwrite, fail\n</code></pre>"},{"location":"usage/autopipeline/#additional-resources","title":"Additional Resources","text":"<p>For more details on the components that AutoPipeline uses:</p> <ul> <li>Crawler Documentation</li> <li>Interlacer Documentation</li> <li>[ROI Matching and Masks] TODO::</li> </ul>"},{"location":"usage/autopipeline/#references","title":"References","text":"<ol> <li> <p>Kim S, Kazmierski M, et al. (2025). \"Med-ImageTools: An open-source Python     package for robust data processing pipelines and curating medical imaging     data.\" F1000Research, 12:118.</p> </li> <li> <p>Clark K, Vendt B, Smith K, et al. (2013). \"The Cancer Imaging Archive (TCIA):     Maintaining and Operating a Public Information Repository.\" Journal of Digital     Imaging, 26(6):1045-1057.</p> </li> </ol>"},{"location":"usage/nnunet_pipeline/","title":"<code>nnUNetPipeline</code>: Structured DICOM-to-NIfTI Conversion for Deep Learning","text":""},{"location":"usage/nnunet_pipeline/#tldr","title":"TL;DR","text":"<p><code>nnUNetPipeline</code> is a variation of <code>AutoPipeline</code> that simplifies the messy reality of clinical imaging data by converting raw DICOM files into the nnUNet standard.  </p> <p>It's fast, reproducible, and designed for segmentation workflows using CT/MR with <code>RTSTRUCT</code> or <code>SEG</code>.</p>"},{"location":"usage/nnunet_pipeline/#why-nnunetpipeline","title":"Why <code>nnUNetPipeline</code>?","text":"<p><code>nnUNet</code> is the standard for training deep automated segmentation models on medical images.</p> <p><code>nnUNetPipeline</code>:</p> <ul> <li>Crawls and indexes DICOM files</li> <li>Links scans to associated segmentations</li> <li>Applies optional transforms (e.g., resample, window/level)</li> <li>Matches ROI labels to your standard keys</li> <li>Outputs clean NIfTI files for training with <code>nnUNet</code></li> </ul>"},{"location":"usage/nnunet_pipeline/#comparison-to-autopipeline","title":"Comparison to <code>AutoPipeline</code>","text":"Feature <code>nnUNetPipeline</code> <code>Autopipeline</code> Primary Use Case Converts clinical DICOMs into nnUNet-ready General-purpose medical image processing Modalities Specific pairs of CT/MR with <code>RTSTRUCT</code> or <code>SEG</code> Flexible, supports any modality list ROI Matching Requires <code>roi_match_map</code> for nnUNet compatibility Optional, with more flexibility for handling ROIs Mask Saving Strategy Uses <code>MaskSavingStrategy</code> (e.g., <code>sparse_mask</code>) Saves each ROI as a separate mask file ROI Strategy <code>ROIMatchStrategy.MERGE</code> for overlaps Default is <code>ROIMatchStrategy.SEPARATE</code>"},{"location":"usage/nnunet_pipeline/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Supports modality pairs: <code>[\"CT\", \"SEG\"]</code>, <code>[\"MR\", \"SEG\"]</code>, <code>[\"CT\", \"RTSTRUCT\"]</code>, <code>[\"MR\", \"RTSTRUCT\"]</code></li> <li>\u2705 Supports different <code>mask_saving_strategy</code> options (<code>\"label_image\"</code>, <code>\"sparse_mask\"</code>, <code>\"region_mask\"</code>)</li> <li>\u2705 Matches ROIs via regex patterns (<code>\"GTV\": [\"gtv\", \"Gross.*Volume\"]</code>)</li> <li>\u2705 Provides <code>nnUNet</code>-specific scripts for preprocessing and training</li> </ul>"},{"location":"usage/nnunet_pipeline/#mask-saving-options","title":"Mask Saving Options","text":"<p>When converting segmentations (e.g., from <code>RTSTRUCT</code> or <code>SEG</code>) into NIfTI masks for training, you have multiple options depending on how you want to handle overlapping regions and label representation.</p>"},{"location":"usage/nnunet_pipeline/#label_image","title":"<code>label_image</code>","text":"<ul> <li>Single-channel label image </li> <li>Each voxel contains an integer label corresponding to a single region.</li> <li>Limitations:</li> <li>Does not allow overlapping ROIs.</li> <li>Each voxel can belong to only one region.</li> <li>Use this when you're certain there are no overlaps, and you want compatibility with standard segmentation workflows.</li> </ul>"},{"location":"usage/nnunet_pipeline/#sparse_mask","title":"<code>sparse_mask</code>","text":"<ul> <li>Single-channel label image </li> <li>Each voxel contains an integer label corresponding to a single region.</li> <li>Overlapping regions are allowed in the input, but only one label is assigned per voxel in the output.</li> <li>When overlaps occur, the label from the last matching region (highest index) is assigned, and earlier labels are overwritten.</li> <li>User can decide the order when passing in ROI map.</li> <li>Lossy if regions overlap, since only one region label is retained per voxel and all others are discarded.</li> </ul>"},{"location":"usage/nnunet_pipeline/#region_mask-recommended-for-nnu-net","title":"<code>region_mask</code> (Recommended for nnU-Net)","text":"<ul> <li>Bitmask encoding of overlapping ROIs </li> <li>Inspired by nnU-Net\u2019s region-based training.</li> <li>Every unique combination of ROIs gets its own integer label \u2014 preserving overlap information without loss.</li> <li>Enables multi-class region training with full overlap preservation.</li> </ul>"},{"location":"usage/nnunet_pipeline/#how-it-works","title":"How It Works","text":"<p>Internally, this uses a bitmask approach. Each voxel stores a value representing a combination of active ROI channels (up to 32 ROIs supported).</p> <p>Imagine you have three regions of interest (ROIs):</p> <ul> <li><code>GTV</code> assigned to bit 0 \u2192 value <code>2^0 = 1</code></li> <li><code>Cord</code> assigned to bit 1 \u2192 value <code>2^1 = 2</code></li> <li><code>Parotid_L</code> assigned to bit 2 \u2192 value <code>2^2 = 4</code></li> </ul> <p>Each voxel in the mask encodes a combination of these ROIs using bitwise addition.</p> Voxel Location Present ROIs Bitmask Calculation Encoded Value (10, 30, 40) GTV <code>2^0</code> 1 (12, 30, 40) Cord <code>2^1</code> 2 (15, 30, 40) Parotid_L <code>2^2</code> 4 (17, 30, 40) GTV + Parotid_L <code>2^0 + 2^2 = 2^0 + 2^2 = 1 + 4</code> 5 (20, 30, 40) GTV + Cord + Parotid_L <code>2^0 + 2^1 + 2^2 = 1 + 2 + 4</code> 7"},{"location":"usage/nnunet_pipeline/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":"Option Overlaps Lossless <code>label_image</code> \u274c \u2705 <code>sparse_mask</code> \u2705 \u274c <code>region_mask</code> \u2705 \u2705"},{"location":"usage/nnunet_pipeline/#example-cli-usage","title":"Example CLI Usage","text":"<pre><code>imgtools nnunet-pipeline \\\n    --modalities CT,RTSTRUCT \\\n    --roi-match-yaml roi_patterns.yaml \\\n    --mask-saving-strategy region_mask \\\n    /data/dicoms/ \\\n    /data/nnunet_ready/\n</code></pre>"},{"location":"usage/Writers/BaseWriter/","title":"Abstract Base Writer","text":"<p>The <code>AbstractBaseWriter</code> class is the foundation for all writers in this library.</p> <p>It provides a standard interface, reusable methods, and tools that writers can extend to handle file writing tasks efficiently and consistently.</p> <p>If you're building a writer to manage file outputs with custom paths, filenames, or formats, this is where you start!</p> <p>For details on implementing the <code>AbstractBaseWriter</code> in your custom writer, see the Implementing Writers guide.</p>"},{"location":"usage/Writers/BaseWriter/#introduction","title":"Introduction","text":""},{"location":"usage/Writers/BaseWriter/#what-is-the-abstractbasewriter","title":"What is the <code>AbstractBaseWriter</code>?","text":"<p>The <code>AbstractBaseWriter</code> is:</p> <ul> <li>A reusable template: Manage file-writing tasks consistently across different writer implementations.  </li> <li>Customizable: Extend it to handle your file formats and workflows.  </li> <li>Safe and robust: Features context management, filename sanitization, and optional CSV indexing.  </li> </ul>"},{"location":"usage/Writers/BaseWriter/#when-should-you-extend-abstractbasewriter-for-your-custom-writer","title":"When should you extend <code>AbstractBaseWriter</code> for your custom writer?","text":"<p>If you write many files with dynamic paths and filenames, or need to manage file existence scenarios, you might consider extending <code>AbstractBaseWriter</code> (or even one of its subclasses) to simplify your implementation.</p> <p><code>AbstractBaseWriter</code> is useful when you need:</p> <ul> <li>Dynamic paths and filenames (e.g., <code>{subject_id}/{study_date}.nii.gz</code>).  </li> <li>Configurable handling of existing files (<code>OVERWRITE</code>, <code>SKIP</code>, etc.).  </li> <li>Logging of saved files via an optional CSV index.  </li> <li>Thread-safe and multiprocessing-compatible file writing.  </li> <li>A consistent interface across different types of writers.  </li> </ul>"},{"location":"usage/Writers/BaseWriter/#core-concepts","title":"Core Concepts","text":""},{"location":"usage/Writers/BaseWriter/#root-directory-and-filename-format-parameters","title":"Root Directory and Filename Format Parameters","text":"<p>Root Directory:</p> <ul> <li>Base folder for all saved files, automatically created if missing (via <code>create_dirs</code> parameter)</li> </ul> <p>Filename Format:</p> <ul> <li>A string template defining your file and folder names.  </li> <li>Uses placeholders like <code>{key}</code> to insert context values dynamically.  </li> </ul> <p>Example:</p> <pre><code>writer = ExampleWriter(\n    root_directory=\"./data\",\n    filename_format=\"{person_name}/{date}_{message_type}.txt\",\n)\n\n# Save a file with context variables\ndata = \"Hello, World!\"\nwriter.save(\n  data, \n  person_name=\"JohnDoe\",\n  date=\"2025-01-01\",\n  message_type=\"greeting\"\n)\n\n# Saved file path: \n# ./data/JohnDoe/2025-01-01_greeting.txt\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#file-existence-modes","title":"File Existence Modes","text":"<p>Why It Matters:</p> <ul> <li>When your writer saves a file, it needs to decide what to do if a file with the same name already exists.</li> <li>This is especially important in batch operations or when writing to shared directories.</li> <li>The <code>AbstractBaseWriter</code> provides several options to handle this scenario through the use of   an <code>enum</code> called <code>ExistingFileMode</code>.</li> </ul> <p>It is important to handle these options carefully in your writer's <code>save()</code> method to avoid data loss or conflicts.</p>"},{"location":"usage/Writers/BaseWriter/#imgtools.io.writers.ExistingFileMode","title":"ExistingFileMode","text":"<p>               Bases: <code>str</code>, <code>enum.Enum</code></p> <p>Enum to specify handling behavior for existing files.</p> <p>Attributes:</p> Name Type Description <code>OVERWRITE</code> <code>str</code> <p>Overwrite the existing file. Logs as debug and continues with the operation.</p> <code>FAIL</code> <code>str</code> <p>Fail the operation if the file exists. Logs as error and raises a FileExistsError.</p> <code>SKIP</code> <code>str</code> <p>Skip the operation if the file exists. Meant to be used for previewing the path before any expensive computation. <code>preview_path()</code> will return None if the file exists. <code>resolve_path()</code> will still return the path even if the file exists. The writer's <code>save</code> method should handle the file existence if set to SKIP.</p>"},{"location":"usage/Writers/BaseWriter/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"usage/Writers/BaseWriter/#sanitizing-filenames","title":"Sanitizing Filenames","text":"<p>Why Sanitize Filenames?:</p> <ul> <li>To ensure that filenames are safe and compatible across different operating systems.  </li> </ul> <p>How It Works:</p> <ul> <li>Replaces illegal characters (e.g., <code>&lt;</code>, <code>&gt;</code>, <code>:</code>, <code>\"</code>, <code>/</code>, <code>\\</code>, <code>|</code>, <code>?</code>, <code>*</code>) with underscores.  </li> <li>Trims leading or trailing spaces and periods to avoid issues.</li> </ul> <p>When Is It Applied?:</p> <ul> <li>Automatically applied when generating filenames, unless disabled by setting <code>sanitize_filenames=False</code>.</li> </ul>"},{"location":"usage/Writers/BaseWriter/#multiprocessing-compatibility","title":"Multiprocessing Compatibility","text":"<p>Why It Matters:</p> <ul> <li>In batch operations or high-performance use cases, multiple processes may write files simultaneously.  </li> </ul> <p>Key Features:</p> <ul> <li>Supports multiprocessing with inter-process locking to ensure thread-safe file writes.  </li> <li>Avoids conflicts or data corruption when multiple instances of a writer are running.</li> </ul>"},{"location":"usage/Writers/BaseWriter/#lifecycle-management","title":"Lifecycle Management","text":"<p>Context Manager Support:</p> <ul> <li>Writers can be used with <code>with</code> statements to ensure proper setup and cleanup.  </li> </ul> <p>What Happens on Exit?:</p> <ul> <li>Removes lock files used for the index file.  </li> <li>Deletes empty directories created during the writing process (if no files were written).  </li> </ul> <p>Example:</p> <pre><code>with TextWriter(root_directory=\"/data\", filename_format=\"{id}.txt\") as writer:\n  data = \"Hello, World!\"\n  writer.save(data, id=\"1234\")\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#previewing-file-paths-and-caching-context","title":"Previewing File Paths and Caching Context","text":"<p>In the simplest usage of a writer, users can pass in the context information as keyword arguments to each <code>save()</code> call.</p> <p>However, this can become cumbersome when the same context variables are used across multiple save operations.</p> <p>Example:</p> <p>In the above example, the <code>date</code> and <code>message_type</code> context variables are the same for all students. Instead of passing them in every time, you can store these variables in the writer itself and update them as needed.</p> <p>Let's use the following example to illustrate this:</p> <p>Say we want to save greetings for students in a particular highschool class:</p> <pre><code>writer = TextWriter(\n    root_directory=\"./data\",\n    filename_format=\"{grade}/{class_subject}/{person_name}/{date}_{message_type}.txt\",\n)\n</code></pre> <p>Basic Usage</p> <p>We see here that the context variables for <code>grade</code>,   <code>class_subject</code>, <code>date</code>, and <code>message_type</code>   are the same for all students.</p> <p>This can become even worse with more   context variables, allowing for mistakes, and making the code harder to read.</p> <pre><code>student, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(\n    message,\n    person_name=student,\n    grade=\"12\",\n    class_subject=\"math\",\n    date=\"2025-01-01\",\n    message_type=\"greeting\"\n)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(\n    message,\n    person_name=student,\n    grade=\"12\",\n    class_subject=\"math\",\n    date=\"2025-01-01\",\n    message_type=\"greeting\"\n)\n</code></pre> <p>Setting Context Variables manually</p> <p>Instead of passing in the context variables every time,   you can store these variables in the writer and update them as needed   using the <code>set_context()</code> method.</p> <p>Then only pass in the unique context variables for each <code>.save()</code> operation.</p> <pre><code>writer.set_context(\n  grade=\"12\",\n  class_subject=\"math\",\n  date=\"2025-01-01\",\n  message_type=\"greeting\"\n)\n\nstudent, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(message, person_name=student)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(message, person_name=student)\n</code></pre> <p>Setting Context Variables during Initialization</p> <p>If majority of the context variables are the same across all save    operations, you can set context when initializing the writer.</p> <p>Note that here, we must pass as a dictionary to the <code>context</code> parameter    instead of individual keyword arguments.</p> <pre><code>writer = TextWriter(\n    root_directory=\"./data\",\n    filename_format=\"{class_subject}/{person_name}/{date}_{message_type}.txt\",\n    context={\"grade\": \"12\", \"class_subject\": \"math\", \"date\": \"2025-01-01\", \"message_type\": \"greeting\"}\n)\n\nstudent, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(message, person_name=student)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(message, person_name=student)\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#previewing-file-paths","title":"Previewing File Paths","text":"<p>Oftentimes, you may want to check if a file exists before performing an expensive computation. If you set the existence mode to <code>ExistingFileMode.SKIP</code>, the <code>preview_path()</code> method will return <code>None</code> if the file already exists, allowing you to skip the computation.</p> <p>This method also caches the additional context variables for future use.</p> <p>Here's an example of how you might handle this:</p> <pre><code># assuming writer is already initialized with `existing_file_mode=ExistingFileMode.SKIP`\n\n# set some context variables\nwriter.set_context(class_subject=\"math\", date=\"2025-01-01\", message_type=\"greeting\")\n\nif (path := writer.preview_path(person_name=\"Alice\")) is None:\n    print(\"File already exists, skipping computation.\")\nelse:\n    print(f\"Proceed with computation for {path}\")\n    ... \n    # perform expensive computation \n    ...\n    writer.save(content=\"Hello, world!\")\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#index-file-management","title":"Index File Management","text":"<p>What is the Index File?:</p> <ul> <li>A CSV file used to log details about saved files, like their paths and context variables.  </li> <li>Helps track what files have been written, especially useful in batch operations.</li> <li>Additionally can save all the context variables used for each file, convienient for   saving additional metadata, while improving traceability.</li> </ul> <p>How It Works:</p> <ul> <li>The AbstractBaseWriter now uses the powerful <code>IndexWriter</code> class to handle all index operations</li> <li>By default, the index file is named <code>{root_directory.name}_index.csv</code></li> <li>You can customize the filename or provide an absolute path for more control</li> <li>When implementing a writer class, call <code>add_to_index(path)</code> in your <code>save()</code> method to record saved files</li> </ul> <p>Key Features:</p> <ul> <li>Customizable Filename: Use <code>index_filename</code> to set a custom name or absolute path.</li> <li>Absolute/Relative Paths: Control file paths in the index with <code>absolute_paths_in_index</code> (defaults to relative).</li> <li>Schema Evolution: Control schema evolution with the <code>merge_columns</code> parameter when calling <code>add_to_index()</code>.</li> <li>Safe Concurrent Access: Uses inter-process locking for thread-safe operations in multi-process environments.</li> <li>Robust Error Handling: Specific exceptions for index-related errors to help troubleshoot issues.</li> </ul> <p>Using the add_to_index Method:</p> <pre><code># In your writer's save method:\ndef save(self, content, **kwargs):\n    output_path = self.resolve_path(**kwargs)\n\n    # Write your content to the file...\n\n    # Record this file in the index, with optional parameters:\n    self.add_to_index(\n        path=output_path,\n        include_all_context=True,   # Include all context variables, not just those used in the filename\n        filepath_column=\"path\",     # Name of the column to store file paths\n        replace_existing=False,     # Whether to replace existing entries for the same file\n        merge_columns=True          # Whether to allow schema evolution\n    )\n\n    return output_path\n</code></pre> <p>Schema Evolution with merge_columns:</p> <p>The <code>merge_columns</code> parameter (defaults to <code>True</code>) controls how the IndexWriter handles changes to your data schema:</p> <p>When <code>True</code>: If your context has new fields that didn't exist in previous CSV entries, they'll be added as new columns. This is great for:</p> <ul> <li>Iterative development when you're adding new metadata fields</li> <li>Different processes writing files with slightly different context variables</li> <li>Ensuring backward compatibility with existing index files</li> </ul> <p>When <code>False</code>: Strict schema enforcement is applied. The IndexWriter will raise an error if the columns don't match exactly what's already in the index file. This is useful when:</p> <ul> <li>You want to enforce a consistent schema across all entries</li> <li>You're concerned about typos or unintended fields creeping into your index</li> <li>Data consistency is critical for downstream processing</li> </ul>"},{"location":"usage/Writers/ImplementingWriters/","title":"Extending the <code>AbstractBaseWriter</code> class","text":"<p>The <code>AbstractBaseWriter</code> is designed to be extended, allowing you to create custom writers tailored to your specific needs. This guide will walk you through the steps to extend the class and implement your custom functionality.</p>"},{"location":"usage/Writers/ImplementingWriters/#setting-up-your-writer","title":"Setting Up Your Writer","text":"<p>To create a custom writer, you need to extend the <code>AbstractBaseWriter</code> and implement the <code>save</code> method. This method is the core of your writer, handling how and where data is saved.</p> <p>For a walkthrough of all key methods and features, see the Key Methods section below.</p>"},{"location":"usage/Writers/ImplementingWriters/#steps-to-set-up","title":"Steps to Set Up","text":"<ol> <li> <p>Inherit from <code>AbstractBaseWriter</code>:    Create a new class and extend <code>AbstractBaseWriter</code> with the appropriate type.    If you are saving text data, use <code>AbstractBaseWriter[str]</code>, for example.    If you are saving image data, use <code>AbstractBaseWriter[sitk.Image]</code>.</p> </li> <li> <p>Define the <code>save</code> Method:   Use <code>resolve_path()</code> or <code>preview_path()</code> to generate file paths.   Implement the logic for saving data.  </p> </li> <li> <p>Customize Behavior (Optional):   Override any existing methods for specific behavior.   Add additional methods or properties to enhance functionality.  </p> </li> </ol>"},{"location":"usage/Writers/ImplementingWriters/#simple-example","title":"Simple Example","text":"<pre><code>from pathlib import Path\nfrom imgtools.io import AbstractBaseWriter\n\nclass MyCustomWriter(AbstractBaseWriter[str]):\n    def save(self, content: str, **kwargs) -&gt; Path:\n        # Resolve the output file path\n        output_path = self.resolve_path(**kwargs)\n\n        # Write content to the file\n        with output_path.open(mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        # Log and track the save operation using the new IndexWriter\n        self.add_to_index(output_path)\n\n        return output_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#implementing-the-save-method","title":"Implementing the <code>save</code> Method","text":"<p>The <code>save</code> method is the heart of your custom writer. It determines how data is written to files and interacts with the core features of <code>AbstractBaseWriter</code>.</p>"},{"location":"usage/Writers/ImplementingWriters/#key-responsibilities-of-save","title":"Key Responsibilities of <code>save</code>","text":"<ol> <li> <p>Path Resolution:</p> <ul> <li>Use <code>resolve_path()</code> to dynamically generate file paths based on the provided     context and filename format.</li> <li>You can optionally use <code>preview_path()</code> as well.</li> <li>Ensure paths are validated to prevent overwriting or duplication.</li> </ul> </li> <li> <p>Data Writing:  </p> <ul> <li>Define how the content will be written to the resolved path.  </li> <li>Use file-handling best practices to ensure reliability.</li> </ul> </li> <li> <p>Logging and Tracking:  </p> <ul> <li>Log each save operation for debugging or auditing purposes.  </li> <li>Use <code>add_to_index()</code> to maintain a record of saved files and their associated     context variables.</li> </ul> </li> <li> <p>Return Value:  </p> <ul> <li>Return the <code>Path</code> object representing the saved file.  </li> <li>This allows users to access the file path for further processing or verification.</li> </ul> </li> </ol>"},{"location":"usage/Writers/ImplementingWriters/#example-implementation","title":"Example Implementation","text":"<p>Here's a minimal implementation of the <code>save</code> method for a custom writer.</p> <pre><code>from pathlib import Path\nfrom mypackage.abstract_base_writer import AbstractBaseWriter\n\nclass MyCustomWriter(AbstractBaseWriter[str]):\n    def save(self, content: str, **kwargs) -&gt; Path:\n        # Step 1: Resolve the output file path\n        # you can try-catch this in case set to \"FAIL\" mode\n        # or just let the error propagate\n        output_path = self.resolve_path(**kwargs) # resolve_path will always return the path\n\n        # OPTIONAL handling for \"SKIP\" modes\n        if output_path.exists():\n            # this will only be true if the file existence mode\n            # is set to SKIP\n            # - OVERWRITE will have already deleted the file\n            # - upto developer to choose to handle this if set to SKIP\n            pass\n\n        # Step 2: Write the content to the resolved path\n        with output_path.open(mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        # Step 3: Log and track the save operation\n        self.add_to_index(\n            path=output_path,\n            include_all_context=True,\n            filepath_column=\"filepath\", \n            replace_existing=False,\n            merge_columns=True,\n        )\n\n        # Step 4: ALWAYS Return the saved file path\n        return output_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#key-methods","title":"Key Methods","text":"<p>The <code>AbstractBaseWriter</code> provides several utility methods that simplify file writing and context management. These methods are designed to be flexible and reusable, allowing you to focus on your custom implementation.</p>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#add_to_index","title":"add_to_index","text":"<p>What It Does:</p> <ul> <li>Records file information in a centralized CSV index file using the powerful IndexWriter</li> <li>Safely handles concurrent writes with inter-process locking</li> <li>Supports schema evolution to handle changing metadata fields</li> </ul> <p>When to Use It:</p> <ul> <li>Call this method from your <code>save()</code> implementation to track files</li> <li>Great for batch operations where you need to maintain records of processed files</li> </ul> <p>Usage Example:</p> <pre><code>def save(self, content, **kwargs):\n    path = self.resolve_path(**kwargs)\n    # ... write content to file ...\n\n    # Add entry to index with all context variables\n    self.add_to_index(\n        path=path,\n        include_all_context=True,  # Include ALL context vars (not just those in filename)\n        filepath_column=\"path\",    # Column name for file paths\n        replace_existing=False     # Whether to replace existing entries\n    )\n\n    return path\n</code></pre> <p>Important Parameters:</p> <ul> <li><code>include_all_context</code>: Controls whether to save all context variables or only those used in the filename</li> <li><code>filepath_column</code>: Customizes the column name for file paths</li> <li><code>replace_existing</code>: Whether to replace or append entries for the same file</li> </ul> <p>Error Handling:</p> <p>The method uses robust error handling with specific exceptions like <code>WriterIndexError</code> that wrap any underlying IndexWriter errors, making troubleshooting easier.</p> <p>What It Does:</p> <ul> <li>A helper method for resolving file paths based on the current context and   filename format.  </li> <li>Automatically sanitizes filenames if <code>sanitize_filenames=True</code>.</li> </ul> <p>When to Use It:</p> <ul> <li>Typically called internally by <code>resolve_path()</code> and <code>preview_path()</code>, which handle   additional validation and error handling.</li> <li>Can be called by your class methods to generate paths without the additional   context checks.</li> </ul> <p>Example:</p> <pre><code>custom_path = writer._generate_path(subject=\"math\", name=\"example\")\nprint(f\"Generated path: {custom_path}\")\n</code></pre> <p>By using these key methods effectively, you can customize your writer to handle a wide range of file-writing scenarios while maintaining clean and consistent logic.</p>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter._generate_path","title":"_generate_path","text":"<pre><code>_generate_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Helper for resolving paths with the given context.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def _generate_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Helper for resolving paths with the given context.\n    \"\"\"\n    save_context = {\n        **self.context,\n        **kwargs,\n        \"saved_time\": datetime.now(timezone.utc).strftime(\n            \"%Y-%m-%d:%H:%M:%S\"\n        ),\n    }\n    self.set_context(**save_context)\n    try:\n        filename = self.pattern_resolver.resolve(save_context)\n    except MissingPlaceholderValueError as e:\n        # Replace the class name in the error message dynamically\n        raise MissingPlaceholderValueError(\n            e.missing_keys,\n            class_name=self.__class__.__name__,\n            key=e.key,\n        ) from e\n    if self.sanitize_filenames:\n        filename = sanitize_file_name(filename)\n    out_path = self.root_directory / filename\n    # logger.debug(\n    #     f\"Resolved path: {out_path} and {out_path.exists()=}\",\n    #     handling=self.existing_file_mode,\n    # )\n    return out_path\n</code></pre>"},{"location":"usage/core/crawler/","title":"Crawler","text":"<p>The <code>Crawler</code> in Med-ImageTools has the core responsibility of walking through a directory and collecting all the dicom files (<code>.dcm</code>/<code>.DCM</code>), regardless of the directory structure. </p> <p>It was designed to extract as much information as possible from the files, to build an internal database of the data users have.</p>"},{"location":"usage/core/crawler/#metadata-extraction","title":"Metadata extraction","text":"<p>As of the 2.0 release, the crawler implements a modality-specific set of extractions which facilitates: </p> <ol> <li>raw extraction of a DICOM tag (i.e <code>SeriesInstanceUID</code>)</li> <li>and possible 'ComputedValue' which requires computation on one or more tags (i.e <code>ROINames</code> in <code>RTSTRUCT</code> files are computed from the <code>ROIContourSequence</code> tag).</li> </ol> <p>This is done using the ModalityMedataExtractor base class, which defines the <code>base_tags</code> that are common to all modalities, and the <code>computed_tags</code> which sub-classes can implement to extract modality-specific information on top of their own <code>modality_tags</code>.</p> <p>See the following api documentation for the supported modalities:</p> <ul> <li>CTMetadataExtractor</li> <li>MRMetadataExtractor</li> <li>PTMetadataExtractor</li> <li>SEGMetadataExtractor</li> <li>RTSTRUCTMetadataExtractor</li> <li>RTDOSEMetadataExtractor</li> <li>RTPLANMetadataExtractor</li> <li>SRMetadataExtractor</li> </ul> <p>For each of the above modalities, there is a <code>modality_tags</code> property that returns a set of tags that can be directly extracted from the DICOM file. There also may be a <code>computed_tags</code> property that returns a mapping of  keys defined by Med-ImageTools (i.e <code>SegSpacing</code> in the <code>SEG</code> extractor) to a callable function that will compute the value from the DICOM file.</p> <p>how can I find what tags are extracted?</p> <p>You can view the <code>modality_tags</code> and <code>computed_tags</code> for each modality in the above links, and open the <code>Souce code</code> dropdown for each property.</p>"},{"location":"usage/core/crawler/#assumptions-made-when-crawling","title":"Assumptions made when crawling","text":"<p>As of 2.0, the crawler makes the following assumptions:</p> <ol> <li> <p>All DICOM files within the specified input directory are valid DICOM files.     That is, they all contain a valid DICOM header and can be read by pydicom.</p> </li> <li> <p>All DICOM files belonging to the same series are in the same directory.     This assumption is made, to simplify pointing to a series of files, by      just pointing to the directory containing the files.</p> </li> </ol> <p>Note</p> <p>Though we require that all files in a series are in the same directory, we do not require that all files in a directory belong to the same series. That is, a directory may contain files from multiple series, and the crawler will still be able to extract the series information from each file.</p>"},{"location":"usage/core/crawler/#reference-building","title":"Reference building","text":"<p>One of the main features of Med-ImageTools is building the internal database of all the complex relationships between DICOM series. The first step in this process is to extract the necessary information from each DICOM file, which is modality-specific, and given the evolution of the DICOM standard, may change over time.</p> <p>For example, while modern <code>RTSTRUCT</code> files contain a <code>RTReferencedSeriesSequence</code> tag that references the <code>SeriesInstanceUID</code> of the series that the structure was derived from, this is not always present in older files. As such, we have to rely on the <code>ReferencedSOPInstanceUID</code> tag, which is a unique identifier for all the files in the Referenced Series.</p> <p>During the crawling process, we also build a database that maps each found DICOM file to its corresponding series <code>SOPInstanceUID -&gt; SeriesInstanceUID</code>.</p> <p>If the <code>RTSTRUCT</code> file does not contain the <code>RTReferencedSeriesSequence</code>, we use all the <code>ReferencedSOPInstanceUID</code> tags in the file to find the corresponding seires using the mapping.</p> <p>After a successful crawl, the <code>Crawler</code> will have two database interfaces:</p> <ol> <li> <p><code>crawl_db</code>: This is the database that contains all the information     extracted from the DICOM files.     It is a dictionary mapping the <code>SeriesInstanceUID</code> to all the metadatal     extracted from the files in the series using the     <code>ModalityMetadataExtractor</code>     extractors.</p> </li> <li> <p><code>index</code>: This is a slimmed database that contains the core metadata     to build a representation of the data relationships.</p> <p>It contains the following information:</p> <ol> <li><code>PatientID</code></li> <li><code>StudyInstanceUID</code></li> <li><code>SeriesInstanceUID</code></li> <li><code>SubSeries</code> (possible unique Acquisition)</li> <li><code>Modality</code></li> <li><code>ReferencedModality</code></li> <li><code>ReferencedSeriesUID</code></li> <li><code>instances</code> (number of instances in the series)</li> <li><code>folder</code> (path to the folder containing the series)</li> </ol> </li> </ol> <p>These files are stored in a <code>.imgtools</code> directory next to the input directory.</p> <p>That is if the input directory is <code>/path/to/DICOM_FILES</code>, the index output files will be stored in <code>/path/to/.imgtools/DICOM_FILES</code>.</p>"},{"location":"usage/core/interlacer/","title":"Interlacer Module","text":"<p>The Interlacer module builds and searches a tree-like structure made from DICOM series using metadata links. </p> <p>This module enables efficient grouping, querying, and vizualization of medical imaging series. The module turns DICOM series into a set of trees (a forest), using metadata to connect related series. This helps users follow the relationships between series \u2014 for example, linking a <code>CT</code> scan to its <code>RTSTRUCT</code> and <code>RTDOSE</code> \u2014 and makes it easier to run queries or group series by type.</p> <p>Note</p> <p>This module is available in <code>med-imagetools 2.0</code> and later versions. It replaces the now deprecated <code>DataGraph</code> module from <code>med-imagetools 1.0</code>.</p>"},{"location":"usage/core/interlacer/#usage-example","title":"Usage Example","text":""},{"location":"usage/core/interlacer/#cli","title":"CLI","text":""},{"location":"usage/core/interlacer/#python-code","title":"Python Code","text":"<pre><code>from pathlib import Path\nfrom imgtools.dicom.crawl import Crawler\nfrom imgtools.dicom.interlacer import Interlacer\n\n# Define path to DICOM directory\ndicom_dir = Path(\"data/\")\n\n# Create crawler and scan the directory\ncrawler = Crawler(\n    dicom_dir=dicom_dir,\n    n_jobs=5,\n    force=False,\n)\ncrawler.crawl()\n\n# Create the Interlacer from crawler results\ninterlacer = Interlacer(crawler.index)\n\n# Visualize the forest structure\ninterlacer.print_tree(dicom_dir)  # Console visualization\n\n# Query for specific modality combinations\n# Find CT series with associated RTSTRUCT objects\nct_rtstruct_results = interlacer.query(\"CT,RTSTRUCT\")\n\n# Get all possible series combinations\nall_results = interlacer.query(\"*\")  # or interlacer.query(\"all\")\n</code></pre>"},{"location":"usage/core/interlacer/#example-output","title":"Example Output","text":"<p>Visualization of the raw graph containing all series in the DICOM directory:</p> <p></p> <p>Visualization of The interlaced connections between series:</p> <p></p> <p>Note</p>"},{"location":"usage/core/interlacer/#query-rules-and-dependencies","title":"Query Rules and Dependencies","text":"<p>The Interlacer enforces the following modality dependency rules:</p> <ol> <li><code>RTSTRUCT</code> and <code>RTDOSE</code> require a <code>CT</code>, <code>MR</code>, or <code>PT</code> series</li> <li><code>SEG</code> requires a <code>CT</code> or <code>MR</code> series</li> </ol> <p>Examples of valid and invalid queries:</p> <ul> <li>\u2705 <code>\"CT,RTDOSE\"</code> - Valid: CT with associated RTDOSE</li> <li>\u2705 <code>\"CT,PT,RTSTRUCT\"</code> - Valid: CT and PT with associated RTSTRUCT</li> <li>\u274c <code>\"PT,SEG\"</code> - Invalid: SEG requires CT or MR, not PT</li> <li>\u274c <code>\"RTSTRUCT,RTDOSE\"</code> - Invalid: Both require a source imaging series</li> </ul>"},{"location":"usage/core/interlacer/#main-classes","title":"Main Classes","text":""},{"location":"usage/core/interlacer/#seriesnode","title":"<code>SeriesNode</code>","text":"<p>Represents an individual DICOM series and its hierarchical relationships:</p> <ul> <li>Attributes:</li> <li><code>SeriesInstanceUID</code></li> <li><code>Modality</code></li> <li><code>PatientID</code></li> <li><code>StudyInstanceUID</code></li> <li><code>folder</code>: Path to the folder containing the DICOM files</li> <li><code>ReferencedSeriesUID</code>: Series that this one references, if any</li> <li><code>children</code>: List of child nodes representing referenced series<ul> <li>i.e a <code>CT</code> series might have 1 or more links to <code>RTSTRUCT</code> and/or  <code>PT</code> series</li> </ul> </li> </ul>"},{"location":"usage/core/interlacer/#interlacer","title":"<code>Interlacer</code>","text":"<p>Key features:</p> <ul> <li>Query validation: Ensures that modality queries follow DICOM standard requirements</li> <li>Interactive visualization: Creates HTML-based network graphs of relationships</li> <li>Rich text console display: Pretty-prints the hierarchy with color-coding</li> <li>Dependency validation: Enforces rules like \"RTSTRUCT requires CT, MR, or PT\"</li> </ul> Supported Modalities <p>Though the Crawler will indiscriminately crawl all DICOM files, the Interlacer currently only supports the following modalities for interlacing:</p> <pre><code>- CT\n- MR\n- PT\n- RTSTRUCT\n- RTDOSE\n- SEG\n- RTPLAN (used in interlacing but not in queries)\n</code></pre> <p>These were the primary modalities targeted during the development of this module, but can easily be extended to support others in the future. Please open an issue for which modalities you would like to see supported.</p>"},{"location":"usage/core/interlacer/#grouping-series","title":"Grouping Series","text":"<p>The Interlacer currently groups series using Referenced Series UID, which links series based on their metadata references  (i.e the <code>ReferencedSeriesInstanceUID</code> tag in <code>RTSTRUCT</code>s). This creates a hierarchical structure showing the relationships between different series.</p> Future Support for Grouping by Study Instance UID and Patient ID <p>In a future release, the Interlacer will support additional grouping methods:</p> <ul> <li>Study Instance UID \u2013 Group everything in the same study</li> <li>Patient ID \u2013 Group all series from the same patient</li> </ul> <p>This enhancement is being tracked in GitHub issue #318.</p>"}]}