{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Med-Imagetools: Transparent and Reproducible Medical Image Processing Pipelines in Python","text":""},{"location":"#med-imagetools-core-features","title":"Med-ImageTools core features","text":"<ul> <li>AutoPipeline CLI</li> <li><code>nnunet</code> nnU-Net compatibility mode</li> <li>Built-in train/test split for both normal/nnU-Net modes</li> <li><code>random_state</code> for reproducible seeds</li> <li>Region of interest (ROI) yaml dictionary intake for RTSTRUCT processing</li> <li>Markdown report output post-processing</li> <li><code>continue_processing</code> flag to continue autopipeline</li> <li><code>dry_run</code> flag to only crawl the dataset</li> </ul> <p>Med-Imagetools, a python package offers the perfect tool to transform messy medical dataset folders to deep learning ready format in few lines of code. It not only processes DICOMs consisting of different modalities (like CT, PET, RTDOSE and RTSTRUCTS), it also transforms them into deep learning ready subject based format taking the dependencies of these modalities into consideration.  </p>"},{"location":"#introduction","title":"Introduction","text":"<p>A medical dataset, typically contains multiple different types of scans for a single patient in a single study. As seen in the figure below, the different scans containing DICOM of different modalities are interdependent on each other. For making effective machine learning models, one ought to take different modalities into account.</p> <p></p> <p>Fig.1 - Different network topology for different studies of different patients</p> <p>Med-Imagetools is a unique tool, which focuses on subject based Machine learning. It crawls the dataset and makes a network by connecting different modalities present in the dataset. Based on the user defined modalities, med-imagetools, queries the graph and process the queried raw DICOMS. The processed DICOMS are saved as nrrds, which med-imagetools converts to torchio subject dataset and eventually torch dataloader for ML pipeline.</p> <p></p> <p>Fig.2 - Med-Imagetools AutoPipeline diagram</p>"},{"location":"#installing-med-imagetools","title":"Installing med-imagetools","text":"<pre><code>pip install med-imagetools\n</code></pre>"},{"location":"#create-new-conda-virtual-environment","title":"Create new conda virtual environment","text":"<pre><code>conda create -n mit\nconda activate mit\npip install med-imagetools\n</code></pre>"},{"location":"#create-a-pixi-environment","title":"Create a <code>pixi</code> environment","text":"<pre><code>pixi init mit\npixi add --pypi med-imagetools\n</code></pre>"},{"location":"#optional-install-in-development-mode","title":"(optional) Install in development mode","text":"<pre><code>conda create -n mit\nconda activate mit\npip install -e git+https://github.com/bhklab/med-imagetools.git\n</code></pre> <p>This will install the package in editable mode, so that the installed package will update when the code is changed.</p>"},{"location":"database_report/","title":"Database Report","text":""},{"location":"database_report/#number-of-patients-100","title":"Number of Patients: 100","text":""},{"location":"database_report/#number-of-studies-315","title":"Number of Studies: 315","text":""},{"location":"database_report/#number-of-series-3123","title":"Number of Series: 3123","text":""},{"location":"database_report/#modality-summary","title":"Modality Summary","text":"Modality Count MR 152 SEG 68 CT 138 OT 1 PT 10 RTSTRUCT 16 RTDOSE 16 RTPLAN 13 DX 8 SR 10 MG 5 NM 1 CR 11"},{"location":"database_report/#data-summary","title":"Data Summary","text":"Patient ID Number of Studies Number of Series Unique Modalities TCGA-06-0184 12 183 {'SEG', 'MR'} TCGA-06-0185 16 176 {'SEG', 'MR'} TCGA-BB-A5HY 11 121 {'CT', 'OT', 'MR'} TCGA-06-0188 9 110 {'SEG', 'MR'} TCGA-CV-7090 15 93 {'PT', 'CT', 'MR'} TCGA-06-0138 6 90 {'SEG', 'MR'} TCGA-06-1084 6 77 {'SEG', 'MR'} TCGA-06-0139 5 76 {'SEG', 'MR'} TCGA-CV-7243 15 72 {'RTDOSE', 'CT', 'RTSTRUCT'} TCGA-CV-A6K0 13 71 {'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-A6JY 14 68 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-DD-A4NG 8 58 {'PT', 'CT', 'MR'} TCGA-06-0238 3 54 {'SEG', 'MR'} TCGA-06-1802 3 51 {'SEG', 'MR'} TCGA-D1-A16D 9 43 {'PT', 'CT', 'MR'} TCGA-06-0164 3 36 {'SEG', 'MR'} TCGA-06-0137 2 34 {'SEG', 'MR'} TCGA-14-1794 4 34 {'CT', 'SEG', 'MR'} TCGA-06-0130 2 33 {'SEG', 'MR'} TCGA-CS-6186 2 33 {'CT', 'SEG', 'MR'} TCGA-CV-A6JO 8 32 {'RTPLAN', 'CT', 'RTDOSE', 'PT', 'RTSTRUCT'} TCGA-06-0154 1 32 {'SEG', 'MR'} TCGA-02-0075 1 31 {'SEG', 'MR'} TCGA-06-0190 2 31 {'SEG', 'MR'} TCGA-02-0087 1 30 {'SEG', 'MR'} TCGA-CV-A6K1 7 28 {'RTDOSE', 'CT', 'RTSTRUCT', 'RTPLAN'} TCGA-G2-A3IE 6 28 {'CT', 'DX', 'MR'} TCGA-AO-A0JB 3 26 {'SR', 'MG', 'MR'} TCGA-02-0064 1 25 {'SEG', 'MR'} TCGA-02-0059 1 25 {'SEG', 'MR'} TCGA-02-0034 1 25 {'SEG', 'MR'} TCGA-06-0122 1 25 {'SEG', 'MR'} TCGA-06-0644 1 25 {'SEG', 'MR'} TCGA-06-0646 1 25 {'SEG', 'MR'} TCGA-06-5413 1 25 {'SEG', 'MR'} TCGA-06-0192 2 25 {'SEG', 'MR'} TCGA-06-0179 1 25 {'SEG', 'MR'} TCGA-08-0359 1 25 {'SEG', 'MR'} TCGA-06-5417 1 25 {'SEG', 'MR'} TCGA-02-0069 1 24 {'SEG', 'MR'} TCGA-06-0119 1 24 {'SEG', 'MR'} TCGA-CV-A6K2 5 24 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-08-0389 1 24 {'SEG', 'MR'} TCGA-06-0149 1 24 {'SEG', 'MR'} TCGA-CR-7368 4 24 {'PT', 'CT', 'MR'} TCGA-06-0158 2 24 {'SEG', 'MR'} TCGA-02-0086 1 24 {'SEG', 'MR'} TCGA-08-0355 1 24 {'SEG', 'MR'} TCGA-06-2570 1 23 {'SEG', 'MR'} TCGA-02-0027 1 22 {'SEG', 'MR'} TCGA-B9-A44B 2 22 {'PT', 'CT', 'MR'} TCGA-06-6389 1 22 {'SEG', 'MR'} TCGA-50-5072 4 22 {'PT', 'CT', 'NM'} TCGA-08-0385 2 22 {'SEG', 'MR'} TCGA-06-0142 1 22 {'SEG', 'MR'} TCGA-02-0068 1 22 {'SEG', 'MR'} TCGA-06-0240 1 22 {'SEG', 'MR'} TCGA-08-0360 1 21 {'SEG', 'MR'} TCGA-06-0182 1 21 {'SEG', 'MR'} TCGA-G2-A2EK 11 21 {'CR', 'CT', 'DX'} TCGA-06-0187 1 21 {'SEG', 'MR'} TCGA-08-0392 1 21 {'SEG', 'MR'} TCGA-06-5408 1 21 {'SEG', 'MR'} TCGA-08-0390 1 20 {'SEG', 'MR'} TCGA-02-0070 1 20 {'SEG', 'MR'} TCGA-02-0054 1 19 {'SEG', 'MR'} TCGA-08-0356 1 19 {'SEG', 'MR'} TCGA-G2-A2EL 5 19 {'CR', 'CT', 'DX'} TCGA-AO-A0JF 5 19 {'SR', 'MG', 'MR'} TCGA-06-0177 1 19 {'SEG', 'MR'} TCGA-02-0011 1 18 {'SEG', 'MR'} TCGA-02-0033 1 18 {'SEG', 'MR'} TCGA-02-0047 1 18 {'SEG', 'MR'} TCGA-02-0037 1 18 {'SEG', 'MR'} TCGA-06-0162 1 18 {'SEG', 'MR'} TCGA-02-0106 1 18 {'SEG', 'MR'} TCGA-06-0145 1 18 {'SEG', 'MR'} TCGA-06-0176 1 17 {'SEG', 'MR'} TCGA-G2-AA3D 5 17 {'CR', 'CT', 'DX'} TCGA-G2-A2EO 7 17 {'CR', 'CT', 'DX'} TCGA-AO-A0JI 2 17 {'SR', 'MG', 'MR'} TCGA-02-0116 1 17 {'SEG', 'MR'} TCGA-02-0009 1 16 {'SEG', 'MR'} TCGA-02-0006 1 16 {'SEG', 'MR'} TCGA-02-0046 1 16 {'SEG', 'MR'} TCGA-G7-A8LD 3 16 {'PT', 'CT', 'MR'} TCGA-02-0102 1 16 {'SEG', 'MR'} TCGA-02-0085 1 16 {'SEG', 'MR'} TCGA-CR-6478 2 14 {'PT', 'CT', 'MR'} TCGA-CV-5973 2 13 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-CV-5977 2 13 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-7235 2 10 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5976 2 10 {'RTDOSE', 'CT', 'RTSTRUCT', 'RTPLAN'} TCGA-CV-6433 2 9 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-08-0509 1 8 {'SEG', 'MR'} TCGA-CV-5966 2 8 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5978 2 8 {'RTPLAN', 'CT', 'RTDOSE', 'RTSTRUCT'} TCGA-CV-5970 2 7 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'} TCGA-CV-7236 2 7 {'RTDOSE', 'CT', 'RTSTRUCT'} TCGA-CV-7245 2 7 {'RTPLAN', 'CT', 'RTSTRUCT', 'RTDOSE'}"},{"location":"cli/AutoPipeline/","title":"AutoPipeline Usage","text":"<p>To use AutoPipeline, follow the installation instructions found at https://github.com/bhklab/med-imagetools#installing-med-imagetools.</p>"},{"location":"cli/AutoPipeline/#intro-to-autopipeline","title":"Intro to AutoPipeline","text":"<p>AutoPipeline will crawl and process any DICOM dataset. To run the most basic variation of the script, run the following command:</p> <pre><code>autopipeline INPUT_DIRECTORY OUTPUT_DIRECTORY --modalities MODALITY_LIST\n</code></pre> <p>Replace INPUT_DIRECTORY with the directory containing all your DICOM data, OUTPUT_DIRECTORY with the directory that you want the data to be outputted to.</p> <p>The <code>--modalities</code> option allows you to only process certain modalities that are present in the DICOM data. The available modalities are:</p> <ol> <li>CT</li> <li>MR</li> <li>RTSTRUCT</li> <li>PT    </li> <li>RTDOSE</li> </ol> <p>Set the modalities you want to use by separating each one with a comma. For example, to use CT and RTSTRUCT, run AutoPipeline with <code>--modalities CT,RTSTRUCT</code></p>"},{"location":"cli/AutoPipeline/#autopipeline-flags","title":"AutoPipeline Flags","text":"<p>AutoPipeline comes with many built-in features to make your data processing easier:</p> <ol> <li> <p>Spacing</p> <p>The spacing for the output image. default = (1., 1., 0.). 0. spacing means maintaining the image's spacing as-is. Spacing of (0., 0., 0.,) will not resample any image.</p> <pre><code>--spacing [Tuple: (int,int,int)]\n</code></pre> </li> <li> <p>Parallel Job Execution</p> <p>The number of jobs to be run in parallel. Set -1 to use all cores. default = -1</p> <pre><code>--n_jobs [int]\n</code></pre> </li> <li> <p>Dataset Graph Visualization (not recommended for large datasets)</p> <p>Whether to visualize the entire dataset using PyViz.</p> <pre><code>--visualize [flag]\n</code></pre> </li> <li> <p>Continue Pipeline Processing</p> <p>Whether to continue the most recent run of AutoPipeline that terminated prematurely for that output directory. Will only work if the <code>.imgtools</code> directory was not deleted from previous run. Using this flag will retain the same flags and parameters carried over from the previous run.</p> <pre><code>--continue_processing [flag]\n</code></pre> </li> <li> <p>Processing Dry Run</p> <p>Whether to execute a dry run, only generating the .imgtools folder, which includes the crawled index.</p> <pre><code>--dry_run [flag]\n</code></pre> </li> <li> <p>Show Progress</p> <p>Whether to print AutoPipeline progress to the standard output.</p> <pre><code>--show_progress [flag]\n</code></pre> </li> <li> <p>Warning on Subject Processing Errors</p> <p>Whether to warn instead of error when processing subjects</p> <pre><code>--warn_on_error [flag]\n</code></pre> </li> <li> <p>Overwrite Existing Output Files</p> <p>Whether to overwrite existing file outputs</p> <pre><code>--overwrite [flag]\n</code></pre> </li> <li> <p>Update existing crawled index</p> <p>Whether to update existing crawled index</p> <pre><code>--update [flag]\n</code></pre> </li> </ol>"},{"location":"cli/AutoPipeline/#flags-for-parsing-rtstruct-contoursregions-of-interest-roi","title":"Flags for parsing RTSTRUCT contours/regions of interest (ROI)","text":"<p>The contours can be selected by creating a YAML file to define a regular expression (regex), or list of potential contour names, or a combination of both. If none of the flags are set or the YAML file does not exist, the AutoPipeline will default to processing every contour.</p> <ol> <li> <p>Defining YAML file path for contours</p> <p>Whether to read a YAML file that defines regex or string options for contour names for regions of interest (ROI). By default, it will look for and read from <code>INPUT_DIRECTORY/roi_names.yaml</code></p> <pre><code>--read_yaml_label_names [flag]\n</code></pre> <p>Path to the above-mentioned YAML file. Path can be absolute or relative. default = \"\" (each ROI will have its own label index in dataset.json for nnUNet)</p> <pre><code>--roi_yaml_path [str]\n</code></pre> </li> <li> <p>Defining contour selection behaviour</p> <p>A typical ROI YAML file may look like this: <pre><code>GTV: GTV*\nLUNG:\n    - LUNG*\n    - LNUG\n    - POUMON*\nNODES:\n    - IL1\n    - IIL2\n    - IIIL3\n    - IVL4\n</code></pre></p> <p>By default, all ROIs that match any of the regex or strings will be saved as one label. For example, GTVn, GTVp, GTVfoo will be saved as GTV. However, this is not always the desirable behaviour. </p> <p>Only select the first matching regex/string</p> <p>The StructureSet iterates through the regex and string in the order it is written in the YAML. When this flag is set, once any contour matches the regex or string, the ROI search is interrupted and moves to the next ROI. This may be useful if you have a priority order of potentially matching contour names. </p> <pre><code>--roi_select_first [flag]\n</code></pre> <p>If a patient has  contours <code>[GTVp, LNUG, IL1, IVL4]</code>, with the above YAML file and <code>--roi_select_first</code> flag set, it will only process <code>[GTVp, LNUG, IL1]</code> contours as <code>[GTV, LUNG, NODES]</code>, respectively. </p> <p>Process each matching contour as a separate ROI</p> <p>Any matching contour will be saved separate with its contour name as a suffix to the ROI name. This will not apply to ROIs that only have one regex/string.</p> <p><pre><code>--roi_separate [flag]\n</code></pre> If a patient had contours <code>[GTVp, LNUG, IL1, IVL4]</code>, with the above YAML file and <code>--roi_sepearate</code> flag set, it will process the contours as <code>[GTV, LUNG_LNUG, NODES_IL1, NODES_IVL4]</code>, respectively. </p> </li> <li> <p>Ignore patients with no contours</p> <p>Ignore patients with no contours that match any of the defined regex or strings instead of throwing error. </p> <pre><code>--ignore_missing_regex [flag]\n</code></pre> </li> </ol>"},{"location":"cli/AutoPipeline/#additional-nnunet-specific-flags","title":"Additional nnUNet-specific flags","text":"<ol> <li> <p>Format Output for nnUNet Training</p> <p>Whether to format output for nnUNet training. Modalities must be CT,RTSTRUCT or MR,RTSTRUCT. <code>--modalities CT,RTSTRUCT</code> or <code>--modalities MR,RTSTRUCT</code></p> <pre><code>--nnunet [flag]\n</code></pre> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 imagesTr\n\u2502           \u251c\u2500\u2500 imagesTs\n\u2502           \u251c\u2500\u2500 labelsTr\n\u2502           \u2514\u2500\u2500 labelsTs\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> </li> <li> <p>Training Size</p> <p>Training size of the train-test-split. default = 1.0 (all data will be in imagesTr/labelsTr)</p> <pre><code>--train_size [float]\n</code></pre> </li> <li> <p>Random State</p> <p>Random state for the train-test-split. Uses sklearn's train_test_split(). default = 42</p> <pre><code>--random_state [int]\n</code></pre> </li> <li> <p>Custom Train-Test-Split YAML</p> <p>Whether to use a custom train-test-split. Must be in a file found at <code>INPUT_DIRECTORY/custom_train_test_split.yaml</code>. All subjects not defined in this file will be randomly split to fill the defined value for <code>--train_size</code> (default = 1.0). File must conform to:</p> <pre><code>train:\n    - subject_1\n    - subject_2\n    ...\ntest:\n    - subject_1\n    - subject_2\n    ...\n</code></pre> <pre><code>--custom_train_test_split [flag]\n</code></pre> </li> </ol>"},{"location":"cli/AutoPipeline/#additional-flags-for-nnunet-inference","title":"Additional flags for nnUNet Inference","text":"<ol> <li> <p>Format Output for nnUNet Inference</p> <p>Whether to format output for nnUNet Inference.</p> <pre><code>--nnunet_inference [flag]\n</code></pre> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> </li> <li> <p>Path to <code>dataset.json</code></p> <p>The path to the <code>dataset.json</code> file for nnUNet inference.</p> <pre><code>--dataset_json_path [str]\n</code></pre> <p>A dataset json file may look like this: <pre><code>{\n    \"modality\":{\n        \"0\": \"CT\"\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"cli/imgtools/","title":"<code>imgtools</code> CLI","text":""},{"location":"cli/imgtools/#imgtools","title":"imgtools","text":"<p>A collection of tools for working with medical imaging data.</p> <p>Usage:</p> <pre><code>imgtools [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--quiet</code>, <code>-q</code> boolean Suppress all logging except errors, overrides verbosity options. <code>False</code> <code>--verbose</code>, <code>-v</code> integer range (<code>0</code> and above) Increase verbosity of logging, overrides environment variable. (0-3: ERROR, WARNING, INFO, DEBUG). <code>0</code> <code>--version</code> boolean Show the version and exit. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-dicomsort","title":"imgtools dicomsort","text":"<p>Sorts DICOM files into directories based on their tags.</p> <p>Usage:</p> <pre><code>imgtools dicomsort [OPTIONS] SOURCE_DIRECTORY TARGET_DIRECTORY\n</code></pre> <p>Options:</p> Name Type Description Default <code>--action</code>, <code>-a</code> choice (<code>move</code> | <code>copy</code> | <code>symlink</code> | <code>hardlink</code>) Action to perform on the files. _required <code>-n</code>, <code>--dry-run</code> boolean Do not move or copy files, just print what would be done. Always recommended to use this first to confirm the operation! <code>False</code> <code>-j</code>, <code>--num-workers</code> integer Number of worker processes to use for sorting. <code>1</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/imgtools/#imgtools-find-dicoms","title":"imgtools find-dicoms","text":"<p>A tool to find DICOM files.</p> <p>PATH is the directory to search for DICOM files.</p> <p>SEARCH_INPUT is an optional list of regex patterns to filter the search results.</p> <p>Usage:</p> <pre><code>imgtools find-dicoms [OPTIONS] PATH [SEARCH_INPUT]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>-e</code>, <code>--extension</code> text File extension to look for. <code>dcm</code> <code>-c</code>, <code>--count</code> boolean Whether to just print the count of files found. This is useful for scripts. <code>False</code> <code>-l</code>, <code>--limit</code> integer The limit of results to return. None <code>-ch</code>, <code>--check-header</code> boolean Whether to check DICOM header for \"DICM\" signature. <code>False</code> <code>-s</code>, <code>--sorted</code> boolean Sort the results alphabetically. <code>False</code> <code>-h</code>, <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"cli/nnUNet/","title":"Preparing Data for nnUNet","text":"<p>nnUNet repo can be found at: https://github.com/MIC-DKFZ/nnUNet</p>"},{"location":"cli/nnUNet/#processing-dicom-data-with-med-imagetools","title":"Processing DICOM Data with Med-ImageTools","text":"<p>Ensure that you have followed the steps in https://github.com/bhklab/med-imagetools#installing-med-imagetools before proceeding.</p> <p>To convert your data from DICOM to NIfTI for training an nnUNet auto-segmentation model, run the following command:</p> <pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT,RTSTRUCT \\\n  --nnunet\n</code></pre> <p>Modalities can also be set to <code>--modalities MR,RTSTRUCT</code></p> <p>AutoPipeline offers many more options and features for you to customize your outputs: &lt;https://github.com/bhklab/med-imagetools/tree/main/README.md </p> <p>.</p>"},{"location":"cli/nnUNet/#nnunet-preprocess-and-train","title":"nnUNet Preprocess and Train","text":""},{"location":"cli/nnUNet/#one-step-preprocess-and-train","title":"One-Step Preprocess and Train","text":"<p>Med-ImageTools generates a file in your output folder called <code>nnunet_preprocess_and_train.sh</code> that combines all the commands needed for preprocessing and training your nnUNet model. Run that shell script to get a fully trained nnUNet model.</p> <p>Alternatively, you can go through each step individually as follows below:</p>"},{"location":"cli/nnUNet/#nnunet-preprocessing","title":"nnUNet Preprocessing","text":"<p>Follow the instructions for setting up your paths for nnUNet: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md</p> <p>Med-ImageTools generates the dataset.json that nnUNet requires in the output directory that you specify.</p> <p>The generated output directory structure will look something like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 nnunet_preprocess_and_train.sh\n\u2502           \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> <p>nnUNet requires that environment variables be set before any commands are executed. To temporarily set them, run the following:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>To permanently set these environment variables, make sure that in your <code>~/.bashrc</code> file, these environment variables are set for nnUNet. The <code>nnUNet_preprocessed</code> and <code>nnUNet_trained_models</code> folders are generated as empty folders for you by Med-ImageTools. <code>nnUNet_raw_data_base</code> is populated with the required raw data files. Add this to the file:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>Then, execute the command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Too allow nnUNet to preprocess your data for training, run the following command. Set XXX to the ID that you want to preprocess. This is your task ID. For example, for Task500_HNSCC, the task ID is 500. Task IDs must be between 500 and 999, so Med-ImageTools can run 500 instances with the <code>--nnunet</code> flag in a single output folder.</p> <pre><code>nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity\n</code></pre>"},{"location":"cli/nnUNet/#nnunet-training","title":"nnUNet Training","text":"<p>Once nnUNet has finished preprocessing, you may begin training your nnUNet model. To train your model, run the following command. Learn more about nnUNet's options here: https://github.com/MIC-DKFZ/nnUNet#model-training</p> <pre><code>nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD\n</code></pre>"},{"location":"cli/nnUNet/#nnunet-inference","title":"nnUNet Inference","text":"<p>For inference data, nnUNet requires data to be in a different output format. To run AutoPipeline for nnUNet inference, run the following command:</p> <p><pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT \\\n  --nnunet_inference \\\n  --dataset_json_path [DATASET_JSON_PATH]\n</code></pre> To execute this command AutoPipeline needs a json file with the image modality definitions.</p> <p>Modalities can also be set to <code>--modalities MR</code>.</p> <p>The directory structue will look like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> <p>To run inference, run the command:</p> <pre><code>nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION\n</code></pre> <p>In this case, the <code>INPUT_FOLDER</code> of nnUNet is the <code>OUTPUT_DIRECTORY</code> of Med-ImageTools.# Preparing Data for nnUNet</p> <p>nnUNet repo can be found at: https://github.com/MIC-DKFZ/nnUNet</p>"},{"location":"cli/nnUNet/#processing-dicom-data-with-med-imagetools_1","title":"Processing DICOM Data with Med-ImageTools","text":"<p>Ensure that you have followed the steps in https://github.com/bhklab/med-imagetools#installing-med-imagetools before proceeding.</p> <p>To convert your data from DICOM to NIfTI for training an nnUNet auto-segmentation model, run the following command:</p> <pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT,RTSTRUCT \\\n  --nnunet\n</code></pre> <p>Modalities can also be set to <code>--modalities MR,RTSTRUCT</code></p> <p>AutoPipeline offers many more options and features for you to customize your outputs: https://github.com/bhklab/med-imagetools/imgtools/README.md.</p>"},{"location":"cli/nnUNet/#nnunet-preprocess-and-train_1","title":"nnUNet Preprocess and Train","text":""},{"location":"cli/nnUNet/#one-step-preprocess-and-train_1","title":"One-Step Preprocess and Train","text":"<p>Med-ImageTools generates a file in your output folder called <code>nnunet_preprocess_and_train.sh</code> that combines all the commands needed for preprocessing and training your nnUNet model. Run that shell script to get a fully trained nnUNet model.</p> <p>Alternatively, you can go through each step individually as follows below:</p>"},{"location":"cli/nnUNet/#nnunet-preprocessing_1","title":"nnUNet Preprocessing","text":"<p>Follow the instructions for setting up your paths for nnUNet: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md</p> <p>Med-ImageTools generates the dataset.json that nnUNet requires in the output directory that you specify.</p> <p>The generated output directory structure will look something like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 nnUNet_preprocessed\n\u251c\u2500\u2500 nnUNet_raw_data_base\n\u2502   \u2514\u2500\u2500 nnUNet_raw_data\n\u2502       \u2514\u2500\u2500 Task500_HNSCC\n\u2502           \u251c\u2500\u2500 nnunet_preprocess_and_train.sh\n\u2502           \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 nnUNet_trained_models\n</code></pre> <p>nnUNet requires that environment variables be set before any commands are executed. To temporarily set them, run the following:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>To permanently set these environment variables, make sure that in your <code>~/.bashrc</code> file, these environment variables are set for nnUNet. The <code>nnUNet_preprocessed</code> and <code>nnUNet_trained_models</code> folders are generated as empty folders for you by Med-ImageTools. <code>nnUNet_raw_data_base</code> is populated with the required raw data files. Add this to the file:</p> <pre><code>export nnUNet_raw_data_base=\"/OUTPUT_DIRECTORY/nnUNet_raw_data_base\"\nexport nnUNet_preprocessed=\"/OUTPUT_DIRECTORY/nnUNet_preprocessed\"\nexport RESULTS_FOLDER=\"/OUTPUT_DIRECTORY/nnUNet_trained_models\"\n</code></pre> <p>Then, execute the command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Too allow nnUNet to preprocess your data for training, run the following command. Set XXX to the ID that you want to preprocess. This is your task ID. For example, for Task500_HNSCC, the task ID is 500. Task IDs must be between 500 and 999, so Med-ImageTools can run 500 instances with the <code>--nnunet</code> flag in a single output folder.</p> <pre><code>nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity\n</code></pre>"},{"location":"cli/nnUNet/#nnunet-training_1","title":"nnUNet Training","text":"<p>Once nnUNet has finished preprocessing, you may begin training your nnUNet model. To train your model, run the following command. Learn more about nnUNet's options here: https://github.com/MIC-DKFZ/nnUNet#model-training</p> <pre><code>nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD\n</code></pre>"},{"location":"cli/nnUNet/#nnunet-inference_1","title":"nnUNet Inference","text":"<p>For inference data, nnUNet requires data to be in a different output format. To run AutoPipeline for nnUNet inference, run the following command:</p> <p><pre><code>autopipeline\\\n  [INPUT_DIRECTORY] \\\n  [OUTPUT_DIRECTORY] \\\n  --modalities CT \\\n  --nnunet_inference \\\n  --dataset_json_path [DATASET_JSON_PATH]\n</code></pre> To execute this command AutoPipeline needs a json file with the image modality definitions.</p> <p>Modalities can also be set to <code>--modalities MR</code>.</p> <p>The directory structue will look like:</p> <pre><code>OUTPUT_DIRECTORY\n\u251c\u2500\u2500 0_subject1_0000.nii.gz\n\u2514\u2500\u2500 ...\n</code></pre> <p>To run inference, run the command:</p> <pre><code>nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION\n</code></pre> <p>In this case, the <code>INPUT_FOLDER</code> of nnUNet is the <code>OUTPUT_DIRECTORY</code> of Med-ImageTools.</p>"},{"location":"reference/dicom-utils/find-dicoms/","title":"Find DICOMs","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms","title":"imgtools.dicom.find_dicoms","text":"<pre><code>find_dicoms(\n    directory: pathlib.Path,\n    recursive: bool,\n    check_header: bool,\n    extension: typing.Optional[str] = None,\n    limit: typing.Optional[int] = None,\n    search_input: typing.Optional[typing.List[str]] = None,\n) -&gt; typing.List[pathlib.Path]\n</code></pre> <p>Locate DICOM files in a specified directory.</p> <p>This function scans a directory for files matching the specified extension and validates them as DICOM files based on the provided options. It supports recursive search and optional header validation to confirm file validity.</p> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The directory in which to search for DICOM files.</p> required <code>bool</code> <p>Whether to include subdirectories in the search</p> required <code>bool</code> <p>Whether to validate files by checking for a valid DICOM header.     - If <code>True</code>, perform DICOM header validation (slower but more accurate).     - If <code>False</code>, skip header validation and rely on extension.</p> required <code>str</code> <p>File extension to search for (e.g., \"dcm\"). If <code>None</code>, consider all files regardless of extension.</p> <code>None</code> <code>int</code> <p>Maximum number of DICOM files to return. If <code>None</code>, return all found files.</p> <code>None</code> <p>Returns:</p> Type Description <code>typing.List[pathlib.Path]</code> <p>A list of valid DICOM file paths found in the directory.</p> Notes <ul> <li>If <code>check_header</code> is enabled, the function checks each file for a valid     DICOM header, which may slow down the search process.</li> </ul> <p>Examples:</p> <p>Setup</p> <pre><code>&gt;&gt;&gt; from pathlib import (\n...     Path,\n... )\n&gt;&gt;&gt; from imgtools.dicom.utils import (\n...     find_dicoms,\n... )\n</code></pre> <p>Find DICOM files recursively without header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), PosixPath('/data/subdir/scan3.dcm')]\n</code></pre> <p>Suppose that <code>scan3.dcm</code> is not a valid DICOM file. Find DICOM files with header validation:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=True,\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files without recursion:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=False,\n...     check_header=False,\n... )\n[PosixPath('/data/scan1.dcm')]\n</code></pre> <p>Find DICOM files with a specific extension:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     extension=\"dcm\",\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files with a search input:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     search_input=[\n...         \"scan1\",\n...         \"scan2\",\n...     ],\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> <p>Find DICOM files with a limit:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=False,\n...     limit=1,\n... )\n[PosixPath('/data/scan1.dcm')]\n</code></pre> <p>Find DICOM files with all options:</p> <pre><code>&gt;&gt;&gt; find_dicoms(\n...     Path(\"/data\"),\n...     recursive=True,\n...     check_header=True,\n...     extension=\"dcm\",\n...     limit=2,\n...     search_input=[\"scan\"],\n... )\n[PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>def find_dicoms(\n    directory: Path,\n    recursive: bool,\n    check_header: bool,\n    extension: Optional[str] = None,\n    limit: Optional[int] = None,\n    search_input: Optional[List[str]] = None,\n) -&gt; List[Path]:\n    \"\"\"Locate DICOM files in a specified directory.\n\n    This function scans a directory for files matching the specified extension\n    and validates them as DICOM files based on the provided options. It supports\n    recursive search and optional header validation to confirm file validity.\n\n    Parameters\n    ----------\n    directory : Path\n        The directory in which to search for DICOM files.\n    recursive : bool\n        Whether to include subdirectories in the search\n    check_header : bool\n        Whether to validate files by checking for a valid DICOM header.\n            - If `True`, perform DICOM header validation (slower but more accurate).\n            - If `False`, skip header validation and rely on extension.\n\n    extension : str, optional\n        File extension to search for (e.g., \"dcm\"). If `None`, consider all files\n        regardless of extension.\n\n    limit : int, optional\n        Maximum number of DICOM files to return. If `None`, return all found files.\n\n    Returns\n    -------\n    List[Path]\n        A list of valid DICOM file paths found in the directory.\n\n    Notes\n    -----\n    - If `check_header` is enabled, the function checks each file for a valid\n        DICOM header, which may slow down the search process.\n\n    Examples\n    --------\n    Setup\n\n    &gt;&gt;&gt; from pathlib import (\n    ...     Path,\n    ... )\n    &gt;&gt;&gt; from imgtools.dicom.utils import (\n    ...     find_dicoms,\n    ... )\n\n    Find DICOM files recursively without header validation:\n\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm'), PosixPath('/data/subdir/scan3.dcm')]\n\n    Suppose that `scan3.dcm` is not a valid DICOM file. Find DICOM files with header validation:\n\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=True,\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files without recursion:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=False,\n    ...     check_header=False,\n    ... )\n    [PosixPath('/data/scan1.dcm')]\n\n    Find DICOM files with a specific extension:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     extension=\"dcm\",\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files with a search input:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     search_input=[\n    ...         \"scan1\",\n    ...         \"scan2\",\n    ...     ],\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n\n    Find DICOM files with a limit:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=False,\n    ...     limit=1,\n    ... )\n    [PosixPath('/data/scan1.dcm')]\n\n    Find DICOM files with all options:\n    &gt;&gt;&gt; find_dicoms(\n    ...     Path(\"/data\"),\n    ...     recursive=True,\n    ...     check_header=True,\n    ...     extension=\"dcm\",\n    ...     limit=2,\n    ...     search_input=[\"scan\"],\n    ... )\n    [PosixPath('/data/scan1.dcm'), PosixPath('/data/subdir/scan2.dcm')]\n    \"\"\"\n    pattern = f\"*.{extension}\" if extension else \"*\"\n\n    glob_method = directory.rglob if recursive else directory.glob\n\n    logger.debug(\n        \"Looking for DICOM files\",\n        directory=directory,\n        recursive=recursive,\n        search_pattern=pattern,\n        check_header=check_header,\n        limit=limit,\n        search_input=search_input,\n    )\n\n    files = (\n        file.absolute()\n        for file in glob_method(pattern)\n        if (\n            not search_input\n            or all(term in str(file.as_posix()) for term in search_input)\n        )\n        and _is_valid_dicom(file, check_header)\n    )\n\n    return list(islice(files, limit)) if limit else list(files)\n</code></pre>"},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms(directory)","title":"<code>directory</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms(recursive)","title":"<code>recursive</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms(check_header)","title":"<code>check_header</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms(extension)","title":"<code>extension</code>","text":""},{"location":"reference/dicom-utils/find-dicoms/#imgtools.dicom.find_dicoms(limit)","title":"<code>limit</code>","text":""},{"location":"reference/dicom-utils/load-dicom/","title":"Load DICOM","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_dicom","title":"imgtools.dicom.load_dicom","text":"<pre><code>load_dicom(\n    dicom_input: imgtools.dicom.input.dicom_reader.DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Load a DICOM file and return the parsed FileDataset object.</p> <p>This function supports various input types including file paths, byte streams, and file-like objects. It uses the <code>pydicom.dcmread</code> function to read the DICOM file.</p> Notes <ul> <li>If <code>dicom_input</code> is already a <code>FileDataset</code>, it is returned as is.</li> <li>If <code>dicom_input</code> is a file path or file-like object, it is read using <code>pydicom.dcmread</code>.</li> <li>If <code>dicom_input</code> is a byte stream, it is wrapped in a <code>BytesIO</code> object and then read.</li> <li>An <code>InvalidDicomError</code> is raised if the input type is unsupported.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.FileDataset | str | pathlib.Path | bytes | typing.BinaryIO</code> <p>Input DICOM file as a <code>pydicom.FileDataset</code>, file path, byte stream, or file-like object.</p> required <code>bool</code> <p>Whether to allow reading DICOM files missing the File Meta Information header, by default True.</p> <code>True</code> <code>bool</code> <p>Whether to stop reading the DICOM file before loading pixel data, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>pydicom.dataset.FileDataset</code> <p>Parsed DICOM dataset.</p> <p>Raises:</p> Type Description <code>imgtools.exceptions.InvalidDicomError</code> <p>If the input is of an unsupported type or cannot be read as a DICOM file.</p> Source code in <code>src/imgtools/dicom/input/dicom_reader.py</code> <pre><code>def load_dicom(\n    dicom_input: DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; FileDataset:\n    \"\"\"Load a DICOM file and return the parsed FileDataset object.\n\n    This function supports various input types including file paths, byte streams,\n    and file-like objects. It uses the `pydicom.dcmread` function to read the DICOM file.\n\n    Notes\n    -----\n    - If `dicom_input` is already a `FileDataset`, it is returned as is.\n    - If `dicom_input` is a file path or file-like object, it is read using `pydicom.dcmread`.\n    - If `dicom_input` is a byte stream, it is wrapped in a `BytesIO` object and then read.\n    - An `InvalidDicomError` is raised if the input type is unsupported.\n\n    Parameters\n    ----------\n    dicom_input : FileDataset | str | Path | bytes | BinaryIO\n        Input DICOM file as a `pydicom.FileDataset`, file path, byte stream, or file-like object.\n    force : bool, optional\n        Whether to allow reading DICOM files missing the *File Meta Information*\n        header, by default True.\n    stop_before_pixels : bool, optional\n        Whether to stop reading the DICOM file before loading pixel data, by default True.\n\n    Returns\n    -------\n    FileDataset\n        Parsed DICOM dataset.\n\n    Raises\n    ------\n    InvalidDicomError\n        If the input is of an unsupported type or cannot be read as a DICOM file.\n    \"\"\"\n    match dicom_input:\n        case FileDataset():\n            return dicom_input\n        case str() | Path() | BinaryIO():\n            dicom_source = path_from_pathlike(dicom_input)\n            return dcmread(\n                dicom_source,\n                force=force,\n                stop_before_pixels=stop_before_pixels,\n            )\n        case bytes():\n            return dcmread(\n                BytesIO(dicom_input),\n                force=force,\n                stop_before_pixels=stop_before_pixels,\n            )\n        case _:\n            msg = (\n                f\"Invalid input type for 'dicom_input': {type(dicom_input)}. \"\n                \"Must be a FileDataset, str, Path, bytes, or BinaryIO object.\"\n            )\n            raise InvalidDicomError(msg)\n</code></pre>"},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_dicom(dicom_input)","title":"<code>dicom_input</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_dicom(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_dicom(stop_before_pixels)","title":"<code>stop_before_pixels</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_rtstruct_dcm","title":"imgtools.dicom.load_rtstruct_dcm","text":"<pre><code>load_rtstruct_dcm(\n    rtstruct_input: imgtools.dicom.input.dicom_reader.DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Load an RTSTRUCT DICOM file and return the parsed FileDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.FileDataset | str | pathlib.Path | bytes</code> <p>Input DICOM file as a <code>pydicom.FileDataset</code>, file path, or byte stream.</p> required <code>bool</code> <p>Whether to allow reading DICOM files missing the File Meta Information header, by default True.</p> <code>True</code> <code>bool</code> <p>Whether to stop reading the DICOM file before loading pixel data, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>pydicom.dataset.FileDataset</code> <p>Parsed RTSTRUCT DICOM dataset.</p> <p>Raises:</p> Type Description <code>imgtools.exceptions.InvalidDicomError</code> <p>If the input is of an unsupported type or cannot be read as a DICOM file.</p> <code>imgtools.exceptions.NotRTSTRUCTError</code> <p>If the input file is not an RTSTRUCT (i.e., <code>Modality</code> field is not \"RTSTRUCT\").</p> Source code in <code>src/imgtools/dicom/input/dicom_reader.py</code> <pre><code>def load_rtstruct_dcm(\n    rtstruct_input: DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; FileDataset:\n    \"\"\"Load an RTSTRUCT DICOM file and return the parsed FileDataset object.\n\n    Parameters\n    ----------\n    rtstruct_input : FileDataset | str | Path | bytes\n        Input DICOM file as a `pydicom.FileDataset`, file path, or byte stream.\n    force : bool, optional\n        Whether to allow reading DICOM files missing the *File Meta Information*\n        header, by default True.\n    stop_before_pixels : bool, optional\n        Whether to stop reading the DICOM file before loading pixel data, by default True.\n\n    Returns\n    -------\n    FileDataset\n        Parsed RTSTRUCT DICOM dataset.\n\n    Raises\n    ------\n    InvalidDicomError\n        If the input is of an unsupported type or cannot be read as a DICOM file.\n    NotRTSTRUCTError\n        If the input file is not an RTSTRUCT (i.e., `Modality` field is not \"RTSTRUCT\").\n    \"\"\"\n\n    dicom = load_dicom(rtstruct_input, force, stop_before_pixels)\n\n    if dicom.Modality != \"RTSTRUCT\":\n        msg = f\"The provided DICOM is not an RTSTRUCT file. Found Modality: {dicom.Modality}\"\n        raise NotRTSTRUCTError(msg)\n\n    return dicom\n</code></pre>"},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_rtstruct_dcm(rtstruct_input)","title":"<code>rtstruct_input</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_rtstruct_dcm(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_rtstruct_dcm(stop_before_pixels)","title":"<code>stop_before_pixels</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_seg_dcm","title":"imgtools.dicom.load_seg_dcm","text":"<pre><code>load_seg_dcm(\n    seg_input: imgtools.dicom.input.dicom_reader.DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; pydicom.dataset.FileDataset\n</code></pre> <p>Load a SEG DICOM file and return the parsed FileDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>pydicom.dataset.FileDataset | str | pathlib.Path | bytes</code> <p>Input DICOM file as a <code>pydicom.FileDataset</code>, file path, or byte stream.</p> required <code>bool</code> <p>Whether to allow reading DICOM files missing the File Meta Information header, by default True.</p> <code>True</code> <code>bool</code> <p>Whether to stop reading the DICOM file before loading pixel data, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>pydicom.dataset.FileDataset</code> <p>Parsed SEG DICOM dataset.</p> <p>Raises:</p> Type Description <code>imgtools.exceptions.InvalidDicomError</code> <p>If the input is of an unsupported type or cannot be read as a DICOM file.</p> <code>imgtools.exceptions.NotSEGError</code> <p>If the input file is not a SEG (i.e., <code>Modality</code> field is not \"SEG\").</p> Source code in <code>src/imgtools/dicom/input/dicom_reader.py</code> <pre><code>def load_seg_dcm(\n    seg_input: DicomInput,\n    force: bool = True,\n    stop_before_pixels: bool = True,\n) -&gt; FileDataset:\n    \"\"\"Load a SEG DICOM file and return the parsed FileDataset object.\n\n    Parameters\n    ----------\n    seg_input : FileDataset | str | Path | bytes\n        Input DICOM file as a `pydicom.FileDataset`, file path, or byte stream.\n    force : bool, optional\n        Whether to allow reading DICOM files missing the *File Meta Information*\n        header, by default True.\n    stop_before_pixels : bool, optional\n        Whether to stop reading the DICOM file before loading pixel data, by default True.\n\n    Returns\n    -------\n    FileDataset\n        Parsed SEG DICOM dataset.\n\n    Raises\n    ------\n    InvalidDicomError\n        If the input is of an unsupported type or cannot be read as a DICOM file.\n    NotSEGError\n        If the input file is not a SEG (i.e., `Modality` field is not \"SEG\").\n    \"\"\"\n    dicom = load_dicom(seg_input, force, stop_before_pixels)\n\n    if dicom.Modality != \"SEG\":\n        msg = f\"The provided DICOM is not a SEG file. Found Modality: {dicom.Modality}\"\n        raise NotSEGError(msg)\n\n    return dicom\n</code></pre>"},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_seg_dcm(seg_input)","title":"<code>seg_input</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_seg_dcm(force)","title":"<code>force</code>","text":""},{"location":"reference/dicom-utils/load-dicom/#imgtools.dicom.load_seg_dcm(stop_before_pixels)","title":"<code>stop_before_pixels</code>","text":""},{"location":"reference/dicom-utils/lookup_tag/","title":"Lookup Tag","text":""},{"location":"reference/dicom-utils/lookup_tag/#imgtools.dicom.lookup_tag","title":"imgtools.dicom.lookup_tag  <code>cached</code>","text":"<pre><code>lookup_tag(\n    keyword: str, hex_format: bool = False\n) -&gt; typing.Optional[str]\n</code></pre> <p>Lookup the tag for a given DICOM keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to look up.</p> required <code>bool</code> <p>If True, return the tag in hexadecimal format (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str or None</code> <p>The DICOM tag as a string, or None if the keyword is invalid.</p> <p>Examples:</p> <p>Lookup a DICOM tag in decimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\"PatientID\")\n'1048608'\n</code></pre> <p>Lookup a DICOM tag in hexadecimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\n...     \"PatientID\",\n...     hex_format=True,\n... )\n'0x100020'\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef lookup_tag(keyword: str, hex_format: bool = False) -&gt; Optional[str]:\n    \"\"\"\n    Lookup the tag for a given DICOM keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to look up.\n    hex_format : bool, optional\n        If True, return the tag in hexadecimal format (default is False).\n\n    Returns\n    -------\n    str or None\n        The DICOM tag as a string, or None if the keyword is invalid.\n\n    Examples\n    --------\n\n    Lookup a DICOM tag in decimal format:\n\n    &gt;&gt;&gt; lookup_tag(\"PatientID\")\n    '1048608'\n\n    Lookup a DICOM tag in hexadecimal format:\n\n    &gt;&gt;&gt; lookup_tag(\n    ...     \"PatientID\",\n    ...     hex_format=True,\n    ... )\n    '0x100020'\n    \"\"\"\n    if (tag := tag_for_keyword(keyword)) is None:\n        return None\n    return f\"0x{tag:X}\" if hex_format else str(tag)\n</code></pre>"},{"location":"reference/dicom-utils/lookup_tag/#imgtools.dicom.lookup_tag(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/lookup_tag/#imgtools.dicom.lookup_tag(hex_format)","title":"<code>hex_format</code>","text":""},{"location":"reference/dicom-utils/similar_tags/","title":"Similar Tags","text":""},{"location":"reference/dicom-utils/similar_tags/#imgtools.dicom.similar_tags","title":"imgtools.dicom.similar_tags  <code>cached</code>","text":"<pre><code>similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; typing.List[str]\n</code></pre> <p>Find similar DICOM tags for a given keyword.</p> <p>Useful for User Interface to suggest similar tags based on a misspelled keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The keyword to search for similar tags.</p> required <code>int</code> <p>Maximum number of similar tags to return (default is 3).</p> <code>3</code> <code>float</code> <p>Minimum similarity ratio (default is 0.6).</p> <code>0.6</code> <p>Returns:</p> Type Description <code>typing.List[str]</code> <p>A list of up to <code>n</code> similar DICOM tags.</p> <p>Examples:</p> <p>Find similar tags for a misspelled keyword:</p> <pre><code>&gt;&gt;&gt; similar_tags(\"PatinetID\")\n['PatientID', 'PatientName', 'PatientBirthDate']\n</code></pre> <p>Adjust the number of results and threshold:</p> <pre><code>&gt;&gt;&gt; similar_tags(\n...     \"PatinetID\",\n...     n=5,\n...     threshold=0.7,\n... )\n['PatientID', 'PatientName']\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; List[str]:\n    \"\"\"Find similar DICOM tags for a given keyword.\n\n    Useful for User Interface to suggest similar tags based on a misspelled keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The keyword to search for similar tags.\n    n : int, optional\n        Maximum number of similar tags to return (default is 3).\n    threshold : float, optional\n        Minimum similarity ratio (default is 0.6).\n\n    Returns\n    -------\n    List[str]\n        A list of up to `n` similar DICOM tags.\n\n    Examples\n    --------\n    Find similar tags for a misspelled keyword:\n\n    &gt;&gt;&gt; similar_tags(\"PatinetID\")\n    ['PatientID', 'PatientName', 'PatientBirthDate']\n\n    Adjust the number of results and threshold:\n\n    &gt;&gt;&gt; similar_tags(\n    ...     \"PatinetID\",\n    ...     n=5,\n    ...     threshold=0.7,\n    ... )\n    ['PatientID', 'PatientName']\n    \"\"\"\n    return difflib.get_close_matches(keyword, ALL_DICOM_TAGS, n, threshold)\n</code></pre>"},{"location":"reference/dicom-utils/similar_tags/#imgtools.dicom.similar_tags(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/similar_tags/#imgtools.dicom.similar_tags(n)","title":"<code>n</code>","text":""},{"location":"reference/dicom-utils/similar_tags/#imgtools.dicom.similar_tags(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/","title":"Tag Helpers","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag","title":"lookup_tag  <code>cached</code>","text":"<pre><code>lookup_tag(\n    keyword: str, hex_format: bool = False\n) -&gt; typing.Optional[str]\n</code></pre> <p>Lookup the tag for a given DICOM keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to look up.</p> required <code>bool</code> <p>If True, return the tag in hexadecimal format (default is False).</p> <code>False</code> <p>Returns:</p> Type Description <code>str or None</code> <p>The DICOM tag as a string, or None if the keyword is invalid.</p> <p>Examples:</p> <p>Lookup a DICOM tag in decimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\"PatientID\")\n'1048608'\n</code></pre> <p>Lookup a DICOM tag in hexadecimal format:</p> <pre><code>&gt;&gt;&gt; lookup_tag(\n...     \"PatientID\",\n...     hex_format=True,\n... )\n'0x100020'\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef lookup_tag(keyword: str, hex_format: bool = False) -&gt; Optional[str]:\n    \"\"\"\n    Lookup the tag for a given DICOM keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to look up.\n    hex_format : bool, optional\n        If True, return the tag in hexadecimal format (default is False).\n\n    Returns\n    -------\n    str or None\n        The DICOM tag as a string, or None if the keyword is invalid.\n\n    Examples\n    --------\n\n    Lookup a DICOM tag in decimal format:\n\n    &gt;&gt;&gt; lookup_tag(\"PatientID\")\n    '1048608'\n\n    Lookup a DICOM tag in hexadecimal format:\n\n    &gt;&gt;&gt; lookup_tag(\n    ...     \"PatientID\",\n    ...     hex_format=True,\n    ... )\n    '0x100020'\n    \"\"\"\n    if (tag := tag_for_keyword(keyword)) is None:\n        return None\n    return f\"0x{tag:X}\" if hex_format else str(tag)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.lookup_tag(hex_format)","title":"<code>hex_format</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.tag_exists","title":"tag_exists  <code>cached</code>","text":"<pre><code>tag_exists(keyword: str) -&gt; bool\n</code></pre> <p>Boolean check if a DICOM tag exists for a given keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tag exists, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tag_exists(\"PatientID\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\nFalse\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef tag_exists(keyword: str) -&gt; bool:\n    \"\"\"Boolean check if a DICOM tag exists for a given keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to check.\n\n    Returns\n    -------\n    bool\n        True if the tag exists, False otherwise.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; tag_exists(\"PatientID\")\n    True\n\n    &gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\n    False\n    \"\"\"\n    return dictionary_has_tag(keyword)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.tag_exists(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags","title":"similar_tags  <code>cached</code>","text":"<pre><code>similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; typing.List[str]\n</code></pre> <p>Find similar DICOM tags for a given keyword.</p> <p>Useful for User Interface to suggest similar tags based on a misspelled keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The keyword to search for similar tags.</p> required <code>int</code> <p>Maximum number of similar tags to return (default is 3).</p> <code>3</code> <code>float</code> <p>Minimum similarity ratio (default is 0.6).</p> <code>0.6</code> <p>Returns:</p> Type Description <code>typing.List[str]</code> <p>A list of up to <code>n</code> similar DICOM tags.</p> <p>Examples:</p> <p>Find similar tags for a misspelled keyword:</p> <pre><code>&gt;&gt;&gt; similar_tags(\"PatinetID\")\n['PatientID', 'PatientName', 'PatientBirthDate']\n</code></pre> <p>Adjust the number of results and threshold:</p> <pre><code>&gt;&gt;&gt; similar_tags(\n...     \"PatinetID\",\n...     n=5,\n...     threshold=0.7,\n... )\n['PatientID', 'PatientName']\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef similar_tags(\n    keyword: str, n: int = 3, threshold: float = 0.6\n) -&gt; List[str]:\n    \"\"\"Find similar DICOM tags for a given keyword.\n\n    Useful for User Interface to suggest similar tags based on a misspelled keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The keyword to search for similar tags.\n    n : int, optional\n        Maximum number of similar tags to return (default is 3).\n    threshold : float, optional\n        Minimum similarity ratio (default is 0.6).\n\n    Returns\n    -------\n    List[str]\n        A list of up to `n` similar DICOM tags.\n\n    Examples\n    --------\n    Find similar tags for a misspelled keyword:\n\n    &gt;&gt;&gt; similar_tags(\"PatinetID\")\n    ['PatientID', 'PatientName', 'PatientBirthDate']\n\n    Adjust the number of results and threshold:\n\n    &gt;&gt;&gt; similar_tags(\n    ...     \"PatinetID\",\n    ...     n=5,\n    ...     threshold=0.7,\n    ... )\n    ['PatientID', 'PatientName']\n    \"\"\"\n    return difflib.get_close_matches(keyword, ALL_DICOM_TAGS, n, threshold)\n</code></pre>"},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(n)","title":"<code>n</code>","text":""},{"location":"reference/dicom-utils/tag-helpers/#imgtools.dicom.utils.similar_tags(threshold)","title":"<code>threshold</code>","text":""},{"location":"reference/dicom-utils/tag_exists/","title":"Tag Exists","text":""},{"location":"reference/dicom-utils/tag_exists/#imgtools.dicom.tag_exists","title":"imgtools.dicom.tag_exists  <code>cached</code>","text":"<pre><code>tag_exists(keyword: str) -&gt; bool\n</code></pre> <p>Boolean check if a DICOM tag exists for a given keyword.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The DICOM keyword to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tag exists, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tag_exists(\"PatientID\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\nFalse\n</code></pre> Source code in <code>src/imgtools/dicom/utils.py</code> <pre><code>@functools.lru_cache(maxsize=1024)\ndef tag_exists(keyword: str) -&gt; bool:\n    \"\"\"Boolean check if a DICOM tag exists for a given keyword.\n\n    Parameters\n    ----------\n    keyword : str\n        The DICOM keyword to check.\n\n    Returns\n    -------\n    bool\n        True if the tag exists, False otherwise.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; tag_exists(\"PatientID\")\n    True\n\n    &gt;&gt;&gt; tag_exists(\"InvalidKeyword\")\n    False\n    \"\"\"\n    return dictionary_has_tag(keyword)\n</code></pre>"},{"location":"reference/dicom-utils/tag_exists/#imgtools.dicom.tag_exists(keyword)","title":"<code>keyword</code>","text":""},{"location":"reference/dicomsort/dicomsorter/","title":"DICOMSorter","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort","title":"sort","text":"<p>Sorting DICOM Files by Specific Tags and Patterns.</p> <p>This module provides functionality to organize DICOM files into structured directories based on customizable target patterns.</p> <p>The target patterns allow metadata-driven file organization using placeholders for DICOM tags, enabling flexible and systematic storage.</p> Extended Summary <p>Target patterns define directory structures using placeholders, such as <code>%&lt;DICOMKey&gt;</code> and <code>{DICOMKey}</code>, which are resolved to their corresponding metadata values in the DICOM file.</p> <p>This approach ensures that files are organized based on their metadata, while retaining their original basenames. Files with identical metadata fields are placed in separate directories to preserve unique identifiers.</p> <p>Examples of target patterns:</p> <pre><code>- `%PatientID/%StudyID/{SeriesID}/`\n- `path/to_destination/%PatientID/images/%Modality/%SeriesInstanceUID/`\n</code></pre> <p>Important: Only the directory structure is modified during the sorting process. The basename of each file remains unchanged.</p> Notes <p>The module ensures that:</p> <ol> <li>Target patterns are resolved accurately based on the metadata in DICOM files.</li> <li>Files are placed in directories that reflect their resolved metadata fields.</li> <li>Original basenames are preserved to prevent unintended overwrites!</li> </ol> <p>Examples:</p> <p>Source file:</p> <pre><code>/source_dir/HN-CHUS-082/1-1.dcm\n</code></pre> <p>Target directory pattern:</p> <pre><code>./data/dicoms/%PatientID/Study-%StudyInstanceUID/Series-%SeriesInstanceUID/%Modality/\n</code></pre> <p>would result in the following structure for each file:</p> <pre><code>data/\n\u2514\u2500\u2500 dicoms/\n    \u2514\u2500\u2500 {PatientID}/\n        \u2514\u2500\u2500 Study-{StudyInstanceUID}/\n            \u2514\u2500\u2500 Series-{SeriesInstanceUID}/\n                \u2514\u2500\u2500 {Modality}/\n                    \u2514\u2500\u2500 1-1.dcm\n</code></pre> <p>And so the resolved path for the file would be:</p> <pre><code>./data/dicoms/HN-CHUS-082/Study-06980/Series-67882/RTSTRUCT/1-1.dcm\n</code></pre> <p>Here, the file is relocated into the resolved directory structure:</p> <pre><code>./data/dicoms/HN-CHUS-082/Study-06980/Series-67882/RTSTRUCT/\n</code></pre> <p>while the basename <code>1-1.dcm</code> remains unchanged.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter","title":"DICOMSorter","text":"<pre><code>DICOMSorter(\n    source_directory: pathlib.Path,\n    target_pattern: str,\n    pattern_parser: typing.Pattern = imgtools.dicom.sort.dicomsorter.DEFAULT_PATTERN_PARSER,\n)\n</code></pre> <p>A specialized implementation of the <code>SorterBase</code> for sorting DICOM files by metadata.</p> <p>This class resolves paths for DICOM files based on specified target patterns, using metadata extracted from the files. The filename of each source file is preserved during this process.</p> <p>Attributes:</p> Name Type Description <code>source_directory</code> <code>pathlib.Path</code> <p>The directory containing the files to be sorted.</p> <code>logger</code> <code>Logger</code> <p>The instance logger bound with the source directory context.</p> <code>dicom_files</code> <code>list of Path</code> <p>The list of DICOM files found in the <code>source_directory</code>.</p> <code>format</code> <code>str</code> <p>The parsed format string with placeholders for DICOM tags.</p> <code>keys</code> <code>typing.Set[str]</code> <p>DICOM tags extracted from the target pattern.</p> <code>invalid_keys</code> <code>typing.Set[str]</code> <p>DICOM tags from the pattern that are invalid.</p> <p>Methods:</p> Name Description <code>execute</code> <p>Execute the file action on DICOM files.</p> <code>print_tree</code> <p>Display the pattern structure as a tree visualization.</p> <code>validate_keys</code> <p>Validate extracted keys. Subclasses should implement this method</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def __init__(\n    self,\n    source_directory: Path,\n    target_pattern: str,\n    pattern_parser: Pattern = DEFAULT_PATTERN_PARSER,\n) -&gt; None:\n    super().__init__(\n        source_directory=source_directory,\n        target_pattern=target_pattern,\n        pattern_parser=pattern_parser,\n    )\n    self.logger.debug(\n        \"All DICOM Keys are Valid in target pattern\", keys=self.keys\n    )\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.format","title":"format  <code>property</code>","text":"<pre><code>format: str\n</code></pre> <p>Get the formatted pattern string.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.invalid_keys","title":"invalid_keys  <code>property</code>","text":"<pre><code>invalid_keys: typing.Set[str]\n</code></pre> <p>Get the set of invalid keys.</p> <p>Essentially, this will check <code>pydicom.dictionary_has_tag</code> for each key in the pattern and return the set of keys that are invalid.</p> <p>Returns:</p> Type Description <code>typing.Set[str]</code> <p>The set of invalid keys.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: typing.Set[str]\n</code></pre> <p>Get the set of keys extracted from the pattern.</p>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.pattern_preview","title":"pattern_preview  <code>property</code>","text":"<pre><code>pattern_preview: str\n</code></pre> <p>Returns a human readable preview of the pattern.</p> <p>Useful for visualizing the pattern structure and can be highlighted using Rich Console.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; target_pattern = \"%key1/%key2/%key3\"\n&gt;&gt;&gt; pattern_preview = \"{key1}/{key2}/{key3}\"\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute","title":"execute","text":"<pre><code>execute(\n    action: (\n        imgtools.dicom.sort.FileAction | str\n    ) = imgtools.dicom.sort.FileAction.MOVE,\n    overwrite: bool = False,\n    dry_run: bool = False,\n    num_workers: int = 1,\n) -&gt; None\n</code></pre> <p>Execute the file action on DICOM files.</p> <p>Users are encouraged to use FileAction.HARDLINK for efficient storage and performance for large dataset, as well as protection against lost data.</p> <p>Using hard links can save disk space and improve performance by creating multiple directory entries (links) for a single file instead of duplicating the file content. This is particularly useful when working with large datasets, such as DICOM files, where storage efficiency is crucial.</p> <p>Parameters:</p> Name Type Description Default <code>imgtools.dicom.sort.FileAction</code> <pre><code>The action to apply to the DICOM files (e.g., move, copy).\n</code></pre> <code>FileAction.MOVE</code> <code>bool</code> <pre><code>If True, overwrite existing files at the destination.\n</code></pre> <code>False</code> <code>bool</code> <pre><code>If True, perform a dry run without making any changes.\n</code></pre> <code>False</code> <code>int</code> <pre><code>The number of worker threads to use for processing files.\n</code></pre> <code>1</code> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def execute(\n    self,\n    action: FileAction | str = FileAction.MOVE,\n    overwrite: bool = False,\n    dry_run: bool = False,\n    num_workers: int = 1,\n) -&gt; None:\n    \"\"\"Execute the file action on DICOM files.\n\n    Users are encouraged to use FileAction.HARDLINK for\n    efficient storage and performance for large dataset, as well as\n    protection against lost data.\n\n    Using hard links can save disk space and improve performance by\n    creating multiple directory entries (links) for a single file\n    instead of duplicating the file content. This is particularly\n    useful when working with large datasets, such as DICOM files,\n    where storage efficiency is crucial.\n\n    Parameters\n    ----------\n    action : FileAction, default: FileAction.MOVE\n            The action to apply to the DICOM files (e.g., move, copy).\n    overwrite : bool, default: False\n            If True, overwrite existing files at the destination.\n    dry_run : bool, default: False\n            If True, perform a dry run without making any changes.\n    num_workers : int, default: 1\n            The number of worker threads to use for processing files.\n\n    Raises\n    ------\n    ValueError\n            If the provided action is not a valid FileAction.\n    \"\"\"\n    if not isinstance(action, FileAction):\n        action = FileAction.validate(action)\n\n    self.logger.debug(\n        f\"Mapping {len(self.dicom_files)} files to new paths\"\n    )\n\n    # Create a progress bar that can be used to track everything\n    with self._progress_bar() as progress_bar:\n        ################################################################################\n        # Resolve new paths\n        ################################################################################\n        file_map: Dict[Path, Path] = self._resolve_new_paths(\n            progress_bar=progress_bar, num_workers=num_workers\n        )\n    self.logger.info(\"Finished resolving paths\")\n\n    ################################################################################\n    # Check if any of the resolved paths are duplicates\n    ################################################################################\n    file_map = self._check_duplicates(file_map)\n    self.logger.info(\"Finished checking for duplicates\")\n\n    ################################################################################\n    # Handle files\n    ################################################################################\n    if dry_run:\n        self._dry_run(file_map)\n        return\n\n    with self._progress_bar() as progress_bar:\n        task_files = progress_bar.add_task(\n            \"Handling files\", total=len(file_map)\n        )\n        new_paths: List[Path | None] = []\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            future_to_file = {\n                executor.submit(\n                    handle_file,\n                    source_path,\n                    resolved_path,\n                    action,\n                    overwrite,\n                ): source_path\n                for source_path, resolved_path in file_map.items()\n            }\n            for future in as_completed(future_to_file):\n                try:\n                    result = future.result()\n                    new_paths.append(result)\n                    progress_bar.update(task_files, advance=1)\n                except Exception as e:\n                    self.logger.exception(\n                        \"Failed to handle file\",\n                        exc_info=e,\n                        file=future_to_file[future],\n                    )\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(action)","title":"<code>action</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(dry_run)","title":"<code>dry_run</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.execute(num_workers)","title":"<code>num_workers</code>","text":""},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.print_tree","title":"print_tree","text":"<pre><code>print_tree(base_dir: pathlib.Path | None = None) -&gt; None\n</code></pre> <p>Display the pattern structure as a tree visualization.</p> Notes <p>This only prints the target pattern, parsed and formatted. Performing a dry-run execute will display more information.</p> Source code in <code>src/imgtools/dicom/sort/sorter_base.py</code> <pre><code>def print_tree(self, base_dir: Path | None = None) -&gt; None:\n    \"\"\"\n    Display the pattern structure as a tree visualization.\n\n    Notes\n    -----\n    This only prints the target pattern, parsed and formatted.\n    Performing a dry-run execute will display more information.\n\n    Raises\n    ------\n    SorterBaseError\n        If the tree visualization fails to generate.\n    \"\"\"\n    try:\n        base_dir = base_dir or Path().cwd().resolve()\n        tree = self._setup_tree(base_dir)\n        self._generate_tree_structure(self.pattern_preview, tree)\n        self._console.print(tree)\n    except Exception as e:\n        errmsg = \"Failed to generate tree visualization.\"\n        raise SorterBaseError(errmsg) from e\n</code></pre>"},{"location":"reference/dicomsort/dicomsorter/#imgtools.dicom.sort.DICOMSorter.validate_keys","title":"validate_keys","text":"<pre><code>validate_keys() -&gt; None\n</code></pre> <p>Validate extracted keys. Subclasses should implement this method to perform specific validations based on their context.</p> <p>Validate the DICOM keys in the target pattern.</p> <p>If any invalid keys are found, it suggests similar valid keys and raises an error.</p> Source code in <code>src/imgtools/dicom/sort/dicomsorter.py</code> <pre><code>def validate_keys(self) -&gt; None:\n    \"\"\"Validate the DICOM keys in the target pattern.\n\n    If any invalid keys are found, it\n    suggests similar valid keys and raises an error.\n    \"\"\"\n    if not self.invalid_keys:\n        return\n\n    for key in sorted(self.invalid_keys):\n        # TODO: keep this logic, but make the suggestion more user-friendly/readable\n        similar = similar_tags(key)\n        suggestion = (\n            f\"\\n\\tDid you mean: [bold green]{', '.join(similar)}[/bold green]?\"\n            if similar\n            else \" And [bold red]no similar keys[/bold red] found.\"\n        )\n        _error = (\n            f\"Invalid DICOM key: [bold red]{key}[/bold red].{suggestion}\"\n        )\n        self._console.print(f\"{_error}\")\n    self._console.print(f\"Parsed Path: `{self.pattern_preview}`\")\n    errmsg = \"Invalid DICOM Keys found.\"\n    raise InvalidDICOMKeyError(errmsg)\n</code></pre>"},{"location":"reference/dicomsort/patternparser/","title":"PatternParser","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser","title":"parser","text":"<p>Parser module for extracting and validating placeholders from target patterns.</p> Summary <p>This module provides functionality to parse and validate sorting patterns with placeholders. Users can define custom regex patterns to extract keys from their sorting patterns.</p> Extended Summary <p>The <code>PatternParser</code> class allows users to define patterns with placeholders that can be replaced with actual values. The placeholders can be defined using custom regex patterns, making the parser flexible for various use cases.</p> <p>Examples:</p> <p>Setup:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import (\n...     PatternParser,\n... )\n</code></pre> <p>Example 1: Suppose you want to parse a target pattern like <code>{Key1}-{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"John\",\n...     \"Key2\": \"Doe\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"{Key1}-{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'John-Doe'\n</code></pre> <p>Example 2: Suppose you want to parse a target pattern like <code>%&lt;Key1&gt; and {Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"Alice\",\n...     \"Key2\": \"Bob\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"%&lt;Key1&gt; and {Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(\n...     r\"%&lt;(\\w+)&gt;|\\{(\\w+)\\}\"\n... )\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s and %(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Alice and Bob'\n</code></pre> <p>Example 3: Suppose you want to parse a target pattern like <code>/path/to/{Key1}/and/{Key2}</code> and replace the placeholders with values from a dictionary:</p> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"folder1\",\n...     \"Key2\": \"folder2\",\n... }\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = \"/path/to/{Key1}/and/{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'/path/to/%(Key1)s/and/%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n</code></pre> <p>Now you can use the formatted pattern to replace the placeholders:</p> <pre><code>&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'/path/to/folder1/and/folder2'\n</code></pre>"},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser.PatternParser","title":"PatternParser","text":"<pre><code>PatternParser(\n    pattern: str, pattern_matcher: typing.Pattern\n)\n</code></pre> <p>A helper class to parse, validate, and sanitize sorting patterns.</p> <p>This class handles: - Pattern parsing and validation - Key extraction from patterns</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <p>The pattern string to parse.</p> required <code>typing.Pattern</code> <p>Custom regex pattern for parsing</p> required <p>Attributes:</p> Name Type Description <code>keys</code> <code>list of str</code> <p>Extracted keys from the pattern.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; from imgtools.dicom.sort.parser import (\n...     PatternParser,\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; key_values = {\n...     \"Key1\": \"Value1\",\n...     \"Key2\": \"Value2\",\n... }\n&gt;&gt;&gt; pattern = \"{Key1}-{Key2}\"\n&gt;&gt;&gt; pattern_matcher = re.compile(r\"\\{(\\w+)\\}\")\n&gt;&gt;&gt; parser = PatternParser(\n...     pattern,\n...     pattern_matcher,\n... )\n&gt;&gt;&gt; (\n...     formatted_pattern,\n...     keys,\n... ) = parser.parse()\n&gt;&gt;&gt; print(formatted_pattern)\n'%(Key1)s-%(Key2)s'\n&gt;&gt;&gt; print(keys)\n['Key1', 'Key2']\n&gt;&gt;&gt; resolved_string = formatted_pattern % key_values\n&gt;&gt;&gt; print(resolved_string)\n'Value1-Value2'\n</code></pre> <p>Methods:</p> Name Description <code>parse</code> <p>Parse and validate the pattern.</p> Source code in <code>src/imgtools/pattern_parser/parser.py</code> <pre><code>def __init__(self, pattern: str, pattern_matcher: Pattern) -&gt; None:\n    assert isinstance(pattern, str) and pattern, (\n        \"Pattern must be a non-empty string.\"\n    )\n    self._pattern = pattern\n    self._keys: List[str] = []\n    assert isinstance(pattern_matcher, Pattern), (\n        \"Pattern parser must be a regex pattern.\"\n    )\n    self._parser: Pattern = pattern_matcher\n</code></pre>"},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser.PatternParser(pattern)","title":"<code>pattern</code>","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser.PatternParser(pattern_matcher)","title":"<code>pattern_matcher</code>","text":""},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser.PatternParser.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: typing.List[str]\n</code></pre> <p>Get the list of extracted keys.</p>"},{"location":"reference/dicomsort/patternparser/#imgtools.pattern_parser.parser.PatternParser.parse","title":"parse","text":"<pre><code>parse() -&gt; typing.Tuple[str, typing.List[str]]\n</code></pre> <p>Parse and validate the pattern.</p> <p>Returns:</p> Type Description <code>typing.Tuple[str, typing.List[str]]</code> <p>The formatted pattern string and a list of extracted keys.</p> Source code in <code>src/imgtools/pattern_parser/parser.py</code> <pre><code>def parse(self) -&gt; Tuple[str, List[str]]:\n    \"\"\"\n    Parse and validate the pattern.\n\n    Returns\n    -------\n    Tuple[str, List[str]]\n        The formatted pattern string and a list of extracted keys.\n\n    Raises\n    ------\n    InvalidPatternError\n        If the pattern contains no valid placeholders or is invalid.\n    \"\"\"\n\n    sanitized_pattern = self._pattern.strip()\n    if not self._parser.search(sanitized_pattern):\n        errmsg = f\"Pattern must contain placeholders matching '{self._parser.pattern}'.\"\n        raise InvalidPatternError(errmsg)\n\n    formatted_pattern = self._parser.sub(\n        self._replace_key, sanitized_pattern\n    )\n    return formatted_pattern, self._keys\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>The following documentation is a work-in-progress. If you encounter any issues or discrepencies, please let us know by opening an issue.</p>"},{"location":"usage/CoreTypes/","title":"Bounding Box","text":""},{"location":"usage/CoreTypes/regionbox/","title":"RegionBox: A Simple Way to Work with 3D Image Regions","text":"In\u00a0[2]: Copied! <pre>from imgtools import Coordinate3D, RegionBox\nfrom rich import print\n\n# Define a box from (5,5,5) to (10,10,10)\nbox = RegionBox(Coordinate3D(5, 5, 5), Coordinate3D(10, 10, 10))\n\nprint(box)\n</pre> from imgtools import Coordinate3D, RegionBox from rich import print  # Define a box from (5,5,5) to (10,10,10) box = RegionBox(Coordinate3D(5, 5, 5), Coordinate3D(10, 10, 10))  print(box) <pre>RegionBox(\n        min=Coordinate3D(x=5, y=5, z=5),\n        max=Coordinate3D(x=10, y=10, z=10)\n        size=Size3D(w=5, h=5, d=5)\n)\n</pre> In\u00a0[3]: Copied! <pre># Expand the box symmetrically by 5 units in all directions\nexpanded_box = box.pad(5)\nprint(expanded_box)\n</pre> # Expand the box symmetrically by 5 units in all directions expanded_box = box.pad(5) print(expanded_box) <pre>RegionBox(\n        min=Coordinate3D(x=0, y=0, z=0),\n        max=Coordinate3D(x=15, y=15, z=15)\n        size=Size3D(w=15, h=15, d=15)\n)\n</pre> <p>By default, the box expands equally in all directions by <code>padding</code> voxels.</p> <p>If you only want to expand in one direction (the max side), use the <code>BoxPadMethod.END</code> option:</p> In\u00a0[9]: Copied! <pre>from imgtools import BoxPadMethod\n\nend_padded_box = box.pad(5, method=BoxPadMethod.END)\nprint(end_padded_box)\n</pre> from imgtools import BoxPadMethod  end_padded_box = box.pad(5, method=BoxPadMethod.END) print(end_padded_box) <pre>RegionBox(\n        min=Coordinate3D(x=5, y=5, z=5),\n        max=Coordinate3D(x=15, y=15, z=15)\n        size=Size3D(w=10, h=10, d=10)\n)\n</pre> In\u00a0[15]: Copied! <pre>from imgtools.datasets import example_data\n\nmask = example_data()['mask']\n\n# Create a box around the mask's bounding box\nbbox_region = RegionBox.from_mask_bbox(mask)\nprint(bbox_region)\n</pre> from imgtools.datasets import example_data  mask = example_data()['mask']  # Create a box around the mask's bounding box bbox_region = RegionBox.from_mask_bbox(mask) print(bbox_region) <pre>RegionBox(\n        min=Coordinate3D(x=45, y=21, z=67),\n        max=Coordinate3D(x=55, y=38, z=84)\n        size=Size3D(w=10, h=17, d=17)\n)\n</pre> <p>You can create a <code>RegionBox</code> from the centroid of the mask.</p> <p>This will create a <code>RegionBox</code> of size 0, where the min and max points are the same.</p> <p>However, you can expand the box to include more context around the mask.</p> In\u00a0[13]: Copied! <pre># Create a box around the mask's centroid\ncentroid_box = RegionBox.from_mask_centroid(mask)\nprint(centroid_box)\n\nexpanded_centroid_box = centroid_box.expand_to_min_size(10)\nprint(expanded_centroid_box)\n</pre> # Create a box around the mask's centroid centroid_box = RegionBox.from_mask_centroid(mask) print(centroid_box)  expanded_centroid_box = centroid_box.expand_to_min_size(10) print(expanded_centroid_box) <pre>RegionBox(\n        min=Coordinate3D(x=50, y=29, z=76),\n        max=Coordinate3D(x=50, y=29, z=76)\n        size=Size3D(w=0, h=0, d=0)\n)\n</pre> <pre>RegionBox(\n        min=Coordinate3D(x=45, y=24, z=71),\n        max=Coordinate3D(x=55, y=34, z=81)\n        size=Size3D(w=10, h=10, d=10)\n)\n</pre> In\u00a0[16]: Copied! <pre>import SimpleITK as sitk\n\n# Load an example image (100x100x100 voxel CT scan)\nimage = example_data()['duck']\n\ncropped_image = bbox_region.crop_image(image)\nprint(cropped_image.GetSize())\n</pre> import SimpleITK as sitk  # Load an example image (100x100x100 voxel CT scan) image = example_data()['duck']  cropped_image = bbox_region.crop_image(image) print(cropped_image.GetSize()) <pre>(10, 17, 17)\n</pre> <p>You can also crop both the image and the mask at the same time.</p> In\u00a0[18]: Copied! <pre>cropped_image, cropped_mask = bbox_region.crop_image_and_mask(image, mask)\n\nprint(cropped_image.GetSize())\nprint(cropped_mask.GetSize())\n</pre> cropped_image, cropped_mask = bbox_region.crop_image_and_mask(image, mask)  print(cropped_image.GetSize()) print(cropped_mask.GetSize()) <pre>(10, 17, 17)\n</pre> <pre>(10, 17, 17)\n</pre> In\u00a0[24]: Copied! <pre># Perform multiple steps together\ncropped_image, cropped_mask = (\n  RegionBox.from_mask_centroid(mask)\n    .expand_to_min_size(20)\n    .crop_image_and_mask(image, mask)\n)\n\nprint(cropped_image.GetSize())\nprint(cropped_mask.GetSize())\n</pre> # Perform multiple steps together cropped_image, cropped_mask = (   RegionBox.from_mask_centroid(mask)     .expand_to_min_size(20)     .crop_image_and_mask(image, mask) )  print(cropped_image.GetSize()) print(cropped_mask.GetSize()) <pre>(20, 20, 20)\n</pre> <pre>(20, 20, 20)\n</pre>"},{"location":"usage/CoreTypes/regionbox/#regionbox-a-simple-way-to-work-with-3d-image-regions","title":"RegionBox: A Simple Way to Work with 3D Image Regions\u00b6","text":""},{"location":"usage/CoreTypes/regionbox/#overview","title":"Overview\u00b6","text":"<p>The <code>RegionBox</code> class helps define, manipulate, and extract regions from 3D images. If you're working with medical images (like CT or MRI scans), you'll often need to isolate specific areas\u2014whether for visualization, processing, or deep learning tasks.</p> <p>With <code>RegionBox</code>, you can:</p> <ul> <li>Define a 3D box with minimum and maximum coordinates.</li> <li>Expand, pad, or adjust the box.</li> <li>Extract regions from images.</li> <li>Create a <code>RegionBox</code> from segmentation masks.</li> </ul>"},{"location":"usage/CoreTypes/regionbox/#getting-started","title":"Getting Started\u00b6","text":"<p>To create a <code>RegionBox</code>, all you need are two 3D points: the minimum (corner closest to the origin) and the maximum (opposite corner).</p>"},{"location":"usage/CoreTypes/regionbox/#working-with-regionbox","title":"Working with RegionBox\u00b6","text":""},{"location":"usage/CoreTypes/regionbox/#expanding-the-box","title":"Expanding the Box\u00b6","text":"<p>Sometimes, you need to make the box bigger\u2014whether to ensure it contains enough context or meets a minimum required size.</p>"},{"location":"usage/CoreTypes/regionbox/#creating-a-regionbox-from-a-mask","title":"Creating a RegionBox from a Mask\u00b6","text":"<p>If you have a segmentation mask (where a structure of interest is labeled), you can automatically create a <code>RegionBox</code> around it.</p>"},{"location":"usage/CoreTypes/regionbox/#cropping-an-image","title":"Cropping an Image\u00b6","text":"<p>Once you have a <code>RegionBox</code>, you can crop an image to the exact region. This is useful when extracting specific anatomical structures.</p>"},{"location":"usage/CoreTypes/regionbox/#summary","title":"Summary\u00b6","text":"<p>The <code>RegionBox</code> class makes it easy to define, manipulate, and extract regions from 3D images. Whether you're cropping scans or defining analysis regions, it provides a simple and flexible interface.</p> <p>Key features:</p> <ul> <li>Define regions with <code>RegionBox(min, max)</code>.</li> <li>Expand regions with <code>.pad()</code>, <code>.expand_to_cube()</code>, etc.</li> <li>Extract image regions using <code>.crop_image()</code>.</li> <li>Create boxes from masks with <code>.from_mask_bbox()</code> or <code>.from_mask_centroid()</code>.</li> </ul> <p>Try it out and make working with medical image regions easier!</p>"},{"location":"usage/Functional/functional/","title":"Functional Transformations","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops.functional import (\n    bounding_box,\n    centroid,\n    clip_intensity,\n    crop,\n    crop_to_mask_bounding_box,\n    image_statistics,\n    min_max_scale,\n    resample,\n    resize,\n    rotate,\n    standard_scale,\n    window_intensity,\n    zoom,\n)\n</pre> from pathlib import Path  from imgtools.io import read_dicom_auto from imgtools.ops.functional import (     bounding_box,     centroid,     clip_intensity,     crop,     crop_to_mask_bounding_box,     image_statistics,     min_max_scale,     resample,     resize,     rotate,     standard_scale,     window_intensity,     zoom, ) In\u00a0[2]: Copied! <pre>image_path = Path.cwd().parent.parent.parent / 'data/Head-Neck-PET-CT/HN-CHUS-052/08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362'\n\n# mask path used later in the notebook \nmask_path = Path.cwd().parent.parent.parent / 'data/Head-Neck-PET-CT/HN-CHUS-052/08-27-1885-CA ORL FDG TEP POS TX-94629/1.000000-RTstructCTsim-PETPET-CT-87625'\n\nexample_scan = read_dicom_auto(image_path.as_posix())\nexample_image = example_scan.image\n</pre> image_path = Path.cwd().parent.parent.parent / 'data/Head-Neck-PET-CT/HN-CHUS-052/08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362'  # mask path used later in the notebook  mask_path = Path.cwd().parent.parent.parent / 'data/Head-Neck-PET-CT/HN-CHUS-052/08-27-1885-CA ORL FDG TEP POS TX-94629/1.000000-RTstructCTsim-PETPET-CT-87625'  example_scan = read_dicom_auto(image_path.as_posix()) example_image = example_scan.image In\u00a0[3]: Copied! <pre>from imgtools.datasets import example_data_paths\nfrom SimpleITK import ReadImage\npaths = example_data_paths()\n\nexample_image = ReadImage(paths['duck'])\nexample_mask = ReadImage(paths['mask'])\n</pre> from imgtools.datasets import example_data_paths from SimpleITK import ReadImage paths = example_data_paths()  example_image = ReadImage(paths['duck']) example_mask = ReadImage(paths['mask']) In\u00a0[4]: Copied! <pre>import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport SimpleITK as sitk\n\n\ndef visualize_3d_slices_adaptive(image: sitk.Image, axis: int = 0, step: int = 1, cmap='gray') -&gt; None:\n    \"\"\"\n    Visualizes every n-th slice of a 3D SimpleITK image along a specified axis in an adaptive grid layout.\n\n    Args:\n        image (sitk.Image): The 3D SimpleITK image to visualize.\n        axis (int): The axis along which to slice (0: x, 1: y, 2: z).\n        step (int): The step size for selecting slices. Defaults to 1 (every slice).\n    \"\"\"\n    array = sitk.GetArrayFromImage(image)\n    num_slices = array.shape[axis]\n    selected_slices = range(0, num_slices, step)\n    num_selected_slices = len(selected_slices)\n\n    vmax = array.max()\n    vmin = array.min()\n\n    # Calculate adaptive grid size\n    cols = math.ceil(math.sqrt(num_selected_slices))\n    rows = math.ceil(num_selected_slices / cols)\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n    axes = axes.ravel()  # Flatten the axes for easy iteration\n\n    for i, slice_index in enumerate(selected_slices):\n        if axis == 0:\n            slice_ = array[slice_index, :, :]\n        elif axis == 1:\n            slice_ = array[:, slice_index, :]\n        elif axis == 2:\n            slice_ = array[:, :, slice_index]\n\n        slice_rotated = np.rot90(slice_, k = 2)\n\n        axes[i].imshow(slice_rotated, cmap, vmax=vmax, vmin=vmin)\n        axes[i].set_title(f'Slice {slice_index}')\n        axes[i].axis('off')\n\n    # Turn off unused axes\n    for ax in axes[num_selected_slices:]:\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_3d_slices_adaptive(example_image, axis=2, step=5)\n</pre> import math import numpy as np import matplotlib.pyplot as plt import SimpleITK as sitk   def visualize_3d_slices_adaptive(image: sitk.Image, axis: int = 0, step: int = 1, cmap='gray') -&gt; None:     \"\"\"     Visualizes every n-th slice of a 3D SimpleITK image along a specified axis in an adaptive grid layout.      Args:         image (sitk.Image): The 3D SimpleITK image to visualize.         axis (int): The axis along which to slice (0: x, 1: y, 2: z).         step (int): The step size for selecting slices. Defaults to 1 (every slice).     \"\"\"     array = sitk.GetArrayFromImage(image)     num_slices = array.shape[axis]     selected_slices = range(0, num_slices, step)     num_selected_slices = len(selected_slices)      vmax = array.max()     vmin = array.min()      # Calculate adaptive grid size     cols = math.ceil(math.sqrt(num_selected_slices))     rows = math.ceil(num_selected_slices / cols)      fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))     axes = axes.ravel()  # Flatten the axes for easy iteration      for i, slice_index in enumerate(selected_slices):         if axis == 0:             slice_ = array[slice_index, :, :]         elif axis == 1:             slice_ = array[:, slice_index, :]         elif axis == 2:             slice_ = array[:, :, slice_index]          slice_rotated = np.rot90(slice_, k = 2)          axes[i].imshow(slice_rotated, cmap, vmax=vmax, vmin=vmin)         axes[i].set_title(f'Slice {slice_index}')         axes[i].axis('off')      # Turn off unused axes     for ax in axes[num_selected_slices:]:         ax.axis('off')      plt.tight_layout()     plt.show()  visualize_3d_slices_adaptive(example_image, axis=2, step=5) In\u00a0[5]: Copied! <pre>image_statistics(example_image)\n</pre> image_statistics(example_image) Out[5]: <pre>ImageStatistics(minimum=0.0, maximum=255.0, sum=24488459.0, mean=24.488459, variance=3802.7554795607984, standard_deviation=61.666485870047744)</pre> In\u00a0[6]: Copied! <pre>print('Original Spacing:', example_image.GetSpacing())\nprint('Original Size:', example_image.GetSize())\n\nresampled_image = resample(example_image, [1, 1, 1], output_size=[128, 128, 128])\nprint('Resampled Spacing:', resampled_image.GetSpacing())\nprint('Resampled Size:', resampled_image.GetSize())\n\nvisualize_3d_slices_adaptive(resampled_image, axis=2, step=5)\n</pre> print('Original Spacing:', example_image.GetSpacing()) print('Original Size:', example_image.GetSize())  resampled_image = resample(example_image, [1, 1, 1], output_size=[128, 128, 128]) print('Resampled Spacing:', resampled_image.GetSpacing()) print('Resampled Size:', resampled_image.GetSize())  visualize_3d_slices_adaptive(resampled_image, axis=2, step=5) <pre>Original Spacing: (1.0, 1.0, 1.0)\nOriginal Size: (100, 100, 100)\nResampled Spacing: (1.0, 1.0, 1.0)\nResampled Size: (128, 128, 128)\n</pre> In\u00a0[7]: Copied! <pre>print('Original Size:', example_image.GetSize())\nresized_image = resize(example_image, [256, 256, 0])\nprint('Resized Size:', resized_image.GetSize())\n\nvisualize_3d_slices_adaptive(resampled_image, axis=2, step=5)\n</pre> print('Original Size:', example_image.GetSize()) resized_image = resize(example_image, [256, 256, 0]) print('Resized Size:', resized_image.GetSize())  visualize_3d_slices_adaptive(resampled_image, axis=2, step=5) <pre>Original Size: (100, 100, 100)\nResized Size: (256, 256, 100)\n</pre> In\u00a0[8]: Copied! <pre>zoomed_image = zoom(example_image, 2.0)\n\nvisualize_3d_slices_adaptive(zoomed_image, axis=2, step=5)\n</pre> zoomed_image = zoom(example_image, 2.0)  visualize_3d_slices_adaptive(zoomed_image, axis=2, step=5) In\u00a0[9]: Copied! <pre>size = example_image.GetSize()\ncenter_voxel = [size[i] // 2 for i in range(len(size))]\n\nrotated_image = rotate(example_image, center_voxel, [45, 45, 45])\n\nvisualize_3d_slices_adaptive(rotated_image, axis=2, step=5)\n</pre> size = example_image.GetSize() center_voxel = [size[i] // 2 for i in range(len(size))]  rotated_image = rotate(example_image, center_voxel, [45, 45, 45])  visualize_3d_slices_adaptive(rotated_image, axis=2, step=5) In\u00a0[10]: Copied! <pre>cropped_image = crop(example_image, [50, 50, 50], [90, 90, 90])\n\nvisualize_3d_slices_adaptive(cropped_image, axis=2, step=5)\n</pre> cropped_image = crop(example_image, [50, 50, 50], [90, 90, 90])  visualize_3d_slices_adaptive(cropped_image, axis=2, step=5) In\u00a0[16]: Copied! <pre>mask = example_mask\nvisualize_3d_slices_adaptive(mask, axis=2, step=3, cmap=None)\n</pre> mask = example_mask visualize_3d_slices_adaptive(mask, axis=2, step=3, cmap=None) In\u00a0[12]: Copied! <pre>centre_coords = centroid(mask)\nprint(centre_coords)\n</pre> centre_coords = centroid(mask) print(centre_coords) <pre>(50, 29, 76)\n</pre> In\u00a0[13]: Copied! <pre>box_coords = bounding_box(mask)\nprint(box_coords)\n</pre> box_coords = bounding_box(mask) print(box_coords) <pre>((45, 21, 67), (10, 17, 17))\n</pre> In\u00a0[14]: Copied! <pre>cropped_image, cropped_mask, crop_centre = crop_to_mask_bounding_box(example_image, mask)\nprint(crop_centre)\n\nvisualize_3d_slices_adaptive(cropped_mask, axis=2, step=1)\nvisualize_3d_slices_adaptive(cropped_image, axis=2, step=1)\n</pre> cropped_image, cropped_mask, crop_centre = crop_to_mask_bounding_box(example_image, mask) print(crop_centre)  visualize_3d_slices_adaptive(cropped_mask, axis=2, step=1) visualize_3d_slices_adaptive(cropped_image, axis=2, step=1) <pre>[np.float64(50.0), np.float64(29.5), np.float64(75.5)]\n</pre> In\u00a0[17]: Copied! <pre>clipped_image = clip_intensity(example_image, lower=100, upper=255)\n\nvisualize_3d_slices_adaptive(clipped_image, axis=2, step=5)\n</pre> clipped_image = clip_intensity(example_image, lower=100, upper=255)  visualize_3d_slices_adaptive(clipped_image, axis=2, step=5) In\u00a0[18]: Copied! <pre>windowed_image = window_intensity(example_image, 1500, 0)\n\nvisualize_3d_slices_adaptive(windowed_image, axis=2, step=5)\n</pre> windowed_image = window_intensity(example_image, 1500, 0)  visualize_3d_slices_adaptive(windowed_image, axis=2, step=5) In\u00a0[20]: Copied! <pre>min_max_image = min_max_scale(example_image, minimum=0, maximum=5)\n\nvisualize_3d_slices_adaptive(min_max_image, axis=2, step=5)\n</pre> min_max_image = min_max_scale(example_image, minimum=0, maximum=5)  visualize_3d_slices_adaptive(min_max_image, axis=2, step=5) In\u00a0[21]: Copied! <pre>standardized_image = standard_scale(example_image)\n\nvisualize_3d_slices_adaptive(standardized_image, axis=2, step=5)\n</pre> standardized_image = standard_scale(example_image)  visualize_3d_slices_adaptive(standardized_image, axis=2, step=5)"},{"location":"usage/Functional/functional/#functional-transformations","title":"Functional Transformations\u00b6","text":""},{"location":"usage/Functional/functional/#helpers","title":"Helpers\u00b6","text":"<p>A set of helper functions to aid in the demo.</p>"},{"location":"usage/Functional/functional/#image-statistics","title":"Image Statistics\u00b6","text":""},{"location":"usage/Functional/functional/#image-transformations","title":"Image Transformations\u00b6","text":""},{"location":"usage/Functional/functional/#resample","title":"Resample\u00b6","text":""},{"location":"usage/Functional/functional/#resize","title":"Resize\u00b6","text":""},{"location":"usage/Functional/functional/#zoom","title":"Zoom\u00b6","text":""},{"location":"usage/Functional/functional/#rotate","title":"Rotate\u00b6","text":""},{"location":"usage/Functional/functional/#crop","title":"Crop\u00b6","text":""},{"location":"usage/Functional/functional/#mask-transformations","title":"Mask Transformations\u00b6","text":""},{"location":"usage/Functional/functional/#centroid","title":"Centroid\u00b6","text":""},{"location":"usage/Functional/functional/#bounding-box","title":"Bounding Box\u00b6","text":""},{"location":"usage/Functional/functional/#crop-to-bounding-box","title":"Crop to Bounding Box\u00b6","text":""},{"location":"usage/Functional/functional/#intensity-transformations","title":"Intensity Transformations\u00b6","text":""},{"location":"usage/Functional/functional/#clip-intensity","title":"Clip Intensity\u00b6","text":""},{"location":"usage/Functional/functional/#window-intensity","title":"Window Intensity\u00b6","text":""},{"location":"usage/Functional/functional/#min-max-normalization","title":"Min-max Normalization\u00b6","text":""},{"location":"usage/Functional/functional/#standardization","title":"Standardization\u00b6","text":""},{"location":"usage/Writers/BaseWriter/","title":"Abstract Base Writer","text":"<p>The <code>AbstractBaseWriter</code> class is the foundation for all writers in this library.</p> <p>It provides a standard interface, reusable methods, and tools that writers can extend to handle file writing tasks efficiently and consistently.</p> <p>If you\u2019re building a writer to manage file outputs with custom paths, filenames, or formats, this is where you start!</p> <p>For details on implementing the <code>AbstractBaseWriter</code> in your custom writer, see the Implementing Writers guide.</p>"},{"location":"usage/Writers/BaseWriter/#introduction","title":"Introduction","text":""},{"location":"usage/Writers/BaseWriter/#what-is-the-abstractbasewriter","title":"What is the <code>AbstractBaseWriter</code>?","text":"<p>The <code>AbstractBaseWriter</code> is:</p> <ul> <li>A reusable template: Manage file-writing tasks consistently across different writer implementations.  </li> <li>Customizable: Extend it to handle your file formats and workflows.  </li> <li>Safe and robust: Features context management, filename sanitization, and optional CSV indexing.  </li> </ul>"},{"location":"usage/Writers/BaseWriter/#when-should-you-extend-abstractbasewriter-for-your-custom-writer","title":"When should you extend <code>AbstractBaseWriter</code> for your custom writer?","text":"<p>If you write many files with dynamic paths and filenames, or need to manage file existence scenarios, you might consider extending <code>AbstractBaseWriter</code> (or even one of its subclasses) to simplify your implementation.</p> <p><code>AbstractBaseWriter</code> is useful when you need:</p> <ul> <li>Dynamic paths and filenames (e.g., <code>{subject_id}/{study_date}.nii.gz</code>).  </li> <li>Configurable handling of existing files (<code>OVERWRITE</code>, <code>SKIP</code>, etc.).  </li> <li>Logging of saved files via an optional CSV index.  </li> <li>Thread-safe and multiprocessing-compatible file writing.  </li> <li>A consistent interface across different types of writers.  </li> </ul>"},{"location":"usage/Writers/BaseWriter/#core-concepts","title":"Core Concepts","text":""},{"location":"usage/Writers/BaseWriter/#root-directory-and-filename-format-parameters","title":"Root Directory and Filename Format Parameters","text":"<p>Root Directory:</p> <ul> <li>Base folder for all saved files, automatically created if missing (via <code>create_dirs</code> parameter)</li> </ul> <p>Filename Format:</p> <ul> <li>A string template defining your file and folder names.  </li> <li>Uses placeholders like <code>{key}</code> to insert context values dynamically.  </li> </ul> <p>Example:</p> <pre><code>writer = ExampleWriter(\n    root_directory=\"./data\",\n    filename_format=\"{person_name}/{date}_{message_type}.txt\",\n)\n\n# Save a file with context variables\ndata = \"Hello, World!\"\nwriter.save(\n  data, \n  person_name=\"JohnDoe\",\n  date=\"2025-01-01\",\n  message_type=\"greeting\"\n)\n\n# Saved file path: \n# ./data/JohnDoe/2025-01-01_greeting.txt\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#file-existence-modes","title":"File Existence Modes","text":"<p>Why It Matters:</p> <ul> <li>When your writer saves a file, it needs to decide what to do if a file with the same name already exists.</li> <li>This is especially important in batch operations or when writing to shared directories.</li> <li>The <code>AbstractBaseWriter</code> provides several options to handle this scenario through the use of   an <code>enum</code> called <code>ExistingFileMode</code>.</li> </ul> <p>It is important to handle these options carefully in your writer's <code>save()</code> method to avoid data loss or conflicts.</p>"},{"location":"usage/Writers/BaseWriter/#imgtools.io.writers.ExistingFileMode","title":"ExistingFileMode","text":"<p>Enum to specify handling behavior for existing files.</p> <p>Attributes:</p> Name Type Description <code>OVERWRITE</code> <code>str</code> <p>Overwrite the existing file. Logs as debug and continues with the operation.</p> <code>FAIL</code> <code>str</code> <p>Fail the operation if the file exists. Logs as error and raises a FileExistsError.</p> <code>SKIP</code> <code>str</code> <p>Skip the operation if the file exists. Meant to be used for previewing the path before any expensive computation. <code>preview_path()</code> will return None if the file exists. <code>resolve_path()</code> will still return the path even if the file exists. The writer's <code>save</code> method should handle the file existence if set to SKIP.</p>"},{"location":"usage/Writers/BaseWriter/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"usage/Writers/BaseWriter/#lifecycle-management","title":"Lifecycle Management","text":"<p>Context Manager Support:</p> <ul> <li>Writers can be used with <code>with</code> statements to ensure proper setup and cleanup.  </li> </ul> <p>What Happens on Exit?:</p> <ul> <li>Removes lock files used for the index file.  </li> <li>Deletes empty directories created during the writing process (if no files were written).  </li> </ul> <p>Example:</p> <pre><code>with TextWriter(root_directory=\"/data\", filename_format=\"{id}.txt\") as writer:\n  data = \"Hello, World!\"\n  writer.save(data, id=\"1234\")\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#previewing-file-paths-and-caching-context","title":"Previewing File Paths and Caching Context","text":"<p>In the simplest usage of a writer, users can pass in the context information as keyword arguments to each <code>save()</code> call.</p> <p>However, this can become cumbersome when the same context variables are used across multiple save operations.</p> <p>Example:</p> <p>In the above example, the <code>date</code> and <code>message_type</code> context variables are the same for all students. Instead of passing them in every time, you can store these variables in the writer itself and update them as needed.</p> <p>Let's use the following example to illustrate this:</p> <p>Say we want to save greetings for students in a particular highschool class:</p> <pre><code>writer = TextWriter(\n    root_directory=\"./data\",\n    filename_format=\"{grade}/{class_subject}/{person_name}/{date}_{message_type}.txt\",\n)\n</code></pre> Basic UsageSetting Context Variables manuallySetting Context Variables during Initialization <p>We see here that the context variables for <code>grade</code>, <code>class_subject</code>, <code>date</code>, and <code>message_type</code> are the same for all students.</p> <p>This can become even worse with more context variables, allowing for mistakes, and making the code harder to read.</p> <pre><code>student, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(\n    message,\n    person_name=student,\n    grade=\"12\",\n    class_subject=\"math\",\n    date=\"2025-01-01\",\n    message_type=\"greeting\"\n)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(\n    message,\n    person_name=student,\n    grade=\"12\",\n    class_subject=\"math\",\n    date=\"2025-01-01\",\n    message_type=\"greeting\"\n)\n</code></pre> <p>Instead of passing in the context variables every time, you can store these variables in the writer and update them as needed using the <code>set_context()</code> method.</p> <p>Then only pass in the unique context variables for each <code>.save()</code> operation.</p> <pre><code>writer.set_context(\n  grade=\"12\",\n  class_subject=\"math\",\n  date=\"2025-01-01\",\n  message_type=\"greeting\"\n)\n\nstudent, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(message, person_name=student)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(message, person_name=student)\n</code></pre> <p>If majority of the context variables are the same across all save  operations, you can set context when initializing the writer.</p> <p>Note that here, we must pass as a dictionary to the <code>context</code> parameter  instead of individual keyword arguments.</p> <pre><code>writer = TextWriter(\n    root_directory=\"./data\",\n    filename_format=\"{class_subject}/{person_name}/{date}_{message_type}.txt\",\n    context={\"grade\": \"12\", \"class_subject\": \"math\", \"date\": \"2025-01-01\", \"message_type\": \"greeting\"}\n)\n\nstudent, message = \"Alice\", \"Hello, Alice!\"\nwriter.save(message, person_name=student)\n\nstudent, message = \"Bob\", \"Good morning, Bob!\"\nwriter.save(message, person_name=student)\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#previewing-file-paths","title":"Previewing File Paths","text":"<p>Oftentimes, you may want to check if a file exists before performing an expensive computation. If you set the existence mode to <code>ExistingFileMode.SKIP</code>, the <code>preview_path()</code> method will return <code>None</code> if the file already exists, allowing you to skip the computation.</p> <p>This method also caches the additional context variables for future use.</p> <p>Here's an example of how you might handle this:</p> <pre><code># assuming writer is already initialized with `existing_file_mode=ExistingFileMode.SKIP`\n\n# set some context variables\nwriter.set_context(class_subject=\"math\", date=\"2025-01-01\", message_type=\"greeting\")\n\nif (path := writer.preview_path(person_name=\"Alice\")) is None:\n    print(\"File already exists, skipping computation.\")\nelse:\n    print(f\"Proceed with computation for {path}\")\n    ... \n    # perform expensive computation \n    ...\n    writer.save(content=\"Hello, world!\")\n</code></pre>"},{"location":"usage/Writers/BaseWriter/#index-file-management","title":"Index File Management","text":"<p>What is the Index File?:</p> <ul> <li>A CSV file used to log details about saved files, like their paths and context variables.  </li> <li>Helps track what files have been written, especially useful in batch operations.</li> <li>Additionally can save all the context variables used for each file, convienient for   saving additional metadata, while improving traceability.</li> </ul> <p>How It Works:</p> <ul> <li>By default, it is named <code>{root_directory.name}_index.csv</code>.</li> <li>You can customize the filename or provide an absolute path for more control.  </li> <li>This is something that the <code>save()</code> method in the <code>AbstractBaseWriter</code> should optionally   implement, or let users decide to include the index by calling <code>add_to_index(path)</code> after <code>save()</code>.</li> </ul> <p>Key Features:</p> <ul> <li>Customizable Filename: Use <code>index_filename</code> to set a custom name or absolute path.</li> <li>Absolute/Relative Paths: Control file paths in the index with <code>absolute_paths_in_index</code>.</li> <li>Inter-process Locking: Prevents conflicts in concurrent writing environments.</li> </ul>"},{"location":"usage/Writers/BaseWriter/#sanitizing-filenames","title":"Sanitizing Filenames","text":"<p>Why Sanitize Filenames?:</p> <ul> <li>To ensure that filenames are safe and compatible across different operating systems.  </li> </ul> <p>How It Works:</p> <ul> <li>Replaces illegal characters (e.g., <code>&lt;</code>, <code>&gt;</code>, <code>:</code>, <code>\"</code>, <code>/</code>, <code>\\</code>, <code>|</code>, <code>?</code>, <code>*</code>) with underscores.  </li> <li>Trims leading or trailing spaces and periods to avoid issues.</li> </ul> <p>When Is It Applied?:</p> <ul> <li>Automatically applied when generating filenames, unless disabled by setting <code>sanitize_filenames=False</code>.</li> </ul>"},{"location":"usage/Writers/BaseWriter/#multiprocessing-compatibility","title":"Multiprocessing Compatibility","text":"<p>Why It Matters:</p> <ul> <li>In batch operations or high-performance use cases, multiple processes may write files simultaneously.  </li> </ul> <p>Key Features:</p> <ul> <li>Supports multiprocessing with inter-process locking to ensure thread-safe file writes.  </li> <li>Avoids conflicts or data corruption when multiple instances of a writer are running.</li> </ul>"},{"location":"usage/Writers/ImplementingWriters/","title":"Extending the <code>AbstractBaseWriter</code> class","text":"<p>The <code>AbstractBaseWriter</code> is designed to be extended, allowing you to create custom writers tailored to your specific needs. This guide will walk you through the steps to extend the class and implement your custom functionality.</p>"},{"location":"usage/Writers/ImplementingWriters/#setting-up-your-writer","title":"Setting Up Your Writer","text":"<p>To create a custom writer, you need to extend the <code>AbstractBaseWriter</code> and implement the <code>save</code> method. This method is the core of your writer, handling how and where data is saved.</p> <p>For a walkthrough of all key methods and features, see the Key Methods section below.</p>"},{"location":"usage/Writers/ImplementingWriters/#steps-to-set-up","title":"Steps to Set Up","text":"<ol> <li> <p>Inherit from <code>AbstractBaseWriter</code>:    Create a new class and extend <code>AbstractBaseWriter</code> with the appropriate type.    If you are saving text data, use <code>AbstractBaseWriter[str]</code>, for example.    If you are saving image data, use <code>AbstractBaseWriter[sitk.Image]</code>.</p> </li> <li> <p>Define the <code>save</code> Method:   Use <code>resolve_path()</code> or <code>preview_path()</code> to generate file paths.   Implement the logic for saving data.  </p> </li> <li> <p>Customize Behavior (Optional):   Override any existing methods for specific behavior.   Add additional methods or properties to enhance functionality.  </p> </li> </ol>"},{"location":"usage/Writers/ImplementingWriters/#simple-example","title":"Simple Example","text":"<pre><code>from pathlib import Path\nfrom imgtools.io import AbstractBaseWriter\n\nclass MyCustomWriter(AbstractBaseWriter[str]):\n    def save(self, content: str, **kwargs) -&gt; Path:\n        # Resolve the output file path\n        output_path = self.resolve_path(**kwargs)\n\n        # Write content to the file\n        with output_path.open(mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        # Log and track the save operation\n        self.add_to_index(output_path, **self.context)\n\n        return output_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#implementing-the-save-method","title":"Implementing the <code>save</code> Method","text":"<p>The <code>save</code> method is the heart of your custom writer. It determines how data is written to files and interacts with the core features of <code>AbstractBaseWriter</code>.</p>"},{"location":"usage/Writers/ImplementingWriters/#key-responsibilities-of-save","title":"Key Responsibilities of <code>save</code>","text":"<ol> <li> <p>Path Resolution:</p> <ul> <li>Use <code>resolve_path()</code> to dynamically generate file paths based on the provided     context and filename format.</li> <li>You can optionally use <code>preview_path()</code> as well.</li> <li>Ensure paths are validated to prevent overwriting or duplication.</li> </ul> </li> <li> <p>Data Writing:  </p> <ul> <li>Define how the content will be written to the resolved path.  </li> <li>Use file-handling best practices to ensure reliability.</li> </ul> </li> <li> <p>Logging and Tracking:  </p> <ul> <li>Log each save operation for debugging or auditing purposes.  </li> <li>Use <code>add_to_index()</code> to maintain a record of saved files and their associated     context variables.</li> </ul> </li> <li> <p>Return Value:  </p> <ul> <li>Return the <code>Path</code> object representing the saved file.  </li> <li>This allows users to access the file path for further processing or verification.</li> </ul> </li> </ol>"},{"location":"usage/Writers/ImplementingWriters/#example-implementation","title":"Example Implementation","text":"<p>Here\u2019s a minimal implementation of the <code>save</code> method for a custom writer.</p> <pre><code>from pathlib import Path\nfrom mypackage.abstract_base_writer import AbstractBaseWriter\n\nclass MyCustomWriter(AbstractBaseWriter[str]):\n    def save(self, content: str, **kwargs) -&gt; Path:\n        # Step 1: Resolve the output file path\n        # you can try-catch this in case set to \"FAIL\" mode\n        # or just let the error propagate\n        output_path = self.resolve_path(**kwargs) # resolve_path will always return the path\n\n        # OPTIONAL handling for \"SKIP\" modes\n        if output_path.exists():\n            # this will only be true if the file existence mode\n            # is set to SKIP\n            # - OVERWRITE will have already deleted the file\n            # - upto developer to choose to handle this if set to SKIP\n            pass\n\n        # Step 2: Write the content to the resolved path\n        with output_path.open(mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(content)\n\n        # Step 3: Log and track the save operation\n        self.add_to_index(output_path, filepath_column=\"filepath\", **kwargs)\n\n        # Step 4: ALWAYS Return the saved file path\n        return output_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#key-methods","title":"Key Methods","text":"<p>The <code>AbstractBaseWriter</code> provides several utility methods that simplify file writing and context management. These methods are designed to be flexible and reusable, allowing you to focus on your custom implementation.</p> <p>What It Does:</p> <ul> <li>A helper method for resolving file paths based on the current context and   filename format.  </li> <li>Automatically sanitizes filenames if <code>sanitize_filenames=True</code>.</li> </ul> <p>When to Use It:</p> <ul> <li>Typically called internally by <code>resolve_path()</code> and <code>preview_path()</code>, which handle   additional validation and error handling.</li> <li>Can be called by your class methods to generate paths without the additional   context checks.</li> </ul> <p>Example:</p> <pre><code>custom_path = writer._generate_path(subject=\"math\", name=\"example\")\nprint(f\"Generated path: {custom_path}\")\n</code></pre> <p>By using these key methods effectively, you can customize your writer to handle a wide range of file-writing scenarios while maintaining clean and consistent logic.</p>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.resolve_path","title":"resolve_path","text":"<pre><code>resolve_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Generate a file path based on the filename format, subject ID, and additional parameters.</p> <p>Meant to be used by developers when creating a new writer class and used internally by the <code>save</code> method.</p> <p>What It Does:</p> <ul> <li>Dynamically generates a file path based on the provided context and filename format.</li> </ul> <p>When to Use It:</p> <ul> <li>This method is meant to be used in the <code>save</code> method to determine the file\u2019s target location, but can also be used by external code to generate paths.</li> <li>It ensures you\u2019re working with a valid path and can handle file existence scenarios.</li> <li>Only raises <code>FileExistsError</code> if the file already exists and the mode is set to <code>FAIL</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>resolved_path</code> <code>pathlib.Path</code> <p>The resolved path for the file.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def resolve_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Generate a file path based on the filename format, subject ID, and\n    additional parameters.\n\n    Meant to be used by developers when creating a new writer class\n    and used internally by the `save` method.\n\n    **What It Does**:\n\n    - Dynamically generates a file path based on the provided context and\n    filename format.\n\n    **When to Use It**:\n\n    - This method is meant to be used in the `save` method to determine the\n    file\u2019s target location, but can also be used by external code to\n    generate paths.\n    - It ensures you\u2019re working with a valid path and can handle file\n    existence scenarios.\n    - Only raises `FileExistsError` if the file already exists and the mode\n    is set to `FAIL`.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    -------\n    resolved_path: Path\n        The resolved path for the file.\n\n    Raises\n    ------\n    FileExistsError\n        If the file already exists and the mode is set to FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n    if not out_path.exists():\n        if self.create_dirs:\n            self._ensure_directory_exists(out_path.parent)\n        # should we raise this error here?\n        # elif not out_path.parent.exists():\n        #     msg = f\"Directory {out_path.parent} does not exist.\"\n        #     raise DirectoryNotFoundError(msg)\n        return out_path\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return out_path\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(f\"Deleting existing {out_path} and overwriting.\")\n            out_path.unlink()\n            return out_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.resolve_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.preview_path","title":"preview_path","text":"<pre><code>preview_path(\n    **kwargs: object,\n) -&gt; typing.Optional[pathlib.Path]\n</code></pre> <p>Pre-checking file existence and setting up the writer context.</p> <p>Meant to be used by users to skip expensive computations if a file already exists and you dont want to overwrite it. Only difference between this and resolve_path is that this method does not return the path if the file exists and the mode is set to <code>SKIP</code>.</p> <p>This is because the <code>.save()</code> method should be able to return the path even if the file exists.</p> <p>What It Does:</p> <ul> <li>Pre-checks the file path based on context without writing the file.</li> <li>Returns <code>None</code> if the file exists and the mode is set to <code>SKIP</code>.</li> <li>Raises a <code>FileExistsError</code> if the mode is set to <code>FAIL</code>.</li> <li>An added benefit of using <code>preview_path</code> is that it automatically caches the context variables for future use, and <code>save()</code> can be called without passing in the context variables again.</li> </ul> <p>Examples:</p> <p>Main idea here is to allow users to save computation if they choose to skip existing files.</p> <p>i.e. if file exists and mode is <code>SKIP</code>, we return <code>None</code>, so the user can skip the computation.</p> <pre><code>&gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n&gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n&gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n</code></pre> <p>if the mode is <code>FAIL</code>, we raise an error if the file exists, so user doesnt have to perform expensive computation only to fail when saving.</p> Useful Feature <p>The context is saved in the instance, so running <code>.save()</code> after this will use the same context, and user can optionally update the context with new values passed to <code>.save()</code>.</p> <p><pre><code>&gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n&gt;&gt;&gt;     ... # do some expensive computation to generate the data\n&gt;&gt;&gt;     writer.save(data)\n</code></pre> <code>.save()</code> automatically uses the context for <code>subject</code> and <code>name</code> we passed to <code>preview_path</code></p> <p>Parameters:</p> Name Type Description Default <code>typing.Any</code> <p>Parameters for resolving the filename and validating existence.</p> <code>{}</code> <p>Returns:</p> Type Description <code>pathlib.Path | None</code> <p>If the file exists and the mode is <code>SKIP</code>, returns <code>None</code>. if the file exists and the mode is FAIL, raises a <code>FileExistsError</code>. If the file exists and the mode is OVERWRITE, logs a debug message and returns the path.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def preview_path(self, **kwargs: object) -&gt; Optional[Path]:\n    \"\"\"\n    Pre-checking file existence and setting up the writer context.\n\n    Meant to be used by users to skip expensive computations if a file\n    already exists and you dont want to overwrite it.\n    Only difference between this and resolve_path is that this method\n    does not return the path if the file exists and the mode is set to\n    `SKIP`.\n\n    This is because the `.save()` method should be able to return\n    the path even if the file exists.\n\n    **What It Does**:\n\n    - Pre-checks the file path based on context without writing the file.\n    - Returns `None` if the file exists and the mode is set to `SKIP`.\n    - Raises a `FileExistsError` if the mode is set to `FAIL`.\n    - An added benefit of using `preview_path` is that it automatically\n    caches the context variables for future use, and `save()` can be called\n    without passing in the context variables again.\n\n    Examples\n    --------\n\n    Main idea here is to allow users to save computation if they choose to\n    skip existing files.\n\n    i.e. if file exists and mode is **`SKIP`**, we return\n    `None`, so the user can skip the computation.\n    &gt;&gt;&gt; if nifti_writer.preview_path(subject=\"math\", name=\"test\") is None:\n    &gt;&gt;&gt;     logger.info(\"File already exists. Skipping computation.\")\n    &gt;&gt;&gt;     continue # could be `break` or `return` depending on the use case\n\n    if the mode is **`FAIL`**, we raise an error if the file exists, so user\n    doesnt have to perform expensive computation only to fail when saving.\n\n    **Useful Feature**\n    ----------------------\n    The context is saved in the instance, so running\n    `.save()` after this will use the same context, and user can optionally\n    update the context with new values passed to `.save()`.\n\n    ```python\n    &gt;&gt;&gt; if path := writer.preview_path(subject=\"math\", name=\"test\"):\n    &gt;&gt;&gt;     ... # do some expensive computation to generate the data\n    &gt;&gt;&gt;     writer.save(data)\n    ```\n    `.save()` automatically uses the context for `subject` and `name` we\n    passed to `preview_path`\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Parameters for resolving the filename and validating existence.\n\n    Returns\n    ------\n    Path | None\n        If the file exists and the mode is `SKIP`, returns `None`. if the file\n        exists and the mode is FAIL, raises a `FileExistsError`. If the file\n        exists and the mode is OVERWRITE, logs a debug message and returns\n        the path.\n\n    Raises\n    ------\n    FileExistsError\n        If the file exists and the mode is FAIL.\n    \"\"\"\n    out_path = self._generate_path(**kwargs)\n\n    if not out_path.exists():\n        return out_path\n    elif out_path.is_dir():\n        msg = f\"Path {out_path} is already a directory that exists.\"\n        msg += \" Use a different filename format or context to avoid this.\"\n        raise IsADirectoryError(msg)\n\n    match self.existing_file_mode:\n        case ExistingFileMode.SKIP:\n            return None\n        case ExistingFileMode.FAIL:\n            msg = f\"File {out_path} already exists.\"\n            raise FileExistsError(msg)\n        case ExistingFileMode.OVERWRITE:\n            logger.debug(\n                f\"File {out_path} exists. Deleting and overwriting.\"\n            )\n            out_path.unlink()\n\n    return out_path\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.preview_path(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.clear_context","title":"clear_context","text":"<pre><code>clear_context() -&gt; None\n</code></pre> <p>Clear the context for the writer.</p> <p>Useful for resetting the context after using <code>preview_path</code> or <code>save</code> and want to make sure that the context is empty for new operations.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def clear_context(self) -&gt; None:\n    \"\"\"\n    Clear the context for the writer.\n\n    Useful for resetting the context after using `preview_path` or `save`\n    and want to make sure that the context is empty for new operations.\n    \"\"\"\n    self.context.clear()\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index","title":"add_to_index","text":"<pre><code>add_to_index(\n    path: pathlib.Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    **additional_context: object\n) -&gt; None\n</code></pre> <p>Add or update an entry in the shared CSV index file.</p> <p>What It Does:</p> <ul> <li>Logs the file\u2019s path and associated context variables to a     shared CSV index file.</li> <li>Uses inter-process locking to avoid conflicts when     multiple writers are active.</li> </ul> <p>When to Use It:</p> <ul> <li>Use this method to maintain a centralized record of saved files for auditing or debugging.</li> </ul> Relevant Writer Parameters <ul> <li> <p>The <code>index_filename</code> parameter allows you to specify a custom filename for the index file. By default, it will be named after the <code>root_directory</code> with <code>_index.csv</code> appended.</p> </li> <li> <p>If the index file already exists in the root directory, it will overwrite it unless the <code>overwrite_index</code> parameter is set to <code>False</code>.</p> </li> <li> <p>The <code>absolute_paths_in_index</code> parameter controls whether the paths in the index file are absolute or relative to the <code>root_directory</code>, with <code>False</code> being the default.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>pathlib.Path</code> <p>The file path being saved.</p> required <code>bool</code> <p>If True, write existing context variables passed into writer and the additional context to the CSV. If False, determines only the context keys parsed from the <code>filename_format</code> (excludes all other context variables, and unused context keys).</p> <code>True</code> <code>str</code> <p>The name of the column to store the file path. Defaults to \"path\".</p> <code>'path'</code> <code>bool</code> <p>If True, checks if the file path already exists in the index and replaces it.</p> <code>False</code> <code>typing.Any</code> <p>Additional context information to include in the CSV passed in as keyword arguments.</p> <code>{}</code> Notes <p>When <code>replace_existing</code> is set to True, the method will check if the file path already exists in the index file using <code>csv.Sniffer</code> and replace the row if it does. If the file path does not exist in the index file, it will add a new row with the file path and context information.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def add_to_index(\n    self,\n    path: Path,\n    include_all_context: bool = True,\n    filepath_column: str = \"path\",\n    replace_existing: bool = False,\n    **additional_context: object,\n) -&gt; None:\n    \"\"\"\n    Add or update an entry in the shared CSV index file.\n\n\n    **What It Does**:\n\n    - Logs the file\u2019s path and associated context variables to a\n        shared CSV index file.\n    - Uses inter-process locking to avoid conflicts when\n        multiple writers are active.\n\n    **When to Use It**:\n\n    - Use this method to maintain a centralized record of saved\n    files for auditing or debugging.\n\n    **Relevant Writer Parameters**\n    ------------------------------\n\n    - The `index_filename` parameter allows you to specify a\n    custom filename for the index file.\n    By default, it will be named after the `root_directory`\n    with `_index.csv` appended.\n\n    - If the index file already exists in the root directory,\n    it will overwrite it unless\n    the `overwrite_index` parameter is set to `False`.\n\n    - The `absolute_paths_in_index` parameter controls whether\n    the paths in the index file are absolute or relative to the\n    `root_directory`, with `False` being the default.\n\n    Parameters\n    ----------\n    path : Path\n        The file path being saved.\n    include_all_context : bool\n        If True, write existing context variables passed into writer and\n        the additional context to the CSV.\n        If False, determines only the context keys parsed from the\n        `filename_format` (excludes all other context variables, and\n        unused context keys).\n    filepath_column : str\n        The name of the column to store the file path. Defaults to \"path\".\n    replace_existing : bool\n        If True, checks if the file path already exists in the index and\n        replaces it.\n    **additional_context : Any\n        Additional context information to include in the CSV passed in as\n        keyword arguments.\n\n    Notes\n    -----\n    When `replace_existing` is set to True, the method will check if the\n    file path already exists in the index file using `csv.Sniffer` and\n    replace the row if it does. If the file path does not exist in the\n    index file, it will add a new row with the file path and context\n    information.\n    \"\"\"\n\n    lock_file = self._get_index_lock()\n    self._ensure_directory_exists(self.index_file.parent)\n\n    # Prepare context and resolve the file path\n    context = {**self.context, **additional_context}\n    resolved_path = (\n        path.resolve().absolute()\n        if self.absolute_paths_in_index\n        else path.relative_to(self.root_directory)\n    )\n    fieldnames = [\n        filepath_column,\n        *(\n            context.keys()\n            if include_all_context\n            else self.pattern_resolver.keys\n        ),\n    ]\n\n    rows = []\n    # Check if replacing existing entries and if the index file exists\n    if replace_existing and self.index_file.exists():\n        # Read and validate the index file format\n        try:\n            with (\n                InterProcessLock(lock_file),\n                self.index_file.open(mode=\"r\", encoding=\"utf-8\") as f,\n            ):\n                # Use csv.Sniffer to check if the file has a header\n                sniffer = csv.Sniffer()\n                if not sniffer.has_header(f.readline()):\n                    msg = (\n                        f\"Index {self.index_file} is missing a header row.\"\n                    )\n                    raise ValueError(msg)\n\n                # Reset the file pointer after sampling\n                f.seek(0)\n                reader = csv.DictReader(f)\n                # Check if the required column is present in the index file\n                if (\n                    reader.fieldnames is None\n                    or filepath_column not in reader.fieldnames\n                ):\n                    msg = (\n                        f\"Index file {self.index_file} does \"\n                        f\"not contain the column '{filepath_column}'.\"\n                    )\n                    raise ValueError(msg)\n                # Filter out the existing entry for the resolved path\n                rows = [\n                    row\n                    for row in reader\n                    if row[filepath_column] != str(resolved_path)\n                ]\n        except Exception as e:\n            # Log and raise any exceptions encountered during validation\n            logger.exception(\n                f\"Error validating index file {self.index_file}.\", error=e\n            )\n            raise\n\n    # Add the new or updated row\n    rows.append({filepath_column: str(resolved_path), **context})\n\n    # Write the updated rows back to the index file\n    try:\n        with (\n            InterProcessLock(lock_file),\n            self.index_file.open(\n                mode=\"w\", newline=\"\", encoding=\"utf-8\"\n            ) as f,\n        ):\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(rows)\n    except Exception as e:\n        logger.exception(\n            f\"Error writing to index file {self.index_file}.\", error=e\n        )\n        raise\n</code></pre>"},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index(path)","title":"<code>path</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index(include_all_context)","title":"<code>include_all_context</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index(filepath_column)","title":"<code>filepath_column</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index(replace_existing)","title":"<code>replace_existing</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter.add_to_index(**additional_context)","title":"<code>**additional_context</code>","text":""},{"location":"usage/Writers/ImplementingWriters/#imgtools.io.writers.AbstractBaseWriter._generate_path","title":"_generate_path","text":"<pre><code>_generate_path(**kwargs: object) -&gt; pathlib.Path\n</code></pre> <p>Helper for resolving paths with the given context.</p> Source code in <code>src/imgtools/io/writers/abstract_base_writer.py</code> <pre><code>def _generate_path(self, **kwargs: object) -&gt; Path:\n    \"\"\"\n    Helper for resolving paths with the given context.\n    \"\"\"\n    save_context = {\n        **self._generate_datetime_strings(),\n        **self.context,\n        **kwargs,\n    }\n    self.set_context(**save_context)\n    try:\n        filename = self.pattern_resolver.resolve(save_context)\n    except MissingPlaceholderValueError as e:\n        # Replace the class name in the error message dynamically\n        raise MissingPlaceholderValueError(\n            e.missing_keys,\n            class_name=self.__class__.__name__,\n            key=e.key,\n        ) from e\n    if self.sanitize_filenames:\n        filename = self._sanitize_filename(filename)\n    out_path = self.root_directory / filename\n    logger.debug(\n        f\"Resolved path: {out_path} and {out_path.exists()=}\",\n        handling=self.existing_file_mode,\n    )\n    return out_path\n</code></pre>"}]}